# TensorFlow ile Sinir AÄŸÄ± SÄ±nÄ±flandÄ±rÄ±lmasÄ±

TÃ¼m derin sinir aÄŸlarÄ±nÄ±n iÃ§erdiÄŸi bazÄ± temeller vardÄ±r:
- Bir giriÅŸ katmanÄ±
- BazÄ± gizli katmanlar
- Bir Ã§Ä±ktÄ± katmanÄ±

Geri kalanÄ±n Ã§oÄŸu, modeli oluÅŸturan veri analistine kalmÄ±ÅŸ.

SÄ±nÄ±flandÄ±rma, bazÄ± girdiler verilen bir sÄ±nÄ±f etiketinin Ã§Ä±ktÄ±sÄ±nÄ± almayÄ± iÃ§eren tahmine dayalÄ± bir modelleme problemidir.

AÅŸaÄŸÄ±dakiler, sÄ±nÄ±flandÄ±rma sinir aÄŸlarÄ±nÄ±zda sÄ±klÄ±kla kullanacaÄŸÄ±nÄ±z bazÄ± standart deÄŸerlerdir.

| **Hyperparameter** | **Ä°kili SÄ±nÄ±flandÄ±rma** | **Ã‡oklu SÄ±nÄ±flandÄ±rma** |
| --- | --- | --- |
| Input layer shape | Ã–zellik sayÄ±sÄ± ile aynÄ± (Ã¶rn; yaÅŸ iÃ§in 5, cinsiyet, kilo, boy, sigara iÃ§enlerin kalp krizi riski) | Ä°kili sÄ±nÄ±flandÄ±rma ile aynÄ± |
| Gizli katman | Probleme Ã¶zel, minimum = 1, maksimum = sÄ±nÄ±rsÄ±z | Ä°kili sÄ±nÄ±flandÄ±rma ile aynÄ± |
| Gizli katman baÅŸÄ±na nÃ¶ron sayÄ±sÄ± | Probleme Ã¶zel, genellikle 10 ila 100 | Ä°kili sÄ±nÄ±flandÄ±rma ile aynÄ± |
| Ã‡Ä±ktÄ± katmanÄ± ÅŸekli | 1 (bir sÄ±nÄ±f veya diÄŸer) | SÄ±nÄ±f baÅŸÄ±na 1 adet (Ã¶rneÄŸin yemek, kiÅŸi veya kÃ¶pek fotoÄŸrafÄ± iÃ§in 3) |
| Gizli aktivasyon | Genelde [ReLU](https://www.kaggle.com/dansbecker/rectified-linear-units-relu-in-deep-learning)  | Ä°kili sÄ±nÄ±flandÄ±rma ile aynÄ± |
| Ã‡Ä±kÄ±ÅŸ aktivasyonu | [Sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function) | [Softmax](https://en.wikipedia.org/wiki/Softmax_function) |
| kayÄ±p fonksiyonu | [Cross entropy](https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_loss_function_and_logistic_regression) ([`tf.keras.losses.BinaryCrossentropy`](https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy) in TensorFlow) | Cross entropy ([`tf.keras.losses.CategoricalCrossentropy`](https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy) in TensorFlow) |
| Optimize Edici | [SGD](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD) (stochastic gradient descent), [Adam](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam) | Ä°kili sÄ±nÄ±flandÄ±rma ile aynÄ± |

## Veri OluÅŸturma

Bir sÄ±nÄ±flandÄ±rma veri setini iÃ§e aktararak baÅŸlayabiliriz, ancak hadi kendi sÄ±nÄ±flandÄ±rma verilerimizden oluÅŸturmaya Ã§alÄ±ÅŸalÄ±m.

Scikit-Learn'Ã¼n `make_circles()` iÅŸlevini veriseti oluÅŸturmak iÃ§in kullanalÄ±m.


```python
from sklearn.datasets import make_circles
import tensorflow as tf

# 1000 Ã¶rnek
n_samples = 1000

# verisetini oluÅŸturma
X, y = make_circles(n_samples, 
                    noise=0.03, 
                    random_state=42)
```

Verisetini oluÅŸturuk gibi. Åimdi X ve y deÄŸerlerini kontrol edelim.


```python
# Ã¶zelliklere gÃ¶z atalÄ±m (x1, x2)
X
```




    array([[ 0.75424625,  0.23148074],
           [-0.75615888,  0.15325888],
           [-0.81539193,  0.17328203],
           ...,
           [-0.13690036, -0.81001183],
           [ 0.67036156, -0.76750154],
           [ 0.28105665,  0.96382443]])




```python
# ilk 10'nun etiketi
y[:10]
```




    array([1, 1, 1, 1, 0, 1, 1, 1, 1, 0])



Tamam, bazÄ± verilerimizi ve etiketlerimizi gÃ¶rdÃ¼k, gÃ¶rselleÅŸtirmeye geÃ§meye ne dersiniz?

> ğŸ”‘ Not: Herhangi bir tÃ¼r makine Ã¶ÄŸrenimi projesi baÅŸlatmanÄ±n Ã¶nemli bir adÄ±mÄ±, verilerle bir olmaktÄ±r. Bunu yapmanÄ±n en iyi yollarÄ±ndan biri, Ã¼zerinde Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ±z verileri mÃ¼mkÃ¼n olduÄŸunca gÃ¶rselleÅŸtirmektir.SlognamÄ±zÄ± unutmayalÄ±m: "gÃ¶rselleÅŸtir, gÃ¶rselleÅŸtir, gÃ¶rselleÅŸtir".

Bir DataFrame yaratma ile baÅŸlayalÄ±m.


```python
import pandas as pd

circles = pd.DataFrame({"X0":X[:, 0], "X1":X[:, 1], "label":y})
circles.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>X0</th>
      <th>X1</th>
      <th>label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.754246</td>
      <td>0.231481</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-0.756159</td>
      <td>0.153259</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-0.815392</td>
      <td>0.173282</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-0.393731</td>
      <td>0.692883</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.442208</td>
      <td>-0.896723</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>




```python
# etiketimize gÃ¶re veri sayÄ±larÄ±mÄ±z
circles.label.value_counts()
```




    1    500
    0    500
    Name: label, dtype: int64



Pekala, bir ikili (binary) sÄ±nÄ±flandÄ±rma problemiyle uÄŸraÅŸÄ±yoruz gibi gÃ¶rÃ¼nÃ¼yor. Ä°kilidir Ã§Ã¼nkÃ¼ yalnÄ±zca iki etiket vardÄ±r (0 veya 1).

Daha fazla etiket seÃ§eneÄŸi olsaydÄ± (Ã¶r. 0, 1, 2, 3 veya 4), Ã§ok sÄ±nÄ±flÄ± (multi-label) sÄ±nÄ±flandÄ±rma olarak adlandÄ±rÄ±lÄ±rdÄ±.

GÃ¶rselleÅŸtirmemizi bir adÄ±m daha ileri gÃ¶tÃ¼relim ve verilerimizi Ã§izelim.


```python
import matplotlib.pyplot as plt

plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu);
```


    
![png](TensorFlow_ile_Sinir_A%C4%9F%C4%B1_S%C4%B1n%C4%B1fland%C4%B1r%C4%B1lmas%C4%B1_files/TensorFlow_ile_Sinir_A%C4%9F%C4%B1_S%C4%B1n%C4%B1fland%C4%B1r%C4%B1lmas%C4%B1_10_0.png)
    


Maviyi kÄ±rmÄ±zÄ± noktalardan ayÄ±rt edebilen bir model yaratalÄ±m.

## Girdi (Input) ve Ã‡Ä±kÄ±ÅŸ (Output) Åekilleri

Sinir aÄŸlarÄ± oluÅŸtururken karÅŸÄ±laÅŸacaÄŸÄ±nÄ±z en yaygÄ±n sorunlardan biri ÅŸekil uyumsuzluklarÄ±dÄ±r.

Daha spesifik olarak, girdi verilerinin ÅŸekli ve Ã§Ä±ktÄ± verilerinin ÅŸekli.

Bizim durumumuzda, X'i girmek ve modelimizin y'yi tahmin etmesini saÄŸlamak istiyoruz.

Åimdi X ve y'nin ÅŸekillerini kontrol edelim.



```python
# Ã–zelliklerimizin ve etiketlerimizin ÅŸekillerini kontrol edelim
X.shape, y.shape
```




    ((1000, 2), (1000,))



Hmm, bu rakamlar nereden geliyor?


```python
# KaÃ§ Ã¶rneÄŸimiz olduÄŸunu kontrol edelim
len(X), len(y)
```




    (1000, 1000)



Yani y deÄŸeri kadar X deÄŸerimiz var, bu mantÄ±klÄ±.

Her birinin bir Ã¶rneÄŸini inceleyelim.


```python
X[0], y[0]
```




    (array([0.75424625, 0.23148074]), 1)



Bir y deÄŸeri sonucunu veren iki X Ã¶zelliÄŸimiz var.

Bu, sinir aÄŸÄ±mÄ±zÄ±n girdi ÅŸeklinin en az bir boyutu iki olan bir tensÃ¶rÃ¼ kabul etmesi ve en az bir deÄŸere sahip bir tensÃ¶r Ã§Ä±kÄ±ÅŸÄ± vermesi gerektiÄŸi anlamÄ±na gelir.

> ğŸ¤” Not: (1000,) ÅŸeklinde bir y olmasÄ± kafa karÄ±ÅŸtÄ±rÄ±cÄ± gÃ¶rÃ¼nebilir. Ancak bunun nedeni, tÃ¼m y deÄŸerlerinin aslÄ±nda skaler (tek deÄŸerler) olmasÄ± ve bu nedenle bir boyutu olmamasÄ±dÄ±r. Åimdilik, Ã§Ä±ktÄ± ÅŸeklinizin en azÄ±ndan bir y Ã¶rneÄŸiyle aynÄ± deÄŸerde olduÄŸunu dÃ¼ÅŸÃ¼nÃ¼n (bizim durumumuzda, sinir aÄŸÄ±mÄ±zÄ±n Ã§Ä±ktÄ±sÄ± en az bir deÄŸer olmalÄ±dÄ±r).

## Modellemedeki adÄ±mlar

ArtÄ±k elimizde hangi verilere ve girdi ve Ã§Ä±ktÄ± ÅŸekillerine sahip olduÄŸumuzu biliyoruz, onu modellemek iÃ§in nasÄ±l bir sinir aÄŸÄ± kuracaÄŸÄ±mÄ±za bakalÄ±m.

TensorFlow'da bir model oluÅŸturmak ve eÄŸitmek iÃ§in tipik olarak 3 temel adÄ±m vardÄ±r.

- **Bir model oluÅŸturma**<br>
Bir sinir aÄŸÄ±nÄ±n katmanlarÄ±nÄ± kendiniz bir araya getirin (iÅŸlevsel veya sÄ±ralÄ± API'yi kullanarak) veya Ã¶nceden oluÅŸturulmuÅŸ bir modeli iÃ§e aktarÄ±n (aktarÄ±m Ã¶ÄŸrenimi (transfer learning) olarak bilinir).
- **Bir model derleme**<br>
Bir modelin performansÄ±nÄ±n nasÄ±l Ã¶lÃ§Ã¼leceÄŸini (kayÄ±p/metrikler) tanÄ±mlamanÄ±n yanÄ± sÄ±ra nasÄ±l iyileÅŸtirileceÄŸini (optimizer) tanÄ±mlama.
- **Modeli fit etme**<br>
Modelin verilerdeki kalÄ±plarÄ± bulmaya Ã§alÄ±ÅŸmasÄ±na izin vermek (X, y'ye nasÄ±l ulaÅŸÄ±r).

SÄ±ralÄ± API'yi kullanarak bunlarÄ± Ã§alÄ±ÅŸÄ±rken gÃ¶relim. Ve sonra her birinin Ã¼zerinden geÃ§eceÄŸiz.


```python
tf.random.set_seed(42)

# 1. SÄ±ralÄ± API'yi kullanarak modeli oluÅŸturma
model_1 = tf.keras.Sequential([
  tf.keras.layers.Dense(1)
])

# Modeli derleme
model_1.compile(loss=tf.keras.losses.BinaryCrossentropy(),
                optimizer=tf.keras.optimizers.SGD(),
                metrics=['accuracy'])

# Modeli fit etme
model_1.fit(X, y, epochs=5)
```

    Epoch 1/5
    32/32 [==============================] - 1s 1ms/step - loss: 2.8544 - accuracy: 0.4600
    Epoch 2/5
    32/32 [==============================] - 0s 1ms/step - loss: 0.7131 - accuracy: 0.5430
    Epoch 3/5
    32/32 [==============================] - 0s 1ms/step - loss: 0.6973 - accuracy: 0.5090
    Epoch 4/5
    32/32 [==============================] - 0s 2ms/step - loss: 0.6950 - accuracy: 0.5010
    Epoch 5/5
    32/32 [==============================] - 0s 1ms/step - loss: 0.6942 - accuracy: 0.4830





    <tensorflow.python.keras.callbacks.History at 0x7f64a380b710>



Accuracy metriÄŸine bakÄ±ldÄ±ÄŸÄ±nda, modelimiz zayÄ±f bir performans sergiliyor (ikili sÄ±nÄ±flandÄ±rma probleminde %50 doÄŸruluk, tahmin etmeye eÅŸdeÄŸerdir), ama ya onu daha uzun sÃ¼re eÄŸitirsek?


```python
# Modelimizi daha uzun sÃ¼re eÄŸitme
model_1.fit(X, y, epochs=200, verbose=0)
model_1.evaluate(X, y)
```

    32/32 [==============================] - 0s 1ms/step - loss: 0.6935 - accuracy: 0.5000





    [0.6934829950332642, 0.5]



Verilerin 200 geÃ§iÅŸinden sonra bile, hala tahmin ediyormuÅŸ gibi performans gÃ¶steriyor.

Fazladan bir katman ekleyip biraz daha uzun sÃ¼re eÄŸitsek ne olur?


```python
tf.random.set_seed(42)

# 1. SÄ±ralÄ± API'yi kullanarak modeli oluÅŸturma
model_2 = tf.keras.Sequential([
  tf.keras.layers.Dense(1), # ek olarak bir katman ekleme
  tf.keras.layers.Dense(1)
])

# Modeli derleme
model_2.compile(loss=tf.keras.losses.BinaryCrossentropy(),
                optimizer=tf.keras.optimizers.SGD(),
                metrics=['accuracy'])

# Modeli fit etme
model_2.fit(X, y, epochs=100, verbose=0)
```




    <tensorflow.python.keras.callbacks.History at 0x7f64a14ff3d0>




```python
# modeli deÄŸerlendirme
model_2.evaluate(X, y)
```

    32/32 [==============================] - 0s 1ms/step - loss: 0.6933 - accuracy: 0.5000





    [0.6933314800262451, 0.5]



## Bir Modeli GeliÅŸtirmek

Modelimizi geliÅŸtirmek iÃ§in daha Ã¶nce geÃ§tiÄŸimiz 3 adÄ±mÄ±n neredeyse her bÃ¶lÃ¼mÃ¼nÃ¼ deÄŸiÅŸtirebiliriz.

- **Bir model oluÅŸturma** <br>
Burada daha fazla katman eklemek, her katmandaki gizli birimlerin (nÃ¶ronlar olarak da adlandÄ±rÄ±lÄ±r) sayÄ±sÄ±nÄ± artÄ±rmak, her katmanÄ±n etkinleÅŸtirme iÅŸlevlerini deÄŸiÅŸtirmek isteyebilirsiniz.
- **Bir model derleme** <br>
FarklÄ± bir optimizasyon iÅŸlevi (genellikle birÃ§ok sorun iÃ§in oldukÃ§a iyi olan Adam optimize edici gibi) seÃ§mek veya belki de optimizasyon iÅŸlevinin Ã¶ÄŸrenme oranÄ±nÄ± deÄŸiÅŸtirmek isteyebilirsiniz.
- **Bir modeli fit etme** <br>
Belki de bir modeli daha fazla epoch'a sÄ±ÄŸdÄ±rabilirsiniz (onu daha uzun sÃ¼re eÄŸitmeye bÄ±rakÄ±n).

Daha fazla nÃ¶ron, fazladan bir katman ve Adam optimize edici eklemeye ne dersiniz?

Elbette bunu yapmak tahmin etmekten daha iyidir...


```python
tf.random.set_seed(42)

# 1. SÄ±ralÄ± API'yi kullanarak modeli oluÅŸturma
model_3 = tf.keras.Sequential([
  tf.keras.layers.Dense(100), # 100 yoÄŸun nÃ¶ron ekleyin
  tf.keras.layers.Dense(10), # 10 nÃ¶ronlu baÅŸka bir katman ekleyin
  tf.keras.layers.Dense(1)
])

# Modeli derleme
model_3.compile(loss=tf.keras.losses.BinaryCrossentropy(),
                optimizer=tf.keras.optimizers.Adam(),
                metrics=['accuracy'])

# Modeli fit etme
model_3.fit(X, y, epochs=100, verbose=0)
```




    <tensorflow.python.keras.callbacks.History at 0x7f64a24d2c10>




```python
#  Modeli deÄŸerlendirme
model_3.evaluate(X, y)
```

    32/32 [==============================] - 0s 1ms/step - loss: 0.6980 - accuracy: 0.5080





    [0.6980254650115967, 0.5080000162124634]



BirkaÃ§ numara Ã§Ä±kardÄ±k ama modelimiz tahmin etmekten bile daha iyi deÄŸil.

Neler olduÄŸunu gÃ¶rmek iÃ§in bazÄ± gÃ¶rselleÅŸtirmeler yapalÄ±m.

> ğŸ”‘ Not: Modeliniz garip bir ÅŸekilde performans gÃ¶sterdiÄŸinde veya verilerinizle ilgili tam olarak emin olmadÄ±ÄŸÄ±nÄ±z bir ÅŸeyler olduÄŸunda, ÅŸu Ã¼Ã§ kelimeyi hatÄ±rlayÄ±n: gÃ¶rselleÅŸtir, gÃ¶rselleÅŸtir, gÃ¶rselleÅŸtir. Verilerinizi inceleyin, modelinizi inceleyin, modelinizin tahminlerini inceleyin.

Modelimizin tahminlerini gÃ¶rselleÅŸtirmek iÃ§in bir `plot_decision_boundary()` fonksiyonu oluÅŸturacaÄŸÄ±z ve bu fonksiyon:

EÄŸitilmiÅŸ bir modeli, Ã¶zellikleri (X) ve etiketleri (y) alÄ±r.
- FarklÄ± X deÄŸerlerinden oluÅŸan bir aÄŸ Ä±zgarasÄ± oluÅŸturur.
- [Meshgrid](https://numpy.org/doc/stable/reference/generated/numpy.meshgrid.html) Ã¼zerinden tahminler yapar.
- Tahminleri ve farklÄ± bÃ¶lgeler (her benzersiz sÄ±nÄ±fÄ±n dÃ¼ÅŸtÃ¼ÄŸÃ¼ yer) arasÄ±nda bir Ã§izgi Ã§izer.

> ğŸ”‘ Not: Bir iÅŸlevin ne yaptÄ±ÄŸÄ±ndan emin deÄŸilseniz, ne yaptÄ±ÄŸÄ±nÄ± gÃ¶rmek iÃ§in onu Ã§Ã¶zmeyi ve satÄ±r satÄ±r yazmayÄ± deneyin. KÃ¼Ã§Ã¼k parÃ§alara ayÄ±rÄ±n ve her bir parÃ§anÄ±n Ã§Ä±ktÄ±sÄ±nÄ± gÃ¶rÃ¼n.


```python
import numpy as np

def plot_decision_boundary(model, X, y):

  # Ã‡izimin eksen sÄ±nÄ±rlarÄ±nÄ± tanÄ±mlayÄ±n ve bir aÄŸ Ä±zgarasÄ± oluÅŸturun
  x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1
  y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1
  xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),
                       np.linspace(y_min, y_max, 100))
  
  # x deÄŸerlerini yaratÄ±n
  x_in = np.c_[xx.ravel(), yy.ravel()] 
  
  # EÄŸitilmiÅŸ modeli kullanarak tahminler yapÄ±n
  y_pred = model.predict(x_in)

  if len(y_pred[0]) > 1:
    print("doing multiclass classification...")
    y_pred = np.argmax(y_pred, axis=1).reshape(xx.shape)
  else:
    print("doing binary classifcation...")
    y_pred = np.round(y_pred).reshape(xx.shape)
  
  plt.contourf(xx, yy, y_pred, cmap=plt.cm.RdYlBu, alpha=0.7)
  plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu)
  plt.xlim(xx.min(), xx.max())
  plt.ylim(yy.min(), yy.max())
```


```python
# Modelimizin yaptÄ±ÄŸÄ± tahminlere gÃ¶z atalÄ±m
plot_decision_boundary(model_3, X, y)
```

    doing binary classifcation...



    
![png](TensorFlow_ile_Sinir_A%C4%9F%C4%B1_S%C4%B1n%C4%B1fland%C4%B1r%C4%B1lmas%C4%B1_files/TensorFlow_ile_Sinir_A%C4%9F%C4%B1_S%C4%B1n%C4%B1fland%C4%B1r%C4%B1lmas%C4%B1_31_1.png)
    


GÃ¶rÃ¼nÃ¼ÅŸe gÃ¶re modelimiz veriler arasÄ±nda dÃ¼z bir Ã§izgi Ã§izmeye Ã§alÄ±ÅŸÄ±yor. 

DÃ¼z Ã§izgi Ã§izmeye Ã§alÄ±ÅŸmasÄ±nÄ±n hatasÄ± ne? <br>
Ana sorun, verilerimizin dÃ¼z bir Ã§izgiyle ayrÄ±lamamasÄ±dÄ±r. EÄŸer verilerimiz bir regresyon tanÄ±mlasaydÄ± modelimiz doÄŸru bir yÃ¶ntem uygulamÄ±ÅŸ olacaktÄ±. Hadi deneyelim


```python
tf.random.set_seed(42)

# regresyon verisi yaratma
X_regression = np.arange(0, 1000, 5)
y_regression = np.arange(100, 1100, 5)

# EÄŸitim ve test setlerine ayÄ±rma
X_reg_train = X_regression[:150]
X_reg_test = X_regression[150:]
y_reg_train = y_regression[:150]
y_reg_test = y_regression[150:]

# modeli yeni veriyle ffit etme
model_3.fit(X_reg_train, y_reg_train, epochs=100)
```

    Epoch 1/100



    ---------------------------------------------------------------------------

    ValueError                                Traceback (most recent call last)

    <ipython-input-18-87efe03ef25e> in <module>()
         12 
         13 # modeli yeni veriyle ffit etme
    ---> 14 model_3.fit(X_reg_train, y_reg_train, epochs=100)
    

    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
       1181                 _r=1):
       1182               callbacks.on_train_batch_begin(step)
    -> 1183               tmp_logs = self.train_function(iterator)
       1184               if data_handler.should_sync:
       1185                 context.async_wait()


    /usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
        887 
        888       with OptionalXlaContext(self._jit_compile):
    --> 889         result = self._call(*args, **kwds)
        890 
        891       new_tracing_count = self.experimental_get_tracing_count()


    /usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)
        915       # In this case we have created variables on the first call, so we run the
        916       # defunned version which is guaranteed to never create variables.
    --> 917       return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable
        918     elif self._stateful_fn is not None:
        919       # Release the lock early so that multiple threads can perform the call


    /usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)
       3020     with self._lock:
       3021       (graph_function,
    -> 3022        filtered_flat_args) = self._maybe_define_function(args, kwargs)
       3023     return graph_function._call_flat(
       3024         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access


    /usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
       3439               call_context_key in self._function_cache.missed):
       3440             return self._define_function_with_shape_relaxation(
    -> 3441                 args, kwargs, flat_args, filtered_flat_args, cache_key_context)
       3442 
       3443           self._function_cache.missed.add(call_context_key)


    /usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py in _define_function_with_shape_relaxation(self, args, kwargs, flat_args, filtered_flat_args, cache_key_context)
       3361 
       3362     graph_function = self._create_graph_function(
    -> 3363         args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)
       3364     self._function_cache.arg_relaxed[rank_only_cache_key] = graph_function
       3365 


    /usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
       3287             arg_names=arg_names,
       3288             override_flat_arg_shapes=override_flat_arg_shapes,
    -> 3289             capture_by_value=self._capture_by_value),
       3290         self._function_attributes,
       3291         function_spec=self.function_spec,


    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
        997         _, original_func = tf_decorator.unwrap(python_func)
        998 
    --> 999       func_outputs = python_func(*func_args, **func_kwargs)
       1000 
       1001       # invariant: `func_outputs` contains only Tensors, CompositeTensors,


    /usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)
        670         # the function a weak reference to itself to avoid a reference cycle.
        671         with OptionalXlaContext(compile_with_xla):
    --> 672           out = weak_wrapped_fn().__wrapped__(*args, **kwds)
        673         return out
        674 


    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
        984           except Exception as e:  # pylint:disable=broad-except
        985             if hasattr(e, "ag_error_metadata"):
    --> 986               raise e.ag_error_metadata.to_exception(e)
        987             else:
        988               raise


    ValueError: in user code:
    
        /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:855 train_function  *
            return step_function(self, iterator)
        /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:845 step_function  **
            outputs = model.distribute_strategy.run(run_step, args=(data,))
        /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:1285 run
            return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
        /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2833 call_for_each_replica
            return self._call_for_each_replica(fn, args, kwargs)
        /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:3608 _call_for_each_replica
            return fn(*args, **kwargs)
        /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:838 run_step  **
            outputs = model.train_step(data)
        /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:795 train_step
            y_pred = self(x, training=True)
        /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py:1013 __call__
            input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)
        /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/input_spec.py:255 assert_input_compatibility
            ' but received input with shape ' + display_shape(x.shape))
    
        ValueError: Input 0 of layer sequential_2 is incompatible with the layer: expected axis -1 of input shape to have value 2 but received input with shape (None, 1)



Hata almak mÄ±? Fark ettiniz deÄŸil mi? Modelimiz sÄ±nÄ±flandÄ±rma iÃ§in eÄŸitilmiÅŸ bir model. Åimdi o modeli regresyon iÃ§in eÄŸitelim.


```python
tf.random.set_seed(42)

# Modeli yeniden oluÅŸturma
model_3 = tf.keras.Sequential([
  tf.keras.layers.Dense(100),
  tf.keras.layers.Dense(10),
  tf.keras.layers.Dense(1)
])

# derlenmiÅŸ modelimizin kaybÄ±nÄ± ve Ã¶lÃ§Ã¼mlerini deÄŸiÅŸtirme
model_3.compile(loss=tf.keras.losses.mae, 
                optimizer=tf.keras.optimizers.Adam(),
                metrics=['mae']) 

# modeli fit etme
model_3.fit(X_reg_train, y_reg_train, epochs=100, verbose=0)
```




    <tensorflow.python.keras.callbacks.History at 0x7f64a1c3abd0>



Tamam, modelimiz bir ÅŸeyler Ã¶ÄŸreniyor gibi gÃ¶rÃ¼nÃ¼yor (mae deÄŸeri her epoch'ta aÅŸaÄŸÄ± doÄŸru eÄŸilim gÃ¶steriyor), hadi tahminlerini Ã§izelim.


```python
# EÄŸitimli modelimiz ile tahminler yapma
y_reg_preds = model_3.predict(y_reg_test)

# Modelin tahminlerini regresyon verilerimize gÃ¶re Ã§izme
plt.figure(figsize=(10, 7))
plt.scatter(X_reg_train, y_reg_train, c='b', label='Training data')
plt.scatter(X_reg_test, y_reg_test, c='g', label='Testing data')
plt.scatter(X_reg_test, y_reg_preds.squeeze(), c='r', label='Predictions')
plt.legend();
```


    
![png](TensorFlow_ile_Sinir_A%C4%9F%C4%B1_S%C4%B1n%C4%B1fland%C4%B1r%C4%B1lmas%C4%B1_files/TensorFlow_ile_Sinir_A%C4%9F%C4%B1_S%C4%B1n%C4%B1fland%C4%B1r%C4%B1lmas%C4%B1_37_0.png)
    


Tahminler mÃ¼kemmel olmamakla birlikte fena da sayÄ±lmaz. Yani bu, modelimizin bir ÅŸeyler Ã¶ÄŸreniyor olmasÄ± gerektiÄŸi anlamÄ±na geliyor. SÄ±nÄ±flandÄ±rma problemimiz iÃ§in gÃ¶zden kaÃ§Ä±rdÄ±ÄŸÄ±mÄ±z bir ÅŸey olmalÄ±.

## DoÄŸrusal Olmama (Non-linearity)

Tamam, sinir aÄŸÄ±mÄ±zÄ±n dÃ¼z Ã§izgileri modelleyebildiÄŸini gÃ¶rdÃ¼k. DÃ¼z olmayan (doÄŸrusal olmayan) Ã§izgiler ne olacak?

SÄ±nÄ±flandÄ±rma verilerimizi (kÄ±rmÄ±zÄ± ve mavi daireleri) modelleyeceksek, bazÄ± doÄŸrusal olmayan Ã§izgilere ihtiyacÄ±mÄ±z olacak.


```python
tf.random.set_seed(42)

# bir model yaratma
model_5 = tf.keras.Sequential([
  tf.keras.layers.Dense(1, activation=tf.keras.activations.linear), # 1 lineer aktivasyonlu gizli katman
  tf.keras.layers.Dense(1) # Ã§Ä±ktÄ± katmanÄ±
])

# modeli derleme
model_5.compile(loss=tf.keras.losses.binary_crossentropy,
                optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
                metrics=["accuracy"])

# modeli fit etme
history = model_5.fit(X, y, epochs=100, verbose=0)
```

Model Ã§Ä±ktÄ±sÄ±na ayrÄ±ntÄ±lÄ± olarak bakarsanÄ±z modelin hala Ã¶ÄŸrenmediÄŸini gÃ¶receksiniz. NÃ¶ron ve katmanlarÄ±n sayÄ±sÄ±nÄ± artÄ±rÄ±rsak sizce Ã¶ÄŸrenmesini saÄŸlayabilir miyiz?


```python
tf.random.set_seed(42)

# bir model yaratma
model_6 = tf.keras.Sequential([
  tf.keras.layers.Dense(4, activation=tf.keras.activations.relu), # gizli katmanÄ± 1, 4 nÃ¶ronlu bir relu aktivasyonu
  tf.keras.layers.Dense(4, activation=tf.keras.activations.relu), # gizli katmanÄ± 2, 4 nÃ¶ronlu bir relu aktivasyonu
  tf.keras.layers.Dense(1) # Ã§Ä±ktÄ± katmanÄ±
])

# modeli derleme
model_6.compile(loss=tf.keras.losses.binary_crossentropy,
                optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
                metrics=["accuracy"])

# modeli fit etme
history = model_6.fit(X, y, epochs=100, verbose=0)
```


```python
# modeli deÄŸerlendirme
model_6.evaluate(X, y)
```

    32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000





    [7.712474346160889, 0.5]



%50 doÄŸruluÄŸa ulaÅŸÄ±yoruz, modelimiz hala berbat sonuÃ§lar veriyor.

Tahminler nasÄ±l gÃ¶rÃ¼nÃ¼yor?


```python
# 2 gizli katman kullanarak tahminlere gÃ¶z atÄ±n
plot_decision_boundary(model_6, X, y)
```

    doing binary classifcation...



    
![png](TensorFlow_ile_Sinir_A%C4%9F%C4%B1_S%C4%B1n%C4%B1fland%C4%B1r%C4%B1lmas%C4%B1_files/TensorFlow_ile_Sinir_A%C4%9F%C4%B1_S%C4%B1n%C4%B1fland%C4%B1r%C4%B1lmas%C4%B1_45_1.png)
    


Ä°deal olarak, sarÄ± Ã§izgiler kÄ±rmÄ±zÄ± dairenin ve mavi dairenin iÃ§ kÄ±smÄ±na gider.

Tamam, hadi bu daireyi bir kereliÄŸine modelleyelim.

Bir model daha (sÃ¶z veriyorum... aslÄ±nda, bu sÃ¶zÃ¼ bozmak zorunda kalacaÄŸÄ±m... daha birÃ§ok model Ã¼reteceÄŸiz).

Bu sefer Ã§Ä±ktÄ± katmanÄ±mÄ±zdaki aktivasyon fonksiyonunu da deÄŸiÅŸtireceÄŸiz. Bir sÄ±nÄ±flandÄ±rma modelinin mimarisini hatÄ±rlÄ±yor musunuz? Ä°kili sÄ±nÄ±flandÄ±rma iÃ§in, Ã§Ä±ktÄ± katmanÄ± aktivasyonu genellikle Sigmoid aktivasyon fonksiyonudur.


```python
tf.random.set_seed(42)

# modeli yaratma
model_7 = tf.keras.Sequential([
  tf.keras.layers.Dense(4, activation=tf.keras.activations.relu),
  tf.keras.layers.Dense(4, activation=tf.keras.activations.relu),
  tf.keras.layers.Dense(1, activation=tf.keras.activations.sigmoid)
  ])

# modeli derleme
model_7.compile(loss=tf.keras.losses.binary_crossentropy,
                optimizer=tf.keras.optimizers.Adam(),
                metrics=['accuracy'])

# modeli fit etme
history = model_7.fit(X, y, epochs=100, verbose=0)
```


```python
# modeli deÄŸerlendirme
model_7.evaluate(X, y)
```

    32/32 [==============================] - 0s 1ms/step - loss: 0.2948 - accuracy: 0.9910





    [0.2948004901409149, 0.9909999966621399]



SÃ¼per. Modelimiz harika bir accuracy deÄŸeri verdi. GÃ¶rselleÅŸtirip nasÄ±l gÃ¶rÃ¼ndÃ¼ÄŸÃ¼ne bakalÄ±m hemen.


```python
plot_decision_boundary(model_7, X, y)
```

    doing binary classifcation...



    
![png](TensorFlow_ile_Sinir_A%C4%9F%C4%B1_S%C4%B1n%C4%B1fland%C4%B1r%C4%B1lmas%C4%B1_files/TensorFlow_ile_Sinir_A%C4%9F%C4%B1_S%C4%B1n%C4%B1fland%C4%B1r%C4%B1lmas%C4%B1_50_1.png)
    


Yapmak istediÄŸimiz sÄ±nÄ±flandÄ±rma iÅŸlemi gerÃ§ekleÅŸmiÅŸ. Mavi ve kÄ±rmÄ±zÄ±yÄ± ayÄ±rmayÄ± baÅŸardÄ±k. 

> ğŸ¤” Soru: YaptÄ±ÄŸÄ±mÄ±z tahminlerde yanlÄ±ÅŸ olan ne? Burada modelimizi gerÃ§ekten doÄŸru deÄŸerlendiriyor muyuz? Ä°pucu: Model hangi verileri Ã¶ÄŸrendi ve biz neyi tahmin ettik?

Buna cevap vermeden Ã¶nce, az Ã¶nce ele aldÄ±ÄŸÄ±mÄ±z ÅŸeyin farkÄ±na varmak Ã¶nemlidir.

> ğŸ”‘ Not: DoÄŸrusal (dÃ¼z Ã§izgiler) ve doÄŸrusal olmayan (dÃ¼z olmayan Ã§izgiler) iÅŸlevlerin birleÅŸimi, sinir aÄŸlarÄ±nÄ±n temel temellerinden biridir.

Bunu ÅŸÃ¶yle dÃ¼ÅŸÃ¼nÃ¼n:

Size sÄ±nÄ±rsÄ±z sayÄ±da dÃ¼z Ã§izgi ve dÃ¼z olmayan Ã§izgi vermiÅŸ olsaydÄ±m, ne tÃ¼r desenler Ã§izebilirdiniz? Esasen sinir aÄŸlarÄ±nÄ±n verilerdeki kalÄ±plarÄ± bulmak iÃ§in yaptÄ±ÄŸÄ± ÅŸey budur.

KullandÄ±ÄŸÄ±mÄ±z aktivasyon fonksiyonlarÄ± sayesinde hedeflediÄŸimiz modele ulaÅŸdÄ±k. Åimdi fikir edinmek adÄ±na aktivasyon fonksiyonlarÄ± oluÅŸturup, oluÅŸturduÄŸumuz bu fonksiyonlarÄ±n sonuÃ§larÄ±nÄ± deÄŸerlendirelim.


```python
A = tf.cast(tf.range(-10, 10), tf.float32)
A
```




    <tf.Tensor: shape=(20,), dtype=float32, numpy=
    array([-10.,  -9.,  -8.,  -7.,  -6.,  -5.,  -4.,  -3.,  -2.,  -1.,   0.,
             1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.],
          dtype=float32)>




```python
# GÃ¶rselleÅŸtirelim
plt.plot(A);
```


    
![png](TensorFlow_ile_Sinir_A%C4%9F%C4%B1_S%C4%B1n%C4%B1fland%C4%B1r%C4%B1lmas%C4%B1_files/TensorFlow_ile_Sinir_A%C4%9F%C4%B1_S%C4%B1n%C4%B1fland%C4%B1r%C4%B1lmas%C4%B1_53_0.png)
    


DÃ¼z (doÄŸrusal) bir Ã§izgi! GÃ¼zel, ÅŸimdi sigmoid fonksiyonunu ile bu modeli yeniden oluÅŸturalÄ±m ve verilerimize ne yaptÄ±ÄŸÄ±nÄ± gÃ¶relim. 


```python
# Sigmoid - https://www.tensorflow.org/api_docs/python/tf/keras/activations/sigmoid
def sigmoid(x):
  return 1 / (1 + tf.exp(-x))

sigmoid(A)
```




    <tf.Tensor: shape=(20,), dtype=float32, numpy=
    array([4.5397872e-05, 1.2339458e-04, 3.3535014e-04, 9.1105117e-04,
           2.4726233e-03, 6.6928510e-03, 1.7986210e-02, 4.7425874e-02,
           1.1920292e-01, 2.6894143e-01, 5.0000000e-01, 7.3105860e-01,
           8.8079703e-01, 9.5257413e-01, 9.8201376e-01, 9.9330717e-01,
           9.9752742e-01, 9.9908900e-01, 9.9966466e-01, 9.9987662e-01],
          dtype=float32)>




```python
# sigmoid fonksiyona soktuÄŸumuz deÄŸerlerin Ã§Ä±ktÄ±sÄ±nÄ± gÃ¶rselleÅŸtirelim
plt.plot(sigmoid(A));
```


    
![png](TensorFlow_ile_Sinir_A%C4%9F%C4%B1_S%C4%B1n%C4%B1fland%C4%B1r%C4%B1lmas%C4%B1_files/TensorFlow_ile_Sinir_A%C4%9F%C4%B1_S%C4%B1n%C4%B1fland%C4%B1r%C4%B1lmas%C4%B1_56_0.png)
    


DÃ¼z olmayan (doÄŸrusal olmayan) bir Ã§izgi!

Tamam, ReLU iÅŸlevine ne dersiniz (ReLU tÃ¼m negatifleri 0'a Ã§evirir ve pozitif sayÄ±lar aynÄ± kalÄ±r)?


```python
# ReLU - https://www.tensorflow.org/api_docs/python/tf/keras/activations/relu
def relu(x):
  return tf.maximum(0, x)

relu(A)
```




    <tf.Tensor: shape=(20,), dtype=float32, numpy=
    array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 2., 3., 4., 5., 6.,
           7., 8., 9.], dtype=float32)>




```python
# relu fonksiyona soktuÄŸumuz deÄŸerlerin Ã§Ä±ktÄ±sÄ±nÄ± gÃ¶rselleÅŸtirelim
plt.plot(relu(A));
```


    
![png](TensorFlow_ile_Sinir_A%C4%9F%C4%B1_S%C4%B1n%C4%B1fland%C4%B1r%C4%B1lmas%C4%B1_files/TensorFlow_ile_Sinir_A%C4%9F%C4%B1_S%C4%B1n%C4%B1fland%C4%B1r%C4%B1lmas%C4%B1_59_0.png)
    


DÃ¼z olmayan bir Ã§izgi daha!

Peki, TensorFlow'un lineer aktivasyon fonksiyonuna ne dersiniz?



```python
tf.keras.activations.linear(A)
```




    <tf.Tensor: shape=(20,), dtype=float32, numpy=
    array([-10.,  -9.,  -8.,  -7.,  -6.,  -5.,  -4.,  -3.,  -2.,  -1.,   0.,
             1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.],
          dtype=float32)>



GirdiÄŸimiz deÄŸerler olduÄŸu gibi Ã§Ä±ktÄ±. Bunu kontrol edelim.


```python
A == tf.keras.activations.linear(A)
```




    <tf.Tensor: shape=(20,), dtype=bool, numpy=
    array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
            True,  True,  True,  True,  True,  True,  True,  True,  True,
            True,  True])>



Tamam, bu nedenle, modelin yalnÄ±zca doÄŸrusal etkinleÅŸtirme fonksiyonlarÄ±nÄ± kullanÄ±rken gerÃ§ekten hiÃ§bir ÅŸey Ã¶ÄŸrenmemesi mantÄ±klÄ±dÄ±r, Ã§Ã¼nkÃ¼ doÄŸrusal etkinleÅŸtirme iÅŸlevi, giriÅŸ verilerimizi hiÃ§bir ÅŸekilde deÄŸiÅŸtirmez.

DoÄŸrusal olmayan fonksiyonlarÄ±mÄ±zla verilerimiz manipÃ¼le edilir. Bir sinir aÄŸÄ±, girdileri ve Ã§Ä±ktÄ±larÄ± arasÄ±ndaki desenleri Ã§izmek iÃ§in bu tÃ¼r dÃ¶nÃ¼ÅŸÃ¼mleri bÃ¼yÃ¼k Ã¶lÃ§ekte kullanÄ±r.

Åimdi, sinir aÄŸlarÄ±nÄ±n derinlerine dalmak yerine, Ã¶ÄŸrendiklerimizi farklÄ± problemlere uygulayarak kodlamaya devam edeceÄŸiz, ancak sahne arkasÄ±nda neler olup bittiÄŸine daha derinlemesine bakmak istiyorsanÄ±z, aÅŸaÄŸÄ±daki bÃ¶lÃ¼me gÃ¶z atabilirsiniz.

> ğŸ“– Kaynak: Aktivasyon iÅŸlevleri hakkÄ±nda daha fazla bilgi iÃ§in, bunlarla ilgili [makine Ã¶ÄŸrenimi sayfasÄ±na ](https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#)bakÄ±n.

## SÄ±nÄ±flandÄ±rma Modelimizin DeÄŸerlendirilmesi Ve Ä°yileÅŸtirilmesi

YukarÄ±daki soruya cevap verdiyseniz, yanlÄ±ÅŸ yaptÄ±ÄŸÄ±mÄ±zÄ± anlamÄ±ÅŸ olabilirsiniz. Modelimizi, train ettiÄŸimiz aynÄ± veriler Ã¼zerinde deÄŸerlendiriyorduk.

Verilerimizi train, validation (isteÄŸe baÄŸlÄ±) ve test kÃ¼melerine bÃ¶lmek daha iyi bir yaklaÅŸÄ±m olacaktÄ±r.

Bunu yaptÄ±ktan sonra, modelimizi train setinde eÄŸiteceÄŸiz (verilerdeki desenleri bulmasÄ±na izin verin) ve ardÄ±ndan test setindeki deÄŸerleri tahmin etmek iÃ§in kullanarak modelleri ne kadar iyi Ã¶ÄŸrendiÄŸini gÃ¶receÄŸiz.

Hadi yapalÄ±m.


```python
# TÃ¼m veri setinde kaÃ§ Ã¶rnek var?
len(X)
```




    1000




```python
# Verileri train ve test setlerine ayÄ±rÄ±n
X_train, y_train = X[:800], y[:800] 
X_test, y_test = X[800:], y[800:] 

# Verilerin ÅŸekillerini kontrol edin
X_train.shape, X_test.shape 
```




    ((800, 2), (200, 2))



Harika, ÅŸimdi train ve test setlerimiz var, hadi train verilerini modelleyelim ve modelimizin test setinde Ã¶ÄŸrendiklerini deÄŸerlendirelim.


```python
tf.random.set_seed(42)

# model yaratma
model_8 = tf.keras.Sequential([
  tf.keras.layers.Dense(4, activation="relu"),
  tf.keras.layers.Dense(4, activation="relu"),
  tf.keras.layers.Dense(1, activation="sigmoid") # binary output deÄŸeri aldÄ±ÄŸÄ± iÃ§in
])

# modeli derleme
model_8.compile(loss=tf.keras.losses.binary_crossentropy,
                optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),
                metrics=['accuracy'])

# modeli fit etme
history = model_8.fit(X_train, y_train, epochs=25)
```

    Epoch 1/25
    25/25 [==============================] - 0s 1ms/step - loss: 0.6847 - accuracy: 0.5425
    Epoch 2/25
    25/25 [==============================] - 0s 1ms/step - loss: 0.6777 - accuracy: 0.5525
    Epoch 3/25
    25/25 [==============================] - 0s 1ms/step - loss: 0.6736 - accuracy: 0.5512
    Epoch 4/25
    25/25 [==============================] - 0s 1ms/step - loss: 0.6681 - accuracy: 0.5775
    Epoch 5/25
    25/25 [==============================] - 0s 2ms/step - loss: 0.6633 - accuracy: 0.5850
    Epoch 6/25
    25/25 [==============================] - 0s 1ms/step - loss: 0.6546 - accuracy: 0.5838
    Epoch 7/25
    25/25 [==============================] - 0s 1ms/step - loss: 0.6413 - accuracy: 0.6750
    Epoch 8/25
    25/25 [==============================] - 0s 1ms/step - loss: 0.6264 - accuracy: 0.7013
    Epoch 9/25
    25/25 [==============================] - 0s 2ms/step - loss: 0.6038 - accuracy: 0.7487
    Epoch 10/25
    25/25 [==============================] - 0s 2ms/step - loss: 0.5714 - accuracy: 0.7738
    Epoch 11/25
    25/25 [==============================] - 0s 1ms/step - loss: 0.5404 - accuracy: 0.7650
    Epoch 12/25
    25/25 [==============================] - 0s 1ms/step - loss: 0.5015 - accuracy: 0.7837
    Epoch 13/25
    25/25 [==============================] - 0s 1ms/step - loss: 0.4683 - accuracy: 0.7975
    Epoch 14/25
    25/25 [==============================] - 0s 1ms/step - loss: 0.4113 - accuracy: 0.8450
    Epoch 15/25
    25/25 [==============================] - 0s 1ms/step - loss: 0.3625 - accuracy: 0.9125
    Epoch 16/25
    25/25 [==============================] - 0s 2ms/step - loss: 0.3209 - accuracy: 0.9312
    Epoch 17/25
    25/25 [==============================] - 0s 1ms/step - loss: 0.2847 - accuracy: 0.9488
    Epoch 18/25
    25/25 [==============================] - 0s 1ms/step - loss: 0.2597 - accuracy: 0.9525
    Epoch 19/25
    25/25 [==============================] - 0s 1ms/step - loss: 0.2375 - accuracy: 0.9563
    Epoch 20/25
    25/25 [==============================] - 0s 1ms/step - loss: 0.2135 - accuracy: 0.9663
    Epoch 21/25
    25/25 [==============================] - 0s 1ms/step - loss: 0.1938 - accuracy: 0.9775
    Epoch 22/25
    25/25 [==============================] - 0s 1ms/step - loss: 0.1752 - accuracy: 0.9737
    Epoch 23/25
    25/25 [==============================] - 0s 3ms/step - loss: 0.1619 - accuracy: 0.9787
    Epoch 24/25
    25/25 [==============================] - 0s 2ms/step - loss: 0.1550 - accuracy: 0.9775
    Epoch 25/25
    25/25 [==============================] - 0s 1ms/step - loss: 0.1490 - accuracy: 0.9762



```python
# Modelimizi test setinde deÄŸerlendirelim
loss, accuracy = model_8.evaluate(X_test, y_test)
print(f"Test setinde model kaybÄ± (loss): {loss}")
print(f"Test setinde model doÄŸruluÄŸu (accuracy): {100*accuracy:.2f}%")
```

    7/7 [==============================] - 0s 2ms/step - loss: 0.1247 - accuracy: 1.0000
    Test setinde model kaybÄ± (loss): 0.1246885135769844
    Test setinde model doÄŸruluÄŸu (accuracy): 100.00%


%100 doÄŸruluk? GÃ¼zel!

Åimdi model_8'i oluÅŸturmaya baÅŸladÄ±ÄŸÄ±mÄ±zda model_7 ile aynÄ± olacaÄŸÄ±nÄ± sÃ¶ylemiÅŸtik ama bunu biraz yalan bulmuÅŸ olabilirsiniz.

Ã‡Ã¼nkÃ¼ birkaÃ§ ÅŸeyi deÄŸiÅŸtirdik:

- **Aktivasyon parametresi**<br>
TensorFlow'da kitaplÄ±k yollarÄ± (tf.keras.activations.relu) yerine dizeler ("relu" & "sigmoid") kullandÄ±k, ikisi de aynÄ± iÅŸlevselliÄŸi sunar.
- **Learning_rate (ayrÄ±ca lr) parametresi**<br> 
Adam optimizer'deki Ã¶ÄŸrenme oranÄ± parametresini 0,001 yerine 0,01'e yÃ¼kselttik (10x artÄ±ÅŸ).
  - Ã–ÄŸrenme oranÄ±nÄ±, bir modelin ne kadar hÄ±zlÄ± Ã¶ÄŸrendiÄŸi olarak dÃ¼ÅŸÃ¼nebilirsiniz. Ã–ÄŸrenme oranÄ± ne kadar yÃ¼ksek olursa, modelin Ã¶ÄŸrenme kapasitesi o kadar hÄ±zlÄ± olur, ancak, bir modelin Ã§ok hÄ±zlÄ± Ã¶ÄŸrenmeye Ã§alÄ±ÅŸtÄ±ÄŸÄ± ve hiÃ§bir ÅŸey Ã¶ÄŸrenmediÄŸi, Ã§ok yÃ¼ksek bir Ã¶ÄŸrenme oranÄ± gibi bir ÅŸey vardÄ±r. (overfitting ve underfitting kavramlarÄ±)
- **Epoch sayÄ±sÄ±**<br>
Epoch sayÄ±sÄ±nÄ± (epochs parametresini kullanarak) 100'den 25'e dÃ¼ÅŸÃ¼rdÃ¼k, ancak modelimiz hem eÄŸitim hem de test setlerinde hala inanÄ±lmaz bir sonuÃ§ aldÄ±.
  - Modelimizin eskisinden bile daha az epoch sayÄ±sÄ± ile iyi performans gÃ¶stermesinin nedenlerinden biri (tek bir epoch, modelin verideki kalÄ±plarÄ± bir kez bakarak Ã¶ÄŸrenmeye Ã§alÄ±ÅŸmasÄ± olduÄŸunu unutmayÄ±n, bu nedenle 25 epoch, modelin 25 ÅŸansÄ± olduÄŸu anlamÄ±na gelir) Ã¶ncekinden daha fazla Ã¶ÄŸrenme oranÄ±.

Modelimizin deÄŸerlendirme Ã¶lÃ§Ã¼tlerine gÃ¶re iyi performans gÃ¶sterdiÄŸini biliyoruz ancak gÃ¶rsel olarak nasÄ±l performans gÃ¶sterdiÄŸini gÃ¶relim.


```python
# EÄŸitim ve test setleri iÃ§in karar sÄ±nÄ±rlarÄ±nÄ± Ã§izin
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.title("Train")
plot_decision_boundary(model_8, X=X_train, y=y_train)
plt.subplot(1, 2, 2)
plt.title("Test")
plot_decision_boundary(model_8, X=X_test, y=y_test)
plt.show()
```

    doing binary classifcation...
    doing binary classifcation...



    
![png](TensorFlow_ile_Sinir_A%C4%9F%C4%B1_S%C4%B1n%C4%B1fland%C4%B1r%C4%B1lmas%C4%B1_files/TensorFlow_ile_Sinir_A%C4%9F%C4%B1_S%C4%B1n%C4%B1fland%C4%B1r%C4%B1lmas%C4%B1_72_1.png)
    


Ä°nce ayar (fine tuning) ile neredeyse mÃ¼kemmel bir model eÄŸittik.

## KayÄ±p (Loss) EÄŸrilerini GÃ¶rselleÅŸtirin

YukarÄ±daki grafiklere baktÄ±ÄŸÄ±mÄ±zda modelimizin Ã§Ä±ktÄ±larÄ±nÄ±n Ã§ok iyi olduÄŸunu gÃ¶rebiliriz. Ama modelimiz Ã¶ÄŸrenirken nasÄ±l bir yol izledi?

OlduÄŸu gibi, modelin verilere bakma ÅŸansÄ± olduÄŸu her seferinde (her epoch bir kez) performans nasÄ±l deÄŸiÅŸti? Bunu anlamak iÃ§in kayÄ±p (loss) eÄŸrilerini kontrol edebiliriz.Bir modelde fit() iÅŸlevini Ã§aÄŸÄ±rÄ±rken deÄŸiÅŸken geÃ§miÅŸini kullandÄ±ÄŸÄ±mÄ±zÄ± gÃ¶rmÃ¼ÅŸ olabilirsiniz (fit() bir GeÃ§miÅŸ nesnesi dÃ¶ndÃ¼rÃ¼r).

Modelimizin Ã¶ÄŸrenirken nasÄ±l performans gÃ¶sterdiÄŸine dair bilgileri buradan alacaÄŸÄ±z. BakalÄ±m nasÄ±l kullanacaÄŸÄ±z...


```python
# history niteliÄŸini kullanarak geÃ§miÅŸ deÄŸiÅŸkenindeki bilgilere eriÅŸebilirsiniz
pd.DataFrame(history.history)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss</th>
      <th>accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.684651</td>
      <td>0.54250</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.677721</td>
      <td>0.55250</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.673595</td>
      <td>0.55125</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.668149</td>
      <td>0.57750</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.663269</td>
      <td>0.58500</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.654567</td>
      <td>0.58375</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.641258</td>
      <td>0.67500</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.626428</td>
      <td>0.70125</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.603831</td>
      <td>0.74875</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.571404</td>
      <td>0.77375</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.540443</td>
      <td>0.76500</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.501504</td>
      <td>0.78375</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0.468332</td>
      <td>0.79750</td>
    </tr>
    <tr>
      <th>13</th>
      <td>0.411302</td>
      <td>0.84500</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0.362506</td>
      <td>0.91250</td>
    </tr>
    <tr>
      <th>15</th>
      <td>0.320904</td>
      <td>0.93125</td>
    </tr>
    <tr>
      <th>16</th>
      <td>0.284708</td>
      <td>0.94875</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0.259720</td>
      <td>0.95250</td>
    </tr>
    <tr>
      <th>18</th>
      <td>0.237469</td>
      <td>0.95625</td>
    </tr>
    <tr>
      <th>19</th>
      <td>0.213520</td>
      <td>0.96625</td>
    </tr>
    <tr>
      <th>20</th>
      <td>0.193820</td>
      <td>0.97750</td>
    </tr>
    <tr>
      <th>21</th>
      <td>0.175244</td>
      <td>0.97375</td>
    </tr>
    <tr>
      <th>22</th>
      <td>0.161893</td>
      <td>0.97875</td>
    </tr>
    <tr>
      <th>23</th>
      <td>0.154989</td>
      <td>0.97750</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0.148973</td>
      <td>0.97625</td>
    </tr>
  </tbody>
</table>
</div>



Ã‡Ä±ktÄ±larÄ± inceleyerek kayÄ±p deÄŸerlerinin dÃ¼ÅŸtÃ¼ÄŸÃ¼nÃ¼ ve doÄŸruluÄŸun arttÄ±ÄŸÄ±nÄ± gÃ¶rebiliriz.

NasÄ±l gÃ¶rÃ¼nÃ¼yor (gÃ¶rselleÅŸtirin, gÃ¶rselleÅŸtirin, gÃ¶rselleÅŸtirin)?



```python
pd.DataFrame(history.history).plot()
plt.title("Model_8 training curves")
```




    Text(0.5, 1.0, 'Model_8 training curves')




    
![png](TensorFlow_ile_Sinir_A%C4%9F%C4%B1_S%C4%B1n%C4%B1fland%C4%B1r%C4%B1lmas%C4%B1_files/TensorFlow_ile_Sinir_A%C4%9F%C4%B1_S%C4%B1n%C4%B1fland%C4%B1r%C4%B1lmas%C4%B1_77_1.png)
    


GÃ¼zel. Bu, bir sÄ±nÄ±flandÄ±rma problemiyle uÄŸraÅŸÄ±rken aradÄ±ÄŸÄ±mÄ±z ideal gÃ¶rsellemedir, kayÄ±p azalÄ±r, doÄŸruluk artar.

> ğŸ”‘ Not: BirÃ§ok problem iÃ§in, kayÄ±p fonksiyonunun dÃ¼ÅŸmesi, modelin iyileÅŸtiÄŸi anlamÄ±na gelir.

## En Ä°yi Ã–ÄŸrenme OranÄ±nÄ± (Learning Rate) Bulma

Mimarinin (katmanlar, nÃ¶ronlarÄ±n sayÄ±sÄ±, aktivasyonlar vb.) yanÄ± sÄ±ra, sinir aÄŸÄ± modelleriniz iÃ§in ayarlayabileceÄŸiniz en Ã¶nemli hiperparametre Ã¶ÄŸrenme oranÄ±dÄ±r (lr). 

model_8'de Adam optimizer'Ä±n Ã¶ÄŸrenme oranÄ±nÄ± varsayÄ±lan 0,001'den (varsayÄ±lan) 0,01'e indirdiÄŸimizi gÃ¶rdÃ¼nÃ¼z. Ve bunu neden yaptÄ±ÄŸÄ±mÄ±zÄ± merak ediyor olabilirsiniz. ÅanslÄ± bir tahmindi.

Daha dÃ¼ÅŸÃ¼k bir Ã¶ÄŸrenme oranÄ± denemeye ve modelin nasÄ±l gittiÄŸini gÃ¶rmeye karar verdim. Åimdi "Cidden mi? Bunu yapabilir misin?" diye dÃ¼ÅŸÃ¼nÃ¼yor olabilirsiniz. Ve cevap evet. Sinir aÄŸlarÄ±nÄ±zÄ±n hiperparametrelerinden herhangi birini deÄŸiÅŸtirebilirsiniz. Pratik yaparak, ne tÃ¼r hiperparametrelerin iÅŸe yarayÄ±p nelerin yaramadÄ±ÄŸÄ±nÄ± gÃ¶rmeye baÅŸlayacaksÄ±nÄ±z.

Bu, genel olarak makine Ã¶ÄŸrenimi ve derin Ã¶ÄŸrenme hakkÄ±nda anlaÅŸÄ±lmasÄ± gereken Ã¶nemli bir ÅŸeydir. Bir model kuruyorsunuz ve onu deÄŸerlendiriyorsunuz, bir model kuruyorsunuz ve onu deÄŸerlendiriyorsunuz ... bu dÃ¶ngÃ¼ en iyi veya ideal sonuca eriÅŸene dek devam ediyor.

Bununla birlikte, ileriye dÃ¶nÃ¼k modelleriniz iÃ§in en uygun Ã¶ÄŸrenme oranÄ±nÄ± (en azÄ±ndan eÄŸitime baÅŸlamak iÃ§in) bulmanÄ±za yardÄ±mcÄ± olacak bir numara tanÄ±tmak istiyorum. Bunu yapmak iÃ§in aÅŸaÄŸÄ±dakileri kullanacaÄŸÄ±z:

- Bir [learning rate callback](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler).
  - Geri aramayÄ± (backpropagation), eÄŸitim sÄ±rasÄ±nda modelinize ekleyebileceÄŸiniz ekstra bir iÅŸlevsellik parÃ§asÄ± olarak dÃ¼ÅŸÃ¼nebilirsiniz.
- BaÅŸka bir model (yukarÄ±dakilerin aynÄ±sÄ±nÄ± kullanabiliriz, burada model oluÅŸturma alÄ±ÅŸtÄ±rmasÄ± yapÄ±yoruz).
- DeÄŸiÅŸtirilmiÅŸ bir kayÄ±p eÄŸrileri grafiÄŸi

Her birinin Ã¼zerinden kodlarla geÃ§eceÄŸiz, sonra da neler olduÄŸunu aÃ§Ä±klayacaÄŸÄ±z.

> ğŸ”‘ Not: TensorFlow'daki birÃ§ok sinir aÄŸÄ± yapÄ± taÅŸÄ±nÄ±n varsayÄ±lan hiperparametreleri, genellikle kutudan Ã§Ä±kar Ã§Ä±kmaz Ã§alÄ±ÅŸacak ÅŸekilde kurulur (Ã¶rneÄŸin, Adam optimizer'Ä±n varsayÄ±lan ayarlarÄ± genellikle birÃ§ok veri kÃ¼mesinde iyi sonuÃ§lar alabilir). Bu nedenle, Ã¶nce varsayÄ±lanlarÄ± denemek ve ardÄ±ndan gerektiÄŸi gibi ayarlamak (fine tuning) iyi bir fikirdir.


```python
tf.random.set_seed(42)

# bir model yaratalÄ±m (model_8'in aynÄ±sÄ±)
model_9 = tf.keras.Sequential([
  tf.keras.layers.Dense(4, activation="relu"),
  tf.keras.layers.Dense(4, activation="relu"),
  tf.keras.layers.Dense(1, activation="sigmoid")
])

# modeli derleyelim
model_9.compile(loss="binary_crossentropy", 
              optimizer="Adam", 
              metrics=["accuracy"]) 

# Ã¶ÄŸrenme oranÄ±nÄ± belli bir kurala gÃ¶re azaltacak fonksiyonu ekleme
lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-4 * 10**(epoch/20)) 
# 1e-4'ten baÅŸlayarak her epochta 10**(epoch/20) artan bir dizi Ã¶ÄŸrenme oranÄ± (learning rate)

# modeli fit etme
history = model_9.fit(X_train, 
                      y_train, 
                      epochs=100,
                      callbacks=[lr_scheduler])
```

    Epoch 1/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.6945 - accuracy: 0.4988
    Epoch 2/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.6938 - accuracy: 0.4975
    Epoch 3/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.6930 - accuracy: 0.4963
    Epoch 4/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.6922 - accuracy: 0.4975
    Epoch 5/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.6914 - accuracy: 0.5063
    Epoch 6/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.6906 - accuracy: 0.5013
    Epoch 7/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.6898 - accuracy: 0.4950
    Epoch 8/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.6889 - accuracy: 0.5038
    Epoch 9/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.6880 - accuracy: 0.5013
    Epoch 10/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.6871 - accuracy: 0.5050
    Epoch 11/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.6863 - accuracy: 0.5200
    Epoch 12/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.6856 - accuracy: 0.5163
    Epoch 13/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.6847 - accuracy: 0.5175
    Epoch 14/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.6842 - accuracy: 0.5200
    Epoch 15/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.6835 - accuracy: 0.5213
    Epoch 16/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.6829 - accuracy: 0.5213
    Epoch 17/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.6826 - accuracy: 0.5225
    Epoch 18/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.6819 - accuracy: 0.5300
    Epoch 19/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.6816 - accuracy: 0.5312
    Epoch 20/100
    25/25 [==============================] - 0s 2ms/step - loss: 0.6811 - accuracy: 0.5387
    Epoch 21/100
    25/25 [==============================] - 0s 2ms/step - loss: 0.6806 - accuracy: 0.5400
    Epoch 22/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.6801 - accuracy: 0.5412
    Epoch 23/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.6796 - accuracy: 0.5400
    Epoch 24/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.6790 - accuracy: 0.5425
    Epoch 25/100
    25/25 [==============================] - 0s 2ms/step - loss: 0.6784 - accuracy: 0.5450
    Epoch 26/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.6778 - accuracy: 0.5387
    Epoch 27/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.6770 - accuracy: 0.5425
    Epoch 28/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.6760 - accuracy: 0.5537
    Epoch 29/100
    25/25 [==============================] - 0s 2ms/step - loss: 0.6754 - accuracy: 0.5512
    Epoch 30/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.6739 - accuracy: 0.5575
    Epoch 31/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.6726 - accuracy: 0.5500
    Epoch 32/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.6711 - accuracy: 0.5512
    Epoch 33/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.6688 - accuracy: 0.5562
    Epoch 34/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.6672 - accuracy: 0.5612
    Epoch 35/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.6660 - accuracy: 0.5888
    Epoch 36/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.6625 - accuracy: 0.5625
    Epoch 37/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.6560 - accuracy: 0.5813
    Epoch 38/100
    25/25 [==============================] - 0s 2ms/step - loss: 0.6521 - accuracy: 0.6025
    Epoch 39/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.6415 - accuracy: 0.7088
    Epoch 40/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.6210 - accuracy: 0.7113
    Epoch 41/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.5904 - accuracy: 0.7487
    Epoch 42/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.5688 - accuracy: 0.7312
    Epoch 43/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.5346 - accuracy: 0.7563
    Epoch 44/100
    25/25 [==============================] - 0s 2ms/step - loss: 0.4533 - accuracy: 0.8150
    Epoch 45/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.3455 - accuracy: 0.9112
    Epoch 46/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.2570 - accuracy: 0.9463
    Epoch 47/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.1968 - accuracy: 0.9575
    Epoch 48/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.1336 - accuracy: 0.9700
    Epoch 49/100
    25/25 [==============================] - 0s 2ms/step - loss: 0.1310 - accuracy: 0.9613
    Epoch 50/100
    25/25 [==============================] - 0s 2ms/step - loss: 0.1002 - accuracy: 0.9700
    Epoch 51/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.1166 - accuracy: 0.9638
    Epoch 52/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.1368 - accuracy: 0.9513
    Epoch 53/100
    25/25 [==============================] - 0s 2ms/step - loss: 0.0879 - accuracy: 0.9787
    Epoch 54/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.1187 - accuracy: 0.9588
    Epoch 55/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.0733 - accuracy: 0.9712
    Epoch 56/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.1132 - accuracy: 0.9550
    Epoch 57/100
    25/25 [==============================] - 0s 2ms/step - loss: 0.1057 - accuracy: 0.9613
    Epoch 58/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.0664 - accuracy: 0.9750
    Epoch 59/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.1898 - accuracy: 0.9275
    Epoch 60/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.1895 - accuracy: 0.9312
    Epoch 61/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.4131 - accuracy: 0.8612
    Epoch 62/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.1707 - accuracy: 0.9725
    Epoch 63/100
    25/25 [==============================] - 0s 2ms/step - loss: 0.0569 - accuracy: 0.9937
    Epoch 64/100
    25/25 [==============================] - 0s 2ms/step - loss: 0.1007 - accuracy: 0.9638
    Epoch 65/100
    25/25 [==============================] - 0s 2ms/step - loss: 0.1323 - accuracy: 0.9488
    Epoch 66/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.1819 - accuracy: 0.9375
    Epoch 67/100
    25/25 [==============================] - 0s 2ms/step - loss: 0.6672 - accuracy: 0.7613
    Epoch 68/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.5301 - accuracy: 0.6687
    Epoch 69/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.4140 - accuracy: 0.7925
    Epoch 70/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.4574 - accuracy: 0.7412
    Epoch 71/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.4759 - accuracy: 0.7262
    Epoch 72/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.3748 - accuracy: 0.8112
    Epoch 73/100
    25/25 [==============================] - 0s 2ms/step - loss: 0.4710 - accuracy: 0.8150
    Epoch 74/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.4143 - accuracy: 0.8087
    Epoch 75/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.5961 - accuracy: 0.7412
    Epoch 76/100
    25/25 [==============================] - 0s 2ms/step - loss: 0.4787 - accuracy: 0.7713
    Epoch 77/100
    25/25 [==============================] - 0s 2ms/step - loss: 0.4720 - accuracy: 0.7113
    Epoch 78/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.2565 - accuracy: 0.8675
    Epoch 79/100
    25/25 [==============================] - 0s 1ms/step - loss: 1.1824 - accuracy: 0.6275
    Epoch 80/100
    25/25 [==============================] - 0s 2ms/step - loss: 0.6873 - accuracy: 0.5425
    Epoch 81/100
    25/25 [==============================] - 0s 2ms/step - loss: 0.7068 - accuracy: 0.5575
    Epoch 82/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.6879 - accuracy: 0.5838
    Epoch 83/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.6996 - accuracy: 0.5700
    Epoch 84/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.6471 - accuracy: 0.5863
    Epoch 85/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.7457 - accuracy: 0.5312
    Epoch 86/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.7546 - accuracy: 0.5038
    Epoch 87/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.7681 - accuracy: 0.5063
    Epoch 88/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.7596 - accuracy: 0.4963
    Epoch 89/100
    25/25 [==============================] - 0s 2ms/step - loss: 0.7778 - accuracy: 0.5063
    Epoch 90/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.7741 - accuracy: 0.4787
    Epoch 91/100
    25/25 [==============================] - 0s 2ms/step - loss: 0.7851 - accuracy: 0.5163
    Epoch 92/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.7441 - accuracy: 0.4888
    Epoch 93/100
    25/25 [==============================] - 0s 2ms/step - loss: 0.7354 - accuracy: 0.5163
    Epoch 94/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.7548 - accuracy: 0.4938
    Epoch 95/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.8087 - accuracy: 0.4863
    Epoch 96/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.7714 - accuracy: 0.4638
    Epoch 97/100
    25/25 [==============================] - 0s 1ms/step - loss: 0.8001 - accuracy: 0.5013
    Epoch 98/100
    25/25 [==============================] - 0s 2ms/step - loss: 0.9554 - accuracy: 0.4963
    Epoch 99/100
    25/25 [==============================] - 0s 2ms/step - loss: 0.9268 - accuracy: 0.4913
    Epoch 100/100
    25/25 [==============================] - 0s 2ms/step - loss: 0.8563 - accuracy: 0.4663


Modelimizin eÄŸitimi bitti, ÅŸimdi eÄŸitim grafiÄŸine bir gÃ¶z atalÄ±m.


```python
pd.DataFrame(history.history).plot(figsize=(10,7), xlabel="epochs");
```


    
![png](TensorFlow_ile_Sinir_A%C4%9F%C4%B1_S%C4%B1n%C4%B1fland%C4%B1r%C4%B1lmas%C4%B1_files/TensorFlow_ile_Sinir_A%C4%9F%C4%B1_S%C4%B1n%C4%B1fland%C4%B1r%C4%B1lmas%C4%B1_82_0.png)
    


GÃ¶rdÃ¼ÄŸÃ¼nÃ¼z gibi epoch sayÄ±sÄ± arttÄ±kÃ§a Ã¶ÄŸrenme oranÄ± katlanarak artÄ±yor. Ve Ã¶ÄŸrenme oranÄ± yavaÅŸÃ§a arttÄ±ÄŸÄ±nda belirli bir noktada modelin doÄŸruluÄŸunun arttÄ±ÄŸÄ±nÄ± (ve kaybÄ±n azaldÄ±ÄŸÄ±nÄ±) gÃ¶rebilirsiniz.

Bu Ã§arpma noktasÄ±nÄ±n nerede olduÄŸunu bulmak iÃ§in, gÃ¼nlÃ¼k Ã¶lÃ§ekli Ã¶ÄŸrenme oranÄ±na karÅŸÄ± kaybÄ± Ã§izebiliriz.



```python
# Kayba karÅŸÄ± Ã¶ÄŸrenme oranÄ±nÄ± Ã§izin
lrs = 1e-4 * (10 ** (np.arange(100)/20))
plt.figure(figsize=(10, 7))
plt.semilogx(lrs, history.history["loss"])
plt.xlabel("Learning Rate")
plt.ylabel("Loss")
plt.title("Learning rate vs. loss");
```


    
![png](TensorFlow_ile_Sinir_A%C4%9F%C4%B1_S%C4%B1n%C4%B1fland%C4%B1r%C4%B1lmas%C4%B1_files/TensorFlow_ile_Sinir_A%C4%9F%C4%B1_S%C4%B1n%C4%B1fland%C4%B1r%C4%B1lmas%C4%B1_84_0.png)
    


Ã–ÄŸrenme hÄ±zÄ±nÄ±n ideal deÄŸerini bulmak iÃ§in (en azÄ±ndan modelimizi eÄŸitmeye baÅŸlamak iÃ§in ideal deÄŸer), temel kural, kaybÄ±n hala azalmakta olduÄŸu ancak tam olarak dÃ¼zleÅŸmediÄŸi eÄŸriyi kullanmaktÄ±r. Bu durumda ideal Ã¶ÄŸrenme oranÄ±mÄ±z 0,01  ile 0,02 arasÄ±nda olur.

Åimdi modelimiz iÃ§in ideal Ã¶ÄŸrenme oranÄ±nÄ± (0,02 kullanacaÄŸÄ±z) tahmin ettik, hadi bu deÄŸerle yeniden eÄŸitelim.


```python
tf.random.set_seed(42)

# bir model yaratma
model_10 = tf.keras.Sequential([
  tf.keras.layers.Dense(4, activation="relu"),
  tf.keras.layers.Dense(4, activation="relu"),
  tf.keras.layers.Dense(1, activation="sigmoid")
])

# yeni lr deÄŸeri ile modeli derleme
model_10.compile(loss="binary_crossentropy",
                optimizer=tf.keras.optimizers.Adam(learning_rate=0.02),
                metrics=["accuracy"])

history = model_10.fit(X_train, y_train, epochs=20)
```

    Epoch 1/20
    25/25 [==============================] - 0s 1ms/step - loss: 0.6837 - accuracy: 0.5600
    Epoch 2/20
    25/25 [==============================] - 0s 1ms/step - loss: 0.6744 - accuracy: 0.5750
    Epoch 3/20
    25/25 [==============================] - 0s 1ms/step - loss: 0.6626 - accuracy: 0.5875
    Epoch 4/20
    25/25 [==============================] - 0s 1ms/step - loss: 0.6332 - accuracy: 0.6388
    Epoch 5/20
    25/25 [==============================] - 0s 1ms/step - loss: 0.5830 - accuracy: 0.7563
    Epoch 6/20
    25/25 [==============================] - 0s 1ms/step - loss: 0.4907 - accuracy: 0.8313
    Epoch 7/20
    25/25 [==============================] - 0s 1ms/step - loss: 0.4251 - accuracy: 0.8450
    Epoch 8/20
    25/25 [==============================] - 0s 2ms/step - loss: 0.3596 - accuracy: 0.8875
    Epoch 9/20
    25/25 [==============================] - 0s 1ms/step - loss: 0.3152 - accuracy: 0.9100
    Epoch 10/20
    25/25 [==============================] - 0s 1ms/step - loss: 0.2512 - accuracy: 0.9500
    Epoch 11/20
    25/25 [==============================] - 0s 1ms/step - loss: 0.2152 - accuracy: 0.9500
    Epoch 12/20
    25/25 [==============================] - 0s 1ms/step - loss: 0.1721 - accuracy: 0.9750
    Epoch 13/20
    25/25 [==============================] - 0s 1ms/step - loss: 0.1443 - accuracy: 0.9837
    Epoch 14/20
    25/25 [==============================] - 0s 1ms/step - loss: 0.1232 - accuracy: 0.9862
    Epoch 15/20
    25/25 [==============================] - 0s 1ms/step - loss: 0.1085 - accuracy: 0.9850
    Epoch 16/20
    25/25 [==============================] - 0s 1ms/step - loss: 0.0940 - accuracy: 0.9937
    Epoch 17/20
    25/25 [==============================] - 0s 1ms/step - loss: 0.0827 - accuracy: 0.9962
    Epoch 18/20
    25/25 [==============================] - 0s 2ms/step - loss: 0.0798 - accuracy: 0.9937
    Epoch 19/20
    25/25 [==============================] - 0s 1ms/step - loss: 0.0845 - accuracy: 0.9875
    Epoch 20/20
    25/25 [==============================] - 0s 2ms/step - loss: 0.0790 - accuracy: 0.9887


GÃ¼zel! Biraz daha yÃ¼ksek Ã¶ÄŸrenme oranÄ±yla (0,01 yerine 0,02), daha az epoch (25 yerine 20) ile model_8'den daha yÃ¼ksek bir doÄŸruluÄŸa ulaÅŸÄ±yoruz.


```python
# Modeli test veri seti ile deÄŸerlendirin
model_10.evaluate(X_test, y_test)
```

    7/7 [==============================] - 0s 3ms/step - loss: 0.0574 - accuracy: 0.9900





    [0.05740184709429741, 0.9900000095367432]



BakalÄ±m tahminler gÃ¶reselleÅŸtirme Ã¼zerinde nasÄ±l duruyor.


```python
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.title("Train")
plot_decision_boundary(model_10, X=X_train, y=y_train)
plt.subplot(1, 2, 2)
plt.title("Test")
plot_decision_boundary(model_10, X=X_test, y=y_test)
plt.show()
```

    doing binary classifcation...
    doing binary classifcation...



    
![png](TensorFlow_ile_Sinir_A%C4%9F%C4%B1_S%C4%B1n%C4%B1fland%C4%B1r%C4%B1lmas%C4%B1_files/TensorFlow_ile_Sinir_A%C4%9F%C4%B1_S%C4%B1n%C4%B1fland%C4%B1r%C4%B1lmas%C4%B1_90_1.png)
    


Ve gÃ¶rdÃ¼ÄŸÃ¼mÃ¼z gibi, yine neredeyse mÃ¼kemmel. Bunlar, kendi modellerinizi oluÅŸtururken sÄ±klÄ±kla gerÃ§ekleÅŸtireceÄŸiniz deneylerdir. VarsayÄ±lan ayarlarla baÅŸlayÄ±n ve bunlarÄ±n verilerinizde nasÄ±l performans gÃ¶sterdiÄŸini gÃ¶rÃ¼n. Ve istediÄŸiniz kadar iyi performans gÃ¶stermiyorlarsa, geliÅŸtirin. SÄ±nÄ±flandÄ±rma modellerimizi deÄŸerlendirmenin birkaÃ§ yoluna daha bakalÄ±m.


## Daha Fazla SÄ±nÄ±flandÄ±rma DeÄŸerlendirme YÃ¶ntemi

YaptÄ±ÄŸÄ±mÄ±z gÃ¶rselleÅŸtirmelerin yanÄ± sÄ±ra, sÄ±nÄ±flandÄ±rma modellerimizi deÄŸerlendirmek iÃ§in kullanabileceÄŸimiz bir dizi farklÄ± deÄŸerlendirme Ã¶lÃ§Ã¼tÃ¼ var.

| **Metrik adÄ±/DeÄŸerlendirme yÃ¶ntemi*** | **TanÄ±m** | **Kod** |
| --- | --- | --- |
| Accuracy | Modeliniz 100 tahminden kaÃ§Ä± doÄŸru Ã§Ä±kÄ±yor? Ã–rneÄŸin. %95 doÄŸruluk, 95/100 tahminlerin doÄŸru olduÄŸu anlamÄ±na gelir. | [`sklearn.metrics.accuracy_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) veya [`tf.keras.metrics.Accuracy()`](tensorflow.org/api_docs/python/tf/keras/metrics/Accuracy) |
| Precision | GerÃ§ek pozitiflerin toplam Ã¶rnek sayÄ±sÄ±na oranÄ±. Daha yÃ¼ksek hassasiyet, daha az yanlÄ±ÅŸ pozitife yol aÃ§ar (model, 0 olmasÄ± gerektiÄŸinde 1'i tahmin eder). | [`sklearn.metrics.precision_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html) veya [`tf.keras.metrics.Precision()`](tensorflow.org/api_docs/python/tf/keras/metrics/Precision) |
| Recall | GerÃ§ek pozitiflerin toplam gerÃ§ek pozitif ve yanlÄ±ÅŸ negatif sayÄ±sÄ± Ã¼zerindeki oranÄ± (model, 1 olmasÄ± gerektiÄŸinde 0'Ä± tahmin eder). Daha yÃ¼ksek hatÄ±rlama, daha az yanlÄ±ÅŸ negatife yol aÃ§ar. | [`sklearn.metrics.recall_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html) or [`tf.keras.metrics.Recall()`](tensorflow.org/api_docs/python/tf/keras/metrics/Recall) |
| F1-score | Kesinlik ve geri Ã§aÄŸÄ±rmayÄ± tek bir metrikte birleÅŸtirir. 1 en iyisidir, 0 en kÃ¶tÃ¼sÃ¼dÃ¼r. | [`sklearn.metrics.f1_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html) |
| [Confusion matrix](https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/)  | Tahmin edilen deÄŸerleri gerÃ§ek deÄŸerlerle tablo ÅŸeklinde karÅŸÄ±laÅŸtÄ±rÄ±r, %100 doÄŸruysa matristeki tÃ¼m deÄŸerler sol Ã¼stten saÄŸ alta olacaktÄ±r. | Custom function or [`sklearn.metrics.plot_confusion_matrix()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_confusion_matrix.html) |
| Classification report | Kesinlik, geri Ã§aÄŸÄ±rma ve f1 puanÄ± gibi bazÄ± ana sÄ±nÄ±flandÄ±rma Ã¶lÃ§Ã¼tlerinin toplanmasÄ±. | [`sklearn.metrics.classification_report()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) |



> ğŸ”‘ Not: Her sÄ±nÄ±flandÄ±rma problemi, farklÄ± tÃ¼rde deÄŸerlendirme yÃ¶ntemleri gerektirecektir. Ama en azÄ±ndan yukarÄ±dakilere aÅŸina olmalÄ±sÄ±nÄ±z.

DoÄŸrulukla baÅŸlayalÄ±m.

Modelimizi derlerken metrics parametresine `["accuracy"]` ilettiÄŸimiz iÃ§in, Ã¼zerinde `evaluate()` Ã¶ÄŸesinin Ã§aÄŸrÄ±lmasÄ±, doÄŸruluÄŸun yanÄ± sÄ±ra kaybÄ± da dÃ¶ndÃ¼recektir.


```python
loss, accuracy = model_10.evaluate(X_test, y_test)
print(f"Test setinde model kaybÄ±:: {loss}")
print(f"Test setinde model doÄŸruluÄŸu: {(accuracy*100):.2f}%")
```

    7/7 [==============================] - 0s 2ms/step - loss: 0.0574 - accuracy: 0.9900
    Test setinde model kaybÄ±:: 0.05740184709429741
    Test setinde model doÄŸruluÄŸu: 99.00%



```python
from sklearn.metrics import confusion_matrix

# tahminler yapma
y_preds = model_10.predict(X_test)

# bir confusion matrix oluÅŸturma
confusion_matrix(y_test, y_preds)
```


    ---------------------------------------------------------------------------

    ValueError                                Traceback (most recent call last)

    <ipython-input-50-f9255bdba7ac> in <module>()
          5 
          6 # bir confusion matrix oluÅŸturma
    ----> 7 confusion_matrix(y_test, y_preds)
    

    /usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py in confusion_matrix(y_true, y_pred, labels, sample_weight, normalize)
        266 
        267     """
    --> 268     y_type, y_true, y_pred = _check_targets(y_true, y_pred)
        269     if y_type not in ("binary", "multiclass"):
        270         raise ValueError("%s is not supported" % y_type)


    /usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py in _check_targets(y_true, y_pred)
         88     if len(y_type) > 1:
         89         raise ValueError("Classification metrics can't handle a mix of {0} "
    ---> 90                          "and {1} targets".format(type_true, type_pred))
         91 
         92     # We can't have more than one value on y_type => The set is no more needed


    ValueError: Classification metrics can't handle a mix of binary and continuous targets


Ahh, gÃ¶rÃ¼nÃ¼ÅŸe gÃ¶re tahminlerimiz olmasÄ± gereken formatta deÄŸil. OnlarÄ± kontrol edelim.



```python
# Ä°lk 10 tahmine gÃ¶z atalÄ±m
y_preds[:10]
```




    array([[9.8526537e-01],
           [9.9923790e-01],
           [9.9032348e-01],
           [9.9706942e-01],
           [3.9622977e-01],
           [1.8126935e-02],
           [9.6829069e-01],
           [1.9746721e-02],
           [9.9967170e-01],
           [5.6460500e-04]], dtype=float32)




```python
# ilk 10 test etiketine gÃ¶z atalÄ±m
y_test[:10]
```




    array([1, 1, 1, 1, 0, 0, 1, 0, 1, 0])



Tahminlerimizi ikili formata (0 veya 1) almamÄ±z gerekiyor gibi gÃ¶rÃ¼nÃ¼yor.

Ama merak ediyor olabilirsiniz, ÅŸu anda hangi formattalar?  Mevcut formatlarÄ±nda (9.8526537e-01), tahmin olasÄ±lÄ±klarÄ± adÄ± verilen bir formdalar.

Bunu sinir aÄŸlarÄ±nÄ±n Ã§Ä±ktÄ±larÄ±nda sÄ±klÄ±kla gÃ¶receksiniz. Genellikle kesin deÄŸerler olmayacaklar, ancak daha Ã§ok, ÅŸu veya bu deÄŸer olma olasÄ±lÄ±klarÄ±nÄ±n bir olasÄ±lÄ±ÄŸÄ± olacaktÄ±r. Bu nedenle, bir sinir aÄŸÄ± ile tahminler yaptÄ±ktan sonra sÄ±klÄ±kla gÃ¶receÄŸiniz adÄ±mlardan biri, tahmin olasÄ±lÄ±klarÄ±nÄ± etiketlere dÃ¶nÃ¼ÅŸtÃ¼rmektir. Bizim durumumuzda, temel doÄŸruluk etiketlerimiz (y_test) ikili (0 veya 1) olduÄŸundan, tf.round() kullanarak tahmin olasÄ±lÄ±klarÄ±nÄ± ikili biÃ§imlerine dÃ¶nÃ¼ÅŸtÃ¼rebiliriz.



```python
tf.round(y_preds)[:10]
```




    <tf.Tensor: shape=(10, 1), dtype=float32, numpy=
    array([[1.],
           [1.],
           [1.],
           [1.],
           [0.],
           [0.],
           [1.],
           [0.],
           [1.],
           [0.]], dtype=float32)>



Ä°ÅŸte ÅŸimdi oldu. confusion matrix uygulayabiliriz.


```python
# bir confusion matrix oluÅŸturma
confusion_matrix(y_test, tf.round(y_preds))
```




    array([[99,  2],
           [ 0, 99]])



Pekala, en yÃ¼ksek sayÄ±larÄ±n sol Ã¼st ve saÄŸ alta olduÄŸunu gÃ¶rebiliyoruz, yani bu iyi bir iÅŸaret, ancak matrisin geri kalanÄ± bize pek bir ÅŸey sÃ¶ylemiyor.

KarÄ±ÅŸÄ±klÄ±k matrisimizi biraz daha gÃ¶rsel hale getirmek iÃ§in bir fonksiyon yazmaua ne dersiniz?



```python
import itertools

figsize = (10, 10)

# confusion matrix oluÅŸturma
cm = confusion_matrix(y_test, tf.round(y_preds))
cm_norm = cm.astype("float") / cm.sum(axis=1)[:, np.newaxis] # normalize etme
n_classes = cm.shape[0]

fig, ax = plt.subplots(figsize=figsize)
cax = ax.matshow(cm, cmap=plt.cm.Blues) 
fig.colorbar(cax)

classes = False

if classes:
  labels = classes
else:
  labels = np.arange(cm.shape[0])

# eksenleri etiketleme
ax.set(title="Confusion Matrix",
       xlabel="Predicted label",
       ylabel="True label",
       xticks=np.arange(n_classes),
       yticks=np.arange(n_classes),
       xticklabels=labels,
       yticklabels=labels)

# x ekseni etiketlerini en alta ayarla
ax.xaxis.set_label_position("bottom")
ax.xaxis.tick_bottom()

# Etiket boyutunu ayarla
ax.xaxis.label.set_size(20)
ax.yaxis.label.set_size(20)
ax.title.set_size(20)

# FarklÄ± renkler iÃ§in eÅŸik ayarla
threshold = (cm.max() + cm.min()) / 2.

for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
  plt.text(j, i, f"{cm[i, j]} ({cm_norm[i, j]*100:.1f}%)",
           horizontalalignment="center",
           color="white" if cm[i, j] > threshold else "black",
           size=15)
```


    
![png](TensorFlow_ile_Sinir_A%C4%9F%C4%B1_S%C4%B1n%C4%B1fland%C4%B1r%C4%B1lmas%C4%B1_files/TensorFlow_ile_Sinir_A%C4%9F%C4%B1_S%C4%B1n%C4%B1fland%C4%B1r%C4%B1lmas%C4%B1_104_0.png)
    


Bu Ã§ok daha iyi gÃ¶rÃ¼nÃ¼yor. GÃ¶rÃ¼nÃ¼ÅŸe gÃ¶re modelimiz, iki yanlÄ±ÅŸ pozitif (saÄŸ Ã¼st kÃ¶ÅŸe) dÄ±ÅŸÄ±nda test setinde neredeyse mÃ¼kemmel tahminler yaptÄ±.

## Daha BÃ¼yÃ¼k Bir Ã–rnekle Ã‡alÄ±ÅŸma (Multi-Class Classification)

Ä°kili bir sÄ±nÄ±flandÄ±rma Ã¶rneÄŸi gÃ¶rdÃ¼k (bir veri noktasÄ±nÄ±n kÄ±rmÄ±zÄ± bir dairenin mi yoksa mavi bir dairenin mi parÃ§asÄ± olduÄŸunu tahmin ederek) ama ya birden fazla farklÄ± nesne sÄ±nÄ±fÄ±nÄ±z varsa?

Ã–rneÄŸin, bir moda ÅŸirketi olduÄŸunuzu ve bir giysinin ayakkabÄ± mÄ±, gÃ¶mlek mi yoksa ceket mi olduÄŸunu tahmin etmek iÃ§in bir sinir aÄŸÄ± kurmak istediÄŸinizi varsayalÄ±m (3 farklÄ± seÃ§enek). SeÃ§enek olarak ikiden fazla sÄ±nÄ±fÄ±nÄ±z olduÄŸunda, buna Ã§ok sÄ±nÄ±flÄ± sÄ±nÄ±flandÄ±rma denir. Ä°yi haber ÅŸu ki, ÅŸimdiye kadar Ã¶ÄŸrendiklerimiz (birkaÃ§ ince ayar ile) Ã§ok sÄ±nÄ±flÄ± sÄ±nÄ±flandÄ±rma problemlerine de uygulanabilir.


```python
import tensorflow as tf
from tensorflow.keras.datasets import fashion_mnist

# verileri deÄŸiÅŸenlere atayalÄ±m
(train_data, train_labels), (test_data, test_labels) = fashion_mnist.load_data()
```

    Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz
    32768/29515 [=================================] - 0s 0us/step
    Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz
    26427392/26421880 [==============================] - 0s 0us/step
    Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz
    8192/5148 [===============================================] - 0s 0us/step
    Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz
    4423680/4422102 [==============================] - 0s 0us/step



```python
# Ä°lk train Ã¶rneÄŸine gÃ¶z atalÄ±m
print(f"Training sample:\n{train_data[0]}\n") 
print(f"Training label: {train_labels[0]}")
```

    Training sample:
    [[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
        0   0   0   0   0   0   0   0   0   0]
     [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
        0   0   0   0   0   0   0   0   0   0]
     [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
        0   0   0   0   0   0   0   0   0   0]
     [  0   0   0   0   0   0   0   0   0   0   0   0   1   0   0  13  73   0
        0   1   4   0   0   0   0   1   1   0]
     [  0   0   0   0   0   0   0   0   0   0   0   0   3   0  36 136 127  62
       54   0   0   0   1   3   4   0   0   3]
     [  0   0   0   0   0   0   0   0   0   0   0   0   6   0 102 204 176 134
      144 123  23   0   0   0   0  12  10   0]
     [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 155 236 207 178
      107 156 161 109  64  23  77 130  72  15]
     [  0   0   0   0   0   0   0   0   0   0   0   1   0  69 207 223 218 216
      216 163 127 121 122 146 141  88 172  66]
     [  0   0   0   0   0   0   0   0   0   1   1   1   0 200 232 232 233 229
      223 223 215 213 164 127 123 196 229   0]
     [  0   0   0   0   0   0   0   0   0   0   0   0   0 183 225 216 223 228
      235 227 224 222 224 221 223 245 173   0]
     [  0   0   0   0   0   0   0   0   0   0   0   0   0 193 228 218 213 198
      180 212 210 211 213 223 220 243 202   0]
     [  0   0   0   0   0   0   0   0   0   1   3   0  12 219 220 212 218 192
      169 227 208 218 224 212 226 197 209  52]
     [  0   0   0   0   0   0   0   0   0   0   6   0  99 244 222 220 218 203
      198 221 215 213 222 220 245 119 167  56]
     [  0   0   0   0   0   0   0   0   0   4   0   0  55 236 228 230 228 240
      232 213 218 223 234 217 217 209  92   0]
     [  0   0   1   4   6   7   2   0   0   0   0   0 237 226 217 223 222 219
      222 221 216 223 229 215 218 255  77   0]
     [  0   3   0   0   0   0   0   0   0  62 145 204 228 207 213 221 218 208
      211 218 224 223 219 215 224 244 159   0]
     [  0   0   0   0  18  44  82 107 189 228 220 222 217 226 200 205 211 230
      224 234 176 188 250 248 233 238 215   0]
     [  0  57 187 208 224 221 224 208 204 214 208 209 200 159 245 193 206 223
      255 255 221 234 221 211 220 232 246   0]
     [  3 202 228 224 221 211 211 214 205 205 205 220 240  80 150 255 229 221
      188 154 191 210 204 209 222 228 225   0]
     [ 98 233 198 210 222 229 229 234 249 220 194 215 217 241  65  73 106 117
      168 219 221 215 217 223 223 224 229  29]
     [ 75 204 212 204 193 205 211 225 216 185 197 206 198 213 240 195 227 245
      239 223 218 212 209 222 220 221 230  67]
     [ 48 203 183 194 213 197 185 190 194 192 202 214 219 221 220 236 225 216
      199 206 186 181 177 172 181 205 206 115]
     [  0 122 219 193 179 171 183 196 204 210 213 207 211 210 200 196 194 191
      195 191 198 192 176 156 167 177 210  92]
     [  0   0  74 189 212 191 175 172 175 181 185 188 189 188 193 198 204 209
      210 210 211 188 188 194 192 216 170   0]
     [  2   0   0   0  66 200 222 237 239 242 246 243 244 221 220 193 191 179
      182 182 181 176 166 168  99  58   0   0]
     [  0   0   0   0   0   0   0  40  61  44  72  41  35   0   0   0   0   0
        0   0   0   0   0   0   0   0   0   0]
     [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
        0   0   0   0   0   0   0   0   0   0]
     [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
        0   0   0   0   0   0   0   0   0   0]]
    
    Training label: 9



```python
# Verilerimizin ÅŸeklini kontrol edelim
train_data.shape, train_labels.shape, test_data.shape, test_labels.shape
```




    ((60000, 28, 28), (60000,), (10000, 28, 28), (10000,))




```python
# Tek bir Ã¶rneÄŸin ÅŸeklini kontrol edelim
train_data[0].shape, train_labels[0].shape
```




    ((28, 28), ())



Tamam, her biri ÅŸekil (28, 28) ve bir etiket iÃ§eren 60.000 train Ã¶rneÄŸinin yanÄ± sÄ±ra 10.000 ÅŸekil test Ã¶rneÄŸi (28, 28) vardÄ±r.

Ama bunlar sadece rakamlar, hadi gÃ¶rselleÅŸtirelim.



```python
# Bir Ã¶rneÄŸe gÃ¶z atalÄ±m hemen
import matplotlib.pyplot as plt
plt.imshow(train_data[7]);
```


    
![png](TensorFlow_ile_Sinir_A%C4%9F%C4%B1_S%C4%B1n%C4%B1fland%C4%B1r%C4%B1lmas%C4%B1_files/TensorFlow_ile_Sinir_A%C4%9F%C4%B1_S%C4%B1n%C4%B1fland%C4%B1r%C4%B1lmas%C4%B1_112_0.png)
    



```python
# etiketini kontrol edelim
train_labels[7]
```




    2



GÃ¶rÃ¼nÃ¼ÅŸe gÃ¶re etiketlerimiz sayÄ±sal biÃ§imde. Ve bu bir sinir aÄŸÄ± iÃ§in iyi olsa da, onlarÄ± insan tarafÄ±ndan okunabilir biÃ§imde almak isteyebilirsiniz.

SÄ±nÄ±f adlarÄ±nÄ±n kÃ¼Ã§Ã¼k bir listesini oluÅŸturalÄ±m (bunlarÄ± veri kÃ¼mesinin [`GitHub sayfasÄ±nda`](https://github.com/zalandoresearch/fashion-mnist#labels) bulabiliriz).

ğŸ”‘ Not: Bu veri kÃ¼mesi bizim iÃ§in hazÄ±rlanmÄ±ÅŸ ve kullanÄ±ma hazÄ±r olsa da, birÃ§ok veri kÃ¼mesinin bunun gibi kullanmaya hazÄ±r olmayacaÄŸÄ±nÄ± unutmamak Ã¶nemlidir. Genellikle bir sinir aÄŸÄ±yla kullanÄ±ma hazÄ±r hale getirmek iÃ§in birkaÃ§ Ã¶n iÅŸleme adÄ±mÄ± yapmanÄ±z gerekir (daha sonra kendi verilerimizle Ã§alÄ±ÅŸÄ±rken bunun daha fazlasÄ±nÄ± gÃ¶receÄŸiz).


```python
class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 
               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']

len(class_names)
```




    10



Åimdi bunlara sahibiz, baÅŸka bir Ã¶rnek Ã§izelim.

> ğŸ¤” Soru: Ãœzerinde Ã§alÄ±ÅŸtÄ±ÄŸÄ±mÄ±z verilerin nasÄ±l gÃ¶rÃ¼ndÃ¼ÄŸÃ¼ne Ã¶zellikle dikkat edin. Sadece dÃ¼z Ã§izgiler mi? Yoksa dÃ¼z olmayan Ã§izgileri de var mÄ±? Giysilerin fotoÄŸraflarÄ±nda (aslÄ±nda piksel koleksiyonlarÄ± olan) desenler bulmak istesek, modelimizin doÄŸrusal olmayanlara (dÃ¼z olmayan Ã§izgiler) ihtiyacÄ± olacak mÄ±, olmayacak mÄ±?




```python
# Ã–rnek bir resim ve etiketini gÃ¶rÃ¼ntÃ¼leme
plt.imshow(train_data[17], cmap=plt.cm.binary) # renkleri siyah beyaz olarak deÄŸiÅŸtirme
plt.title(class_names[train_labels[17]]);
```


    
![png](TensorFlow_ile_Sinir_A%C4%9F%C4%B1_S%C4%B1n%C4%B1fland%C4%B1r%C4%B1lmas%C4%B1_files/TensorFlow_ile_Sinir_A%C4%9F%C4%B1_S%C4%B1n%C4%B1fland%C4%B1r%C4%B1lmas%C4%B1_117_0.png)
    



```python
# Moda MNIST'in birden fazla rastgele gÃ¶rÃ¼ntÃ¼
import random
plt.figure(figsize=(7, 7))
for i in range(4):
  ax = plt.subplot(2, 2, i + 1)
  rand_index = random.choice(range(len(train_data)))
  plt.imshow(train_data[rand_index], cmap=plt.cm.binary)
  plt.title(class_names[train_labels[rand_index]])
  plt.axis(False)
```


    
![png](TensorFlow_ile_Sinir_A%C4%9F%C4%B1_S%C4%B1n%C4%B1fland%C4%B1r%C4%B1lmas%C4%B1_files/TensorFlow_ile_Sinir_A%C4%9F%C4%B1_S%C4%B1n%C4%B1fland%C4%B1r%C4%B1lmas%C4%B1_118_0.png)
    


Pekala, piksel deÄŸerleri ve etiketleri arasÄ±ndaki iliÅŸkiyi bulmak iÃ§in bir model oluÅŸturalÄ±m.

Bu Ã§ok sÄ±nÄ±flÄ± bir sÄ±nÄ±flandÄ±rma problemi olduÄŸundan, mimarimizde birkaÃ§ deÄŸiÅŸiklik yapmamÄ±z gerekecek (yukarÄ±daki Tablo 1 ile aynÄ± hizada):

- GiriÅŸ ÅŸekli 28x28 tensÃ¶rlerle (resimlerimizin yÃ¼ksekliÄŸi ve geniÅŸliÄŸi) uÄŸraÅŸmak zorunda kalacak.
AslÄ±nda girdiyi (784) bir tensÃ¶re (vektÃ¶re) sÄ±kÄ±ÅŸtÄ±racaÄŸÄ±z.
Ã‡Ä±ktÄ± ÅŸeklinin 10 olmasÄ± gerekecek Ã§Ã¼nkÃ¼ modelimizin 10 farklÄ± sÄ±nÄ±f iÃ§in tahmin yapmasÄ±na ihtiyacÄ±mÄ±z var.
- AyrÄ±ca Ã§Ä±ktÄ± katmanÄ±mÄ±zÄ±n aktivasyon parametresini 'sigmoid' yerine "softmax" olarak deÄŸiÅŸtireceÄŸiz. 
  - GÃ¶receÄŸimiz gibi, "softmax" etkinleÅŸtirme iÅŸlevi 0 ve 1 arasÄ±nda bir dizi deÄŸer verir (Ã§Ä±ktÄ± ÅŸekliyle aynÄ± ÅŸekil, toplamlarÄ± ~1'e eÅŸittir. En yÃ¼ksek deÄŸere sahip indeks, model tarafÄ±ndan tahmin edilir.
- KayÄ±p fonksiyonumuzu ikili kayÄ±p fonksiyonundan Ã§ok sÄ±nÄ±flÄ± kayÄ±p fonksiyonuna deÄŸiÅŸtirmemiz gerekecek.
  - Daha spesifik olarak, etiketlerimiz tamsayÄ± biÃ§iminde olduÄŸundan, etiketlerimiz one-hot encoding (Ã¶rneÄŸin, [0, 0, 1, 0, 0 gibi gÃ¶rÃ¼nÃ¼yorlardÄ±) tf.keras.losses.SparseCategoricalCrossentropy() kullanacaÄŸÄ±z. ..]), tf.keras.losses.CategoricalCrossentropy() kullanÄ±rdÄ±k.
- fit() iÅŸlevini Ã§aÄŸÄ±rÄ±rken validation_data parametresini de kullanacaÄŸÄ±z. Bu bize eÄŸitim sÄ±rasÄ±nda modelin test setinde nasÄ±l performans gÃ¶sterdiÄŸi hakkÄ±nda bir fikir verecektir.



```python
tf.random.set_seed(42)

# bir model oluÅŸturma
model_11 = tf.keras.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)), 
  tf.keras.layers.Dense(4, activation="relu"),
  tf.keras.layers.Dense(4, activation="relu"),
  tf.keras.layers.Dense(10, activation="softmax") 
])

# modeli derleme
model_11.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), 
                 optimizer=tf.keras.optimizers.Adam(),
                 metrics=["accuracy"])

# modeli fit etme
non_norm_history = model_11.fit(train_data,
                                train_labels,
                                epochs=10,
                                validation_data=(test_data, test_labels))
```

    Epoch 1/10
    1875/1875 [==============================] - 4s 2ms/step - loss: 2.1671 - accuracy: 0.1606 - val_loss: 1.7959 - val_accuracy: 0.2046
    Epoch 2/10
    1875/1875 [==============================] - 3s 2ms/step - loss: 1.7066 - accuracy: 0.2509 - val_loss: 1.6567 - val_accuracy: 0.2805
    Epoch 3/10
    1875/1875 [==============================] - 3s 2ms/step - loss: 1.6321 - accuracy: 0.2806 - val_loss: 1.6094 - val_accuracy: 0.2857
    Epoch 4/10
    1875/1875 [==============================] - 3s 2ms/step - loss: 1.6052 - accuracy: 0.2833 - val_loss: 1.6041 - val_accuracy: 0.2859
    Epoch 5/10
    1875/1875 [==============================] - 3s 2ms/step - loss: 1.5975 - accuracy: 0.2862 - val_loss: 1.6064 - val_accuracy: 0.2756
    Epoch 6/10
    1875/1875 [==============================] - 3s 2ms/step - loss: 1.5950 - accuracy: 0.2920 - val_loss: 1.5747 - val_accuracy: 0.2994
    Epoch 7/10
    1875/1875 [==============================] - 3s 2ms/step - loss: 1.5775 - accuracy: 0.3040 - val_loss: 1.6030 - val_accuracy: 0.3000
    Epoch 8/10
    1875/1875 [==============================] - 3s 2ms/step - loss: 1.5708 - accuracy: 0.3175 - val_loss: 1.5635 - val_accuracy: 0.3315
    Epoch 9/10
    1875/1875 [==============================] - 3s 2ms/step - loss: 1.5638 - accuracy: 0.3280 - val_loss: 1.5534 - val_accuracy: 0.3334
    Epoch 10/10
    1875/1875 [==============================] - 3s 2ms/step - loss: 1.5432 - accuracy: 0.3346 - val_loss: 1.5390 - val_accuracy: 0.3549



```python
# Modelimizin ÅŸekillerini kontrol edelim
model_11.summary()
```

    Model: "sequential_10"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    flatten (Flatten)            (None, 784)               0         
    _________________________________________________________________
    dense_26 (Dense)             (None, 4)                 3140      
    _________________________________________________________________
    dense_27 (Dense)             (None, 4)                 20        
    _________________________________________________________________
    dense_28 (Dense)             (None, 10)                50        
    =================================================================
    Total params: 3,210
    Trainable params: 3,210
    Non-trainable params: 0
    _________________________________________________________________


Pekala, modelimiz, ikili sÄ±nÄ±flandÄ±rma problemimizde kullandÄ±ÄŸÄ±mÄ±za benzer bir stil modeli kullanarak 10 epochtan sonra yaklaÅŸÄ±k %35 doÄŸruluÄŸa ulaÅŸÄ±yor.

Hangisi tahmin etmekten daha iyidir (10 sÄ±nÄ±fla tahmin etmek yaklaÅŸÄ±k %10 doÄŸrulukla sonuÃ§lanÄ±r), ancak daha iyisini yapabiliriz.

0 ile 1 arasÄ±ndaki sayÄ±larÄ± tercih eden sinir aÄŸlarÄ±ndan bahsettiÄŸimizi hatÄ±rlÄ±yor musunuz? (eÄŸer hatÄ±rlamÄ±yorsanÄ±z bunu bir hatÄ±rlatma olarak kabul edin)

Åu anda elimizdeki veriler 0 ile 1 arasÄ±nda deÄŸil, baÅŸka bir deyiÅŸle, normalleÅŸtirilmedi (bu nedenle fit()'i Ã§aÄŸÄ±rÄ±rken non_norm_history deÄŸiÅŸkenini kullandÄ±k). Piksel deÄŸerleri 0 ile 255 arasÄ±ndadÄ±r.


```python
# EÄŸitim verilerinin minimum ve maksimum deÄŸerlerini kontrol edin
train_data.min(), train_data.max()
```




    (0, 255)



Bu deÄŸerleri 0 ile 1 arasÄ±nda, tÃ¼m diziyi maksimuma bÃ¶lerek elde edebiliriz: 0-255.

Bunu yapmak, tÃ¼m verilerimizin 0 ile 1 arasÄ±nda olmasÄ±na neden olur (Ã¶lÃ§eklendirme veya normalleÅŸtirme olarak bilinir).


```python
# traini bÃ¶lÃ¼n ve gÃ¶rÃ¼ntÃ¼leri maksimum deÄŸere gÃ¶re test edin (normalleÅŸtirin)
train_data = train_data / 255.0
test_data = test_data / 255.0

# EÄŸitim verilerinin minimum ve maksimum deÄŸerlerini kontrol edin
train_data.min(), train_data.max()
```




    (0.0, 1.0)



GÃ¼zel! Åimdi verilerimiz 0 ile 1 arasÄ±nda. ModellediÄŸimiz zaman bakalÄ±m ne olacak. Daha Ã¶nce olduÄŸu gibi (model_11) aynÄ± modeli kullanacaÄŸÄ±z, ancak bu sefer veriler normalize edilecek.


```python
tf.random.set_seed(42)

# bir model oluÅŸturma
model_12 = tf.keras.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)), 
  tf.keras.layers.Dense(4, activation="relu"),
  tf.keras.layers.Dense(4, activation="relu"),
  tf.keras.layers.Dense(10, activation="softmax") 
])

# modeli derleme
model_12.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), 
                 optimizer=tf.keras.optimizers.Adam(),
                 metrics=["accuracy"])

# modeli fit etme
norm_history = model_12.fit(train_data,
                                train_labels,
                                epochs=10,
                                validation_data=(test_data, test_labels))
```

    Epoch 1/10
    1875/1875 [==============================] - 4s 2ms/step - loss: 1.0348 - accuracy: 0.6474 - val_loss: 0.6937 - val_accuracy: 0.7617
    Epoch 2/10
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.6376 - accuracy: 0.7757 - val_loss: 0.6400 - val_accuracy: 0.7820
    Epoch 3/10
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.5942 - accuracy: 0.7914 - val_loss: 0.6247 - val_accuracy: 0.7783
    Epoch 4/10
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.5750 - accuracy: 0.7979 - val_loss: 0.6078 - val_accuracy: 0.7881
    Epoch 5/10
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.5641 - accuracy: 0.8006 - val_loss: 0.6169 - val_accuracy: 0.7881
    Epoch 6/10
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.5544 - accuracy: 0.8043 - val_loss: 0.5855 - val_accuracy: 0.7951
    Epoch 7/10
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.5488 - accuracy: 0.8063 - val_loss: 0.6097 - val_accuracy: 0.7836
    Epoch 8/10
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.5428 - accuracy: 0.8077 - val_loss: 0.5787 - val_accuracy: 0.7971
    Epoch 9/10
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.5373 - accuracy: 0.8097 - val_loss: 0.5698 - val_accuracy: 0.7977
    Epoch 10/10
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.5360 - accuracy: 0.8124 - val_loss: 0.5658 - val_accuracy: 0.8014


Vayy, daha Ã¶nce olduÄŸu gibi aynÄ± modeli kullandÄ±k ama normalleÅŸtirilmiÅŸ verilerle artÄ±k Ã§ok daha yÃ¼ksek bir doÄŸruluk deÄŸeri gÃ¶rÃ¼yoruz!

Her modelin history deÄŸerlerini (kayÄ±p eÄŸrilerini) gÃ¶rselleÅŸtirelim.



```python
import pandas as pd
pd.DataFrame(non_norm_history.history).plot(title="Non-normalized Data")
pd.DataFrame(norm_history.history).plot(title="Normalized data");
```


    
![png](TensorFlow_ile_Sinir_A%C4%9F%C4%B1_S%C4%B1n%C4%B1fland%C4%B1r%C4%B1lmas%C4%B1_files/TensorFlow_ile_Sinir_A%C4%9F%C4%B1_S%C4%B1n%C4%B1fland%C4%B1r%C4%B1lmas%C4%B1_129_0.png)
    



    
![png](TensorFlow_ile_Sinir_A%C4%9F%C4%B1_S%C4%B1n%C4%B1fland%C4%B1r%C4%B1lmas%C4%B1_files/TensorFlow_ile_Sinir_A%C4%9F%C4%B1_S%C4%B1n%C4%B1fland%C4%B1r%C4%B1lmas%C4%B1_129_1.png)
    


Vay. Bu iki grafikten, normalize verili modelimizin (model_12) normalleÅŸtirilmemiÅŸ verili modele (model_11) gÃ¶re ne kadar hÄ±zlÄ± geliÅŸtiÄŸini gÃ¶rebiliriz.

> ğŸ”‘ Not: Biraz farklÄ± verilere sahip aynÄ± model, Ã¶nemli Ã¶lÃ§Ã¼de farklÄ± sonuÃ§lar Ã¼retebilir. Bu nedenle, modelleri karÅŸÄ±laÅŸtÄ±rÄ±rken, onlarÄ± aynÄ± kriterlere gÃ¶re karÅŸÄ±laÅŸtÄ±rdÄ±ÄŸÄ±nÄ±zdan emin olmanÄ±z Ã¶nemlidir (Ã¶rneÄŸin, aynÄ± mimari ancak farklÄ± veriler veya aynÄ± veriler ancak farklÄ± mimari). Ä°deal Ã¶ÄŸrenme oranÄ±nÄ± bulup ne olduÄŸunu gÃ¶rmeye ne dersiniz? Kullanmakta olduÄŸumuz mimarinin aynÄ±sÄ±nÄ± kullanacaÄŸÄ±z.


```python
tf.random.set_seed(42)

# bir model oluÅŸturma
model_13 = tf.keras.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)), 
  tf.keras.layers.Dense(4, activation="relu"),
  tf.keras.layers.Dense(4, activation="relu"),
  tf.keras.layers.Dense(10, activation="softmax") 
])

# modeli derleme
model_13.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), 
                 optimizer=tf.keras.optimizers.Adam(),
                 metrics=["accuracy"])

lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-3 * 10**(epoch/20))

# modeli fit etme
find_lr_history = model_13.fit(train_data,
                                train_labels,
                                epochs=40,
                                validation_data=(test_data, test_labels),
                                callbacks=[lr_scheduler])
```

    Epoch 1/40
    1875/1875 [==============================] - 4s 2ms/step - loss: 1.0348 - accuracy: 0.6474 - val_loss: 0.6937 - val_accuracy: 0.7617
    Epoch 2/40
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.6366 - accuracy: 0.7759 - val_loss: 0.6400 - val_accuracy: 0.7808
    Epoch 3/40
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.5934 - accuracy: 0.7911 - val_loss: 0.6278 - val_accuracy: 0.7770
    Epoch 4/40
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.5749 - accuracy: 0.7969 - val_loss: 0.6122 - val_accuracy: 0.7871
    Epoch 5/40
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.5655 - accuracy: 0.7987 - val_loss: 0.6061 - val_accuracy: 0.7913
    Epoch 6/40
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.5569 - accuracy: 0.8022 - val_loss: 0.5917 - val_accuracy: 0.7940
    Epoch 7/40
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.5542 - accuracy: 0.8036 - val_loss: 0.5898 - val_accuracy: 0.7896
    Epoch 8/40
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.5509 - accuracy: 0.8039 - val_loss: 0.5829 - val_accuracy: 0.7949
    Epoch 9/40
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.5468 - accuracy: 0.8047 - val_loss: 0.6036 - val_accuracy: 0.7833
    Epoch 10/40
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.5478 - accuracy: 0.8058 - val_loss: 0.5736 - val_accuracy: 0.7974
    Epoch 11/40
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.5446 - accuracy: 0.8059 - val_loss: 0.5672 - val_accuracy: 0.8016
    Epoch 12/40
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.5432 - accuracy: 0.8067 - val_loss: 0.5773 - val_accuracy: 0.7950
    Epoch 13/40
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.5425 - accuracy: 0.8056 - val_loss: 0.5775 - val_accuracy: 0.7992
    Epoch 14/40
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.5407 - accuracy: 0.8078 - val_loss: 0.5616 - val_accuracy: 0.8075
    Epoch 15/40
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.5408 - accuracy: 0.8052 - val_loss: 0.5773 - val_accuracy: 0.8039
    Epoch 16/40
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.5437 - accuracy: 0.8058 - val_loss: 0.5682 - val_accuracy: 0.8015
    Epoch 17/40
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.5419 - accuracy: 0.8075 - val_loss: 0.5995 - val_accuracy: 0.7964
    Epoch 18/40
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.5488 - accuracy: 0.8058 - val_loss: 0.5544 - val_accuracy: 0.8087
    Epoch 19/40
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.5506 - accuracy: 0.8042 - val_loss: 0.6068 - val_accuracy: 0.7864
    Epoch 20/40
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.5537 - accuracy: 0.8030 - val_loss: 0.5597 - val_accuracy: 0.8076
    Epoch 21/40
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.5572 - accuracy: 0.8036 - val_loss: 0.5998 - val_accuracy: 0.7934
    Epoch 22/40
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.5615 - accuracy: 0.8013 - val_loss: 0.5756 - val_accuracy: 0.8034
    Epoch 23/40
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.5655 - accuracy: 0.8017 - val_loss: 0.6386 - val_accuracy: 0.7668
    Epoch 24/40
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.5819 - accuracy: 0.7963 - val_loss: 0.6356 - val_accuracy: 0.7869
    Epoch 25/40
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.5810 - accuracy: 0.7977 - val_loss: 0.6481 - val_accuracy: 0.7865
    Epoch 26/40
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.5960 - accuracy: 0.7901 - val_loss: 0.6997 - val_accuracy: 0.7802
    Epoch 27/40
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.6101 - accuracy: 0.7870 - val_loss: 0.6124 - val_accuracy: 0.7917
    Epoch 28/40
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.6178 - accuracy: 0.7846 - val_loss: 0.6137 - val_accuracy: 0.7962
    Epoch 29/40
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.6357 - accuracy: 0.7771 - val_loss: 0.6655 - val_accuracy: 0.7621
    Epoch 30/40
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.6671 - accuracy: 0.7678 - val_loss: 0.7597 - val_accuracy: 0.7194
    Epoch 31/40
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.6836 - accuracy: 0.7585 - val_loss: 0.6958 - val_accuracy: 0.7342
    Epoch 32/40
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.7062 - accuracy: 0.7553 - val_loss: 0.7015 - val_accuracy: 0.7732
    Epoch 33/40
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.7383 - accuracy: 0.7500 - val_loss: 0.7146 - val_accuracy: 0.7706
    Epoch 34/40
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.8033 - accuracy: 0.7300 - val_loss: 0.8987 - val_accuracy: 0.6848
    Epoch 35/40
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.8429 - accuracy: 0.7110 - val_loss: 0.8750 - val_accuracy: 0.7053
    Epoch 36/40
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.8651 - accuracy: 0.7033 - val_loss: 0.8176 - val_accuracy: 0.6989
    Epoch 37/40
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.9203 - accuracy: 0.6837 - val_loss: 0.7876 - val_accuracy: 0.7333
    Epoch 38/40
    1875/1875 [==============================] - 3s 2ms/step - loss: 1.2374 - accuracy: 0.5191 - val_loss: 1.3699 - val_accuracy: 0.4902
    Epoch 39/40
    1875/1875 [==============================] - 3s 2ms/step - loss: 1.1828 - accuracy: 0.5311 - val_loss: 1.1010 - val_accuracy: 0.5819
    Epoch 40/40
    1875/1875 [==============================] - 3s 2ms/step - loss: 1.6640 - accuracy: 0.3303 - val_loss: 1.8528 - val_accuracy: 0.2779



```python
import numpy as np
import matplotlib.pyplot as plt
lrs = 1e-3 * (10**(np.arange(40)/20))
plt.semilogx(lrs, find_lr_history.history["loss"])
plt.xlabel("earning rate")
plt.ylabel("Loss")
plt.title("Finding the ideal learning rate");
```


    
![png](TensorFlow_ile_Sinir_A%C4%9F%C4%B1_S%C4%B1n%C4%B1fland%C4%B1r%C4%B1lmas%C4%B1_files/TensorFlow_ile_Sinir_A%C4%9F%C4%B1_S%C4%B1n%C4%B1fland%C4%B1r%C4%B1lmas%C4%B1_132_0.png)
    


Bu durumda, Adam optimizer'Ä±n (0.001) varsayÄ±lan Ã¶ÄŸrenme oranÄ±na yakÄ±n bir yerde ideal Ã¶ÄŸrenme oranÄ± gibi gÃ¶rÃ¼nÃ¼yor.

Ä°deal Ã¶ÄŸrenme oranÄ±nÄ± kullanarak bir modeli yeniden yerleÅŸtirelim.



```python
tf.random.set_seed(42)

# bir model oluÅŸturma
model_14 = tf.keras.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)), 
  tf.keras.layers.Dense(4, activation="relu"),
  tf.keras.layers.Dense(4, activation="relu"),
  tf.keras.layers.Dense(10, activation="softmax") 
])

# modeli derleme
model_14.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), 
                 optimizer=tf.keras.optimizers.Adam(),
                 metrics=["accuracy"])

lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-3 * 10**(epoch/20))

# modeli fit etme
history = model_14.fit(train_data,
                                train_labels,
                                epochs=20,
                                validation_data=(test_data, test_labels),
                                callbacks=[lr_scheduler])
```

    Epoch 1/20
    1875/1875 [==============================] - 4s 2ms/step - loss: 1.0348 - accuracy: 0.6474 - val_loss: 0.6937 - val_accuracy: 0.7617
    Epoch 2/20
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.6366 - accuracy: 0.7759 - val_loss: 0.6400 - val_accuracy: 0.7808
    Epoch 3/20
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.5934 - accuracy: 0.7911 - val_loss: 0.6278 - val_accuracy: 0.7770
    Epoch 4/20
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.5749 - accuracy: 0.7969 - val_loss: 0.6122 - val_accuracy: 0.7871
    Epoch 5/20
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.5655 - accuracy: 0.7987 - val_loss: 0.6061 - val_accuracy: 0.7913
    Epoch 6/20
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.5569 - accuracy: 0.8022 - val_loss: 0.5917 - val_accuracy: 0.7940
    Epoch 7/20
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.5542 - accuracy: 0.8036 - val_loss: 0.5898 - val_accuracy: 0.7896
    Epoch 8/20
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.5509 - accuracy: 0.8039 - val_loss: 0.5829 - val_accuracy: 0.7949
    Epoch 9/20
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.5468 - accuracy: 0.8047 - val_loss: 0.6036 - val_accuracy: 0.7833
    Epoch 10/20
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.5478 - accuracy: 0.8058 - val_loss: 0.5736 - val_accuracy: 0.7974
    Epoch 11/20
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.5446 - accuracy: 0.8059 - val_loss: 0.5672 - val_accuracy: 0.8016
    Epoch 12/20
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.5432 - accuracy: 0.8067 - val_loss: 0.5773 - val_accuracy: 0.7950
    Epoch 13/20
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.5425 - accuracy: 0.8056 - val_loss: 0.5775 - val_accuracy: 0.7992
    Epoch 14/20
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.5407 - accuracy: 0.8078 - val_loss: 0.5616 - val_accuracy: 0.8075
    Epoch 15/20
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.5408 - accuracy: 0.8052 - val_loss: 0.5773 - val_accuracy: 0.8039
    Epoch 16/20
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.5437 - accuracy: 0.8058 - val_loss: 0.5682 - val_accuracy: 0.8015
    Epoch 17/20
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.5419 - accuracy: 0.8075 - val_loss: 0.5995 - val_accuracy: 0.7964
    Epoch 18/20
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.5488 - accuracy: 0.8058 - val_loss: 0.5544 - val_accuracy: 0.8087
    Epoch 19/20
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.5506 - accuracy: 0.8042 - val_loss: 0.6068 - val_accuracy: 0.7864
    Epoch 20/20
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.5537 - accuracy: 0.8030 - val_loss: 0.5597 - val_accuracy: 0.8076


Åimdi ideale yakÄ±n bir Ã¶ÄŸrenme oranÄ±yla eÄŸitilmiÅŸ ve oldukÃ§a iyi performans gÃ¶steren bir modelimiz var, birkaÃ§ seÃ§eneÄŸimiz var.

Yapabiliriz:
- DiÄŸer sÄ±nÄ±flandÄ±rma Ã¶lÃ§Ã¼tlerini (karÄ±ÅŸÄ±klÄ±k matrisi veya sÄ±nÄ±flandÄ±rma raporu gibi) kullanarak performansÄ±nÄ± deÄŸerlendirin.
- Tahminlerinden bazÄ±larÄ±nÄ± deÄŸerlendirin (gÃ¶rselleÅŸtirmeler aracÄ±lÄ±ÄŸÄ±yla).
- DoÄŸruluÄŸunu artÄ±rÄ±n (daha uzun sÃ¼re eÄŸiterek veya mimariyi deÄŸiÅŸtirerek).
- Bir uygulamada kullanmak Ã¼zere kaydedin ve dÄ±ÅŸa aktarÄ±n.
Ä°lk iki seÃ§eneÄŸi inceleyelim.

Ä°lk olarak, farklÄ± sÄ±nÄ±flardaki tahminlerini gÃ¶rselleÅŸtirmek iÃ§in bir sÄ±nÄ±flandÄ±rma matrisi oluÅŸturacaÄŸÄ±z.


```python
import itertools
from sklearn.metrics import confusion_matrix

def make_confusion_matrix(y_true, y_pred, classes=None, figsize=(10, 10), text_size=15): 

  cm = confusion_matrix(y_true, y_pred)
  cm_norm = cm.astype("float") / cm.sum(axis=1)[:, np.newaxis] 
  n_classes = cm.shape[0] 
  
  fig, ax = plt.subplots(figsize=figsize)
  cax = ax.matshow(cm, cmap=plt.cm.Blues) 
  fig.colorbar(cax)

  if classes:
    labels = classes
  else:
    labels = np.arange(cm.shape[0])

  ax.set(title="Confusion Matrix",
         xlabel="Predicted label",
         ylabel="True label",
         xticks=np.arange(n_classes),
         yticks=np.arange(n_classes), 
         xticklabels=labels, 
         yticklabels=labels)
  
  ax.xaxis.set_label_position("bottom")
  ax.xaxis.tick_bottom()

  threshold = (cm.max() + cm.min()) / 2.

  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
    plt.text(j, i, f"{cm[i, j]} ({cm_norm[i, j]*100:.1f}%)",
             horizontalalignment="center",
             color="white" if cm[i, j] > threshold else "black",
             size=text_size)
```


```python
# En son modelle tahminler yapÄ±n
y_probs = model_14.predict(test_data)
y_probs[:5]
```




    array([[1.2742422e-09, 5.3467939e-08, 8.3849936e-06, 7.0589194e-06,
            9.7621414e-06, 5.4065306e-02, 7.2848763e-08, 6.3599236e-02,
            1.8366773e-03, 8.8047338e-01],
           [1.2383326e-05, 2.3110788e-17, 9.0172756e-01, 3.3549309e-06,
            5.8316510e-02, 1.4708356e-14, 3.9687403e-02, 3.2329574e-30,
            2.5285641e-04, 3.8686530e-21],
           [1.4126861e-04, 9.8552668e-01, 7.5693160e-06, 1.4010524e-02,
            2.0143854e-04, 9.7611138e-12, 1.0748472e-04, 4.0757726e-08,
            4.5021643e-06, 5.8545976e-07],
           [3.9738816e-06, 9.9383062e-01, 2.1684928e-06, 5.7096859e-03,
            2.5507511e-04, 4.3098709e-11, 1.8117787e-05, 1.1347497e-06,
            2.2362808e-06, 1.7706068e-04],
           [2.8454942e-01, 7.6419547e-06, 1.0144152e-01, 1.9927604e-02,
            3.2220736e-02, 1.3905409e-12, 5.6163126e-01, 1.6594300e-18,
            2.2172352e-04, 2.4364064e-15]], dtype=float32)



Modelimiz bir tahmin olasÄ±lÄ±klarÄ± listesi verir, yani belirli bir sÄ±nÄ±fÄ±n etiket olma olasÄ±lÄ±ÄŸÄ±nÄ±n ne kadar muhtemel olduÄŸuna dair bir sayÄ± verir.

Tahmin olasÄ±lÄ±klarÄ± listesindeki sayÄ± ne kadar yÃ¼ksekse, modelin doÄŸru sÄ±nÄ±f olduÄŸuna inanmasÄ± o kadar olasÄ±dÄ±r.

En yÃ¼ksek deÄŸeri bulmak iÃ§in argmax() yÃ¶ntemini kullanabiliriz.



```python
# Ä°lk Ã¶rnek iÃ§in Ã¶ngÃ¶rÃ¼len sÄ±nÄ±f numarasÄ±na ve etiketine bakÄ±n
y_probs[0].argmax(), class_names[y_probs[0].argmax()]
```




    (9, 'Ankle boot')




```python
# OlasÄ±lÄ±klardan tÃ¼m tahminleri etiketlere dÃ¶nÃ¼ÅŸtÃ¼rÃ¼n
y_preds = y_probs.argmax(axis=1)

# Ä°lk 10 tahmin etiketini gÃ¶rÃ¼ntÃ¼leyin
y_preds[:10]
```




    array([9, 2, 1, 1, 6, 1, 4, 6, 5, 7])



Harika, ÅŸimdi modelimizin tahminlerini etiket biÃ§iminde aldÄ±k, onlarÄ± doÄŸruluk etiketlerine karÅŸÄ± gÃ¶rmek iÃ§in bir karÄ±ÅŸÄ±klÄ±k matrisi oluÅŸturalÄ±m.


```python
# GÃ¼zelleÅŸtirilmemiÅŸ karÄ±ÅŸÄ±klÄ±k matrisine gÃ¶z atÄ±n
confusion_matrix(y_true=test_labels, 
                 y_pred=y_preds)
```




    array([[868,   5,  17,  60,   1,   0,  36,   0,  12,   1],
           [  3, 951,   4,  29,   5,   5,   3,   0,   0,   0],
           [ 50,   3, 667,  11, 158,   2,  97,   0,  12,   0],
           [ 86,  18,  11, 824,  20,   1,  27,   2,  11,   0],
           [  6,   1, 116,  48, 728,   3,  93,   0,   5,   0],
           [  0,   2,   0,   0,   0, 862,   0,  78,   9,  49],
           [263,   5, 155,  45, 123,   5, 390,   0,  14,   0],
           [  0,   0,   0,   0,   0,  21,   0, 924,   1,  54],
           [ 15,   1,  35,  16,   3,   4,   5,   5, 916,   0],
           [  0,   3,   0,   0,   1,   8,   0,  39,   3, 946]])




```python
make_confusion_matrix(y_true=test_labels, 
                      y_pred=y_preds,
                      classes=class_names,
                      figsize=(15, 15),
                      text_size=10)
```


    
![png](TensorFlow_ile_Sinir_A%C4%9F%C4%B1_S%C4%B1n%C4%B1fland%C4%B1r%C4%B1lmas%C4%B1_files/TensorFlow_ile_Sinir_A%C4%9F%C4%B1_S%C4%B1n%C4%B1fland%C4%B1r%C4%B1lmas%C4%B1_143_0.png)
    


Bu Ã§ok daha iyi gÃ¶rÃ¼nÃ¼yor. SonuÃ§larÄ±n o kadar iyi olmamasÄ± dÄ±ÅŸÄ±nda... GÃ¶rÃ¼nÃ¼ÅŸe gÃ¶re modelimizin GÃ¶mlek ve T-shirt/Ã¼st sÄ±nÄ±flar arasÄ±nda kafasÄ± karÄ±ÅŸÄ±yor.

Bir karÄ±ÅŸÄ±klÄ±k matrisi kullanarak model tahminlerimizin doÄŸruluk etiketleriyle nasÄ±l hizalandÄ±ÄŸÄ±nÄ± gÃ¶rdÃ¼k, peki ya bazÄ±larÄ±nÄ± gÃ¶rselleÅŸtirmeye ne dersiniz?

Tahmini ile birlikte rastgele bir gÃ¶rÃ¼ntÃ¼ Ã§izmek iÃ§in bir fonksiyon oluÅŸturalÄ±m.

> ğŸ”‘ Not: Genellikle gÃ¶rÃ¼ntÃ¼lerle ve diÄŸer gÃ¶rsel veri biÃ§imleriyle Ã§alÄ±ÅŸÄ±rken, verileri ve modelinizin Ã§Ä±ktÄ±larÄ±nÄ± daha iyi anlamak iÃ§in mÃ¼mkÃ¼n olduÄŸunca gÃ¶rselleÅŸtirmek iyi bir fikirdir.


```python
 import random

def plot_random_image(model, images, true_labels, classes):
  i = random.randint(0, len(images))
  
  # Tahminler ve hedefleri oluÅŸturalÄ±m
  target_image = images[i]
  pred_probs = model.predict(target_image.reshape(1, 28, 28))
  pred_label = classes[pred_probs.argmax()]
  true_label = classes[true_labels[i]]

  # Hedef gÃ¶rÃ¼ntÃ¼yÃ¼ Ã§izelim
  plt.imshow(target_image, cmap=plt.cm.binary)

  # Tahminin doÄŸru veya yanlÄ±ÅŸ olmasÄ±na baÄŸlÄ± olarak baÅŸlÄ±klarÄ±n rengini deÄŸiÅŸtirelim
  if pred_label == true_label:
    color = "green"
  else:
    color = "red"

  plt.xlabel("Pred: {} {:2.0f}% (True: {})".format(pred_label,
                                                   100*tf.reduce_max(pred_probs),
                                                   true_label),
             color=color)
```


```python
from sklearn.metrics import confusion_matrix
confusion_matrix(y_true=test_labels, 
                 y_pred=y_preds)
```




    array([[868,   5,  17,  60,   1,   0,  36,   0,  12,   1],
           [  3, 951,   4,  29,   5,   5,   3,   0,   0,   0],
           [ 50,   3, 667,  11, 158,   2,  97,   0,  12,   0],
           [ 86,  18,  11, 824,  20,   1,  27,   2,  11,   0],
           [  6,   1, 116,  48, 728,   3,  93,   0,   5,   0],
           [  0,   2,   0,   0,   0, 862,   0,  78,   9,  49],
           [263,   5, 155,  45, 123,   5, 390,   0,  14,   0],
           [  0,   0,   0,   0,   0,  21,   0, 924,   1,  54],
           [ 15,   1,  35,  16,   3,   4,   5,   5, 916,   0],
           [  0,   3,   0,   0,   1,   8,   0,  39,   3, 946]])




```python
plot_random_image(model=model_14, 
                  images=test_data, 
                  true_labels=test_labels, 
                  classes=class_names)
```


    
![png](TensorFlow_ile_Sinir_A%C4%9F%C4%B1_S%C4%B1n%C4%B1fland%C4%B1r%C4%B1lmas%C4%B1_files/TensorFlow_ile_Sinir_A%C4%9F%C4%B1_S%C4%B1n%C4%B1fland%C4%B1r%C4%B1lmas%C4%B1_147_0.png)
    


YukarÄ±daki hÃ¼creyi birkaÃ§ kez Ã§alÄ±ÅŸtÄ±rdÄ±ktan sonra, modelin tahminleri ile gerÃ§ek etiketler arasÄ±ndaki iliÅŸkiyi gÃ¶rsel olarak anlamaya baÅŸlayacaksÄ±nÄ±z.

Modelin hangi tahminlerde kafasÄ±nÄ±n karÄ±ÅŸtÄ±ÄŸÄ±nÄ± anladÄ±nÄ±z mÄ±?

Benzer sÄ±nÄ±flarÄ± karÄ±ÅŸtÄ±rÄ±yor gibi gÃ¶rÃ¼nÃ¼yor, Ã¶rneÄŸin Sneaker with Ankle boot. Resimlere baktÄ±ÄŸÄ±nÄ±zda bunun nasÄ±l olabileceÄŸini gÃ¶rebilirsiniz. Bir Sneaker ve Ankle Boot'un genel ÅŸekli benzerdir. Genel ÅŸekil, modelin Ã¶ÄŸrendiÄŸi kalÄ±plardan biri olabilir ve bu nedenle, iki gÃ¶rÃ¼ntÃ¼ benzer bir ÅŸekle sahip olduÄŸunda, tahminleri karÄ±ÅŸÄ±r.


## Modelimiz Hangi KalÄ±plarÄ± Ã–ÄŸreniyor?

Bir sinir aÄŸÄ±nÄ±n sayÄ±lardaki kalÄ±plarÄ± nasÄ±l bulduÄŸu hakkÄ±nda Ã§ok konuÅŸtuk ama bu kalÄ±plar tam olarak neye benziyor? Modellerimizden birini aÃ§Ä±p Ã¶ÄŸrenelim.

Ä°lk olarak, en son modelimizde (model_14) katmanlar Ã¶zniteliÄŸini kullanarak katmanlarÄ±n bir listesini alacaÄŸÄ±z.


```python
# En son modelimizin katmanlarÄ±nÄ± bulun
model_14.layers
```




    [<tensorflow.python.keras.layers.core.Flatten at 0x7f64a0b61b10>,
     <tensorflow.python.keras.layers.core.Dense at 0x7f64a0b61550>,
     <tensorflow.python.keras.layers.core.Dense at 0x7f64a0af9950>,
     <tensorflow.python.keras.layers.core.Dense at 0x7f64a0acdf50>]



Ä°ndeksleme kullanarak bir hedef katmana eriÅŸebiliriz.


```python
# Belirli bir katmanÄ± gÃ¶rÃ¼ntÃ¼leme
model_14.layers[1]
```




    <tensorflow.python.keras.layers.core.Dense at 0x7f64a0b61550>



Ve `get_weights()` yÃ¶ntemini kullanarak belirli bir katman tarafÄ±ndan Ã¶ÄŸrenilen kalÄ±plarÄ± bulabiliriz.

`get_weights()` yÃ¶ntemi, belirli bir katmanÄ±n aÄŸÄ±rlÄ±klarÄ±nÄ± (aÄŸÄ±rlÄ±k matrisi olarak da bilinir) ve sapmalarÄ±nÄ± (Ã¶nyargÄ± vektÃ¶rÃ¼ olarak da bilinir) dÃ¶ndÃ¼rÃ¼r.



```python
weights, biases = model_14.layers[1].get_weights()
weights, weights.shape
```




    (array([[ 3.0885503 , -2.430857  ,  0.45438388, -3.0628507 ],
            [ 0.98286426, -2.71804   , -0.38760266, -1.1560956 ],
            [ 2.6185486 , -1.6931161 , -2.659585  , -2.343221  ],
            ...,
            [-0.5499583 ,  2.1220326 , -0.22042169,  0.75220233],
            [-0.5888785 ,  3.346401  ,  1.4520893 , -1.5131956 ],
            [ 0.90688974, -0.6245389 ,  0.64969605,  0.05348392]],
           dtype=float32), (784, 4))



AÄŸÄ±rlÄ±k matrisi, bizim durumumuzda 784 (28x28 piksel) olan giriÅŸ verileriyle aynÄ± ÅŸekildedir. Ve seÃ§ilen katmandaki her nÃ¶ron iÃ§in aÄŸÄ±rlÄ±k matrisinin bir kopyasÄ± var (seÃ§ilen katmanÄ±mÄ±zda 4 nÃ¶ron var).

AÄŸÄ±rlÄ±k matrisindeki her deÄŸer, girdi verilerindeki belirli bir deÄŸerin aÄŸÄ±n kararlarÄ±nÄ± nasÄ±l etkilediÄŸine karÅŸÄ±lÄ±k gelir.

Bu deÄŸerler rastgele sayÄ±lar olarak baÅŸlar (bir katman oluÅŸtururken kernel_initializer parametresi tarafÄ±ndan ayarlanÄ±rlar, varsayÄ±lan "glorot_uniform"dur) ve daha sonra eÄŸitim sÄ±rasÄ±nda sinir aÄŸÄ± tarafÄ±ndan verilerin daha iyi temsili deÄŸerlerine (rastgele olmayan) gÃ¼ncellenir. 


```python
biases, biases.shape
```




    (array([ 2.1505804 ,  0.45967796, -0.38694024,  2.9040031 ], dtype=float32),
     (4,))



Her nÃ¶ronun bir Ã¶nyargÄ± vektÃ¶rÃ¼ vardÄ±r. BunlarÄ±n her biri bir aÄŸÄ±rlÄ±k matrisi ile eÅŸleÅŸtirilir. Ã–nyargÄ± deÄŸerleri varsayÄ±lan olarak sÄ±fÄ±r olarak baÅŸlatÄ±lÄ±r (bias_initializer parametresi kullanÄ±larak).

Ã–nyargÄ± vektÃ¶rÃ¼, karÅŸÄ±lÄ±k gelen aÄŸÄ±rlÄ±k matrisindeki kalÄ±plarÄ±n bir sonraki katmanÄ± ne kadar etkilemesi gerektiÄŸini belirler.



```python
model_14.summary()
```

    Model: "sequential_13"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    flatten_3 (Flatten)          (None, 784)               0         
    _________________________________________________________________
    dense_35 (Dense)             (None, 4)                 3140      
    _________________________________________________________________
    dense_36 (Dense)             (None, 4)                 20        
    _________________________________________________________________
    dense_37 (Dense)             (None, 10)                50        
    =================================================================
    Total params: 3,210
    Trainable params: 3,210
    Non-trainable params: 0
    _________________________________________________________________


Åimdi birkaÃ§ derin Ã¶ÄŸrenme modeli oluÅŸturduk, tÃ¼m girdiler ve Ã§Ä±ktÄ±lar kavramÄ±nÄ±n yalnÄ±zca bir modelin tamamÄ±yla deÄŸil, bir model iÃ§indeki her katmanla da ilgili olduÄŸunu belirtmenin tam zamanÄ±.

Bunu zaten tahmin etmiÅŸ olabilirsiniz, ancak girdi katmanÄ±ndan baÅŸlayarak, sonraki her katmanÄ±n girdisi bir Ã¶nceki katmanÄ±n Ã§Ä±ktÄ±sÄ±dÄ±r. Bunu `plot_model()` kullanarak aÃ§Ä±kÃ§a gÃ¶rebiliriz.



```python
from tensorflow.keras.utils import plot_model

plot_model(model_14, show_shapes=True)
```




    
![png](TensorFlow_ile_Sinir_A%C4%9F%C4%B1_S%C4%B1n%C4%B1fland%C4%B1r%C4%B1lmas%C4%B1_files/TensorFlow_ile_Sinir_A%C4%9F%C4%B1_S%C4%B1n%C4%B1fland%C4%B1r%C4%B1lmas%C4%B1_160_0.png)
    



Bir model nasÄ±l Ã¶ÄŸrenir (kÄ±saca):

Pekala, bir sÃ¼rÃ¼ model eÄŸittik ama kaputun altÄ±nda neler olduÄŸunu hiÃ§ tartÄ±ÅŸmadÄ±k. Peki bir model tam olarak nasÄ±l Ã¶ÄŸrenir?

Bir model, aÄŸÄ±rlÄ±k matrislerini ve yanlÄ±lÄ±k deÄŸerlerini her Ã§aÄŸda gÃ¼ncelleyerek ve geliÅŸtirerek Ã¶ÄŸrenir (bizim durumumuzda, fit() iÅŸlevini Ã§aÄŸÄ±rdÄ±ÄŸÄ±mÄ±zda).

Bunu, veriler ve etiketler arasÄ±nda Ã¶ÄŸrendiÄŸi kalÄ±plarÄ± gerÃ§ek etiketlerle karÅŸÄ±laÅŸtÄ±rarak yapar.

Mevcut modeller (aÄŸÄ±rlÄ±k matrisleri ve yanlÄ±lÄ±k deÄŸerleri) kayÄ±p fonksiyonunda istenen bir azalmaya neden olmazsa (daha yÃ¼ksek kayÄ±p daha kÃ¶tÃ¼ tahminler anlamÄ±na gelir), optimize edici modeli, modellerini doÄŸru ÅŸekilde gÃ¼ncellemek iÃ§in yÃ¶nlendirmeye Ã§alÄ±ÅŸÄ±r (gerÃ§ek kullanarak referans olarak etiketler).

Modelin tahminlerini geliÅŸtirmek iÃ§in gerÃ§ek etiketleri referans olarak kullanma sÃ¼recine geri yayÄ±lÄ±m (backpropagation) denir. BaÅŸka bir deyiÅŸle, veriler ve etiketler bir modelden geÃ§er (ileri geÃ§iÅŸ) ve veriler ile etiketler arasÄ±ndaki iliÅŸkiyi Ã¶ÄŸrenmeye Ã§alÄ±ÅŸÄ±r. Ve eÄŸer bu Ã¶ÄŸrenilen iliÅŸki gerÃ§ek iliÅŸkiye yakÄ±n deÄŸilse veya geliÅŸtirilebilirse, model bunu kendi iÃ§inden geÃ§erek (geriye geÃ§iÅŸ) ve verileri daha iyi temsil etmek iÃ§in aÄŸÄ±rlÄ±k matrislerini ve Ã¶nyargÄ± deÄŸerlerini deÄŸiÅŸtirerek yapar.
