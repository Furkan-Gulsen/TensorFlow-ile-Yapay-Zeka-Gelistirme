# Tensorflow ile Transfer Learning - Ä°nce Ayarlama (Fine Tuning)

Ã–nceki bÃ¶lÃ¼mde, Food Vision projemizde (daha az veriyle bile olsa) kendi modellerimizi oluÅŸturmaktan Ã§ok daha iyi sonuÃ§lar elde etmek iÃ§in Ã¶zellik Ã§Ä±karma aktarÄ±mÄ± Ã¶ÄŸreniminden nasÄ±l yararlanabileceÄŸimizi gÃ¶rdÃ¼k.

Åžimdi baÅŸka bir transfer Ã¶ÄŸrenme tÃ¼rÃ¼nÃ¼ ele alacaÄŸÄ±z: ince ayar (Fine Tuning).

Transfer Ã¶ÄŸreniminde ince ayar yapÄ±lÄ±rken, baÅŸka bir modelden Ã¶nceden eÄŸitilmiÅŸ model aÄŸÄ±rlÄ±klarÄ±, kendi verilerinize daha iyi uymasÄ± iÃ§in donmaz ve ince ayar yapÄ±lÄ±r.

Ã–zellik Ã§Ä±karma transferi Ã¶ÄŸrenimi iÃ§in, Ã¶nceden eÄŸitilmiÅŸ bir modelin yalnÄ±zca ilk 1-3 katmanÄ±nÄ± kendi verilerinizle eÄŸitebilirsiniz, transfer Ã¶ÄŸreniminde ince ayarda, Ã¶nceden eÄŸitilmiÅŸ bir modelin 1-3+ katmanÄ±nÄ± eÄŸitebilirsiniz (burada '+', katmanlarÄ±n Ã§oÄŸunun veya tamamÄ±nÄ±n eÄŸitilebileceÄŸini gÃ¶sterir).

<img src="https://miro.medium.com/proxy/1*1CxVzTNILTHgDs5yJO4W9A.png" />

## YardÄ±mcÄ± fonksiyonlar oluÅŸturma

Makine Ã¶ÄŸrenimi projeleriniz boyunca, muhtemelen tekrar tekrar kullanmak istediÄŸiniz kod parÃ§acÄ±klarÄ±yla karÅŸÄ±laÅŸacaksÄ±nÄ±z.

Ã–rneÄŸin, bir modelin geÃ§miÅŸ nesnesini Ã§izen bir plot iÅŸlevi: (Bu iÅŸlevleri tekrar tekrar oluÅŸturabilirsiniz.)
```
from helper_functions import plot_loss_curves
 
...
 
plot_loss_curves(history)
```
Bunun neye benzediÄŸini gÃ¶relim.


```python
import datetime

def create_tensorboard_callback(dir_name, experiment_name):
  log_dir = dir_name + "/" + experiment_name + "/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
  tensorboard_callback = tf.keras.callbacks.TensorBoard(
      log_dir=log_dir
  )
  print(f"TensorBoard verilerini bu klasÃ¶re kaydet: {log_dir}")
  return tensorboard_callback
```


```python
import matplotlib.pyplot as plt

def plot_loss_curves(history):
  loss = history.history['loss']
  val_loss = history.history['val_loss']

  accuracy = history.history['accuracy']
  val_accuracy = history.history['val_accuracy']

  epochs = range(len(history.history['loss']))

  plt.plot(epochs, loss, label='training_loss')
  plt.plot(epochs, val_loss, label='val_loss')
  plt.title('Loss')
  plt.xlabel('Epochs')
  plt.legend()

  plt.figure()
  plt.plot(epochs, accuracy, label='training_accuracy')
  plt.plot(epochs, val_accuracy, label='val_accuracy')
  plt.title('Accuracy')
  plt.xlabel('Epochs')
  plt.legend();
```


```python
import zipfile

def unzip_data(filename):
  zip_ref = zipfile.ZipFile(filename, "r")
  zip_ref.extractall()
  zip_ref.close()
```


```python
import os

def walk_through_dir(dir_path):
  for dirpath, dirnames, filenames in os.walk(dir_path):
    print(f"'{dirpath}' klasÃ¶rÃ¼nde {len(filenames)} veri var.")
```

Harika, ÅŸimdi her seferinde sÄ±fÄ±rdan yeniden yazmak zorunda kalmadan dizÃ¼stÃ¼ bilgisayar boyunca kullanabileceÄŸimiz bir dizi yardÄ±mcÄ± iÅŸlevimiz var.


## 10 Food Classes: Daha Az Veriyle Ã‡alÄ±ÅŸmak

Bir Ã¶nceki not defterinde, TensorFlow Hub ile transfer Ã¶ÄŸrenimini kullanarak eÄŸitim verilerinin yalnÄ±zca %10'u ile harika sonuÃ§lar elde edebileceÄŸimizi gÃ¶rmÃ¼ÅŸtÃ¼k.

Bu not defterinde, verilerin daha kÃ¼Ã§Ã¼k alt kÃ¼meleriyle Ã§alÄ±ÅŸmaya devam edeceÄŸiz, ancak bu sefer `tf.keras.applications` modÃ¼lÃ¼ndeki yerleÅŸik Ã¶nceden eÄŸitilmiÅŸ modellerine ve  nasÄ±l kullanabileceÄŸimize bir gÃ¶z atacaÄŸÄ±z. BunlarÄ± kendi Ã¶zel veri kÃ¼memizlee nasÄ±l ince ayar yapabileceÄŸimizi Ã¶ÄŸreneceÄŸiz.

AyrÄ±ca, `tf.keras.preprocessing` modÃ¼lÃ¼nÃ¼n bir parÃ§asÄ± olan `image_dataset_from_directory()` adlÄ± daha Ã¶nce kullandÄ±ÄŸÄ±mÄ±za benzer yeni ama benzer bir veri yÃ¼kleyici iÅŸlevi kullanarak alÄ±ÅŸtÄ±rma yapacaÄŸÄ±z.

Son olarak, derin Ã¶ÄŸrenme modelleri oluÅŸturmak iÃ§in [Keras Function API](https://keras.io/guides/functional_api/)'sini kullanma alÄ±ÅŸtÄ±rmasÄ± da yapacaÄŸÄ±z. Ä°ÅŸlevsel API, modeller oluÅŸturmanÄ±n [tf.keras.Sequential](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing) API'sinden daha esnek bir yoludur.

BunlarÄ±n her birini ilerledikÃ§e daha ayrÄ±ntÄ±lÄ± olarak inceleyeceÄŸiz.

BazÄ± verileri indirerek baÅŸlayalÄ±m.


```python
# verisetini iÃ§eriye aktaralÄ±m
!gdown --id 1EJHNCG19hJG6XwIFxt2rpah-Q1Ikrbxw
unzip_data("10_food_classes_10_percent.zip")
```

    Downloading...
    From: https://drive.google.com/uc?id=1EJHNCG19hJG6XwIFxt2rpah-Q1Ikrbxw
    To: /content/10_food_classes_10_percent.zip
    169MB [00:02, 74.3MB/s]


Ä°ndirdiÄŸimiz veri seti, Ã¶nceki not defterinde kullandÄ±ÄŸÄ±mÄ±z eÄŸitim gÃ¶rÃ¼ntÃ¼lerinin %10'unu iÃ§eren 10 food classes veri setidir (Food 101'den).


```python
# dosya sayÄ±sÄ±nÄ± listeleyin
walk_through_dir("10_food_classes_10_percent")
```

    '10_food_classes_10_percent' klasÃ¶rÃ¼nde 0 veri var.
    '10_food_classes_10_percent/train' klasÃ¶rÃ¼nde 0 veri var.
    '10_food_classes_10_percent/train/chicken_wings' klasÃ¶rÃ¼nde 75 veri var.
    '10_food_classes_10_percent/train/sushi' klasÃ¶rÃ¼nde 75 veri var.
    '10_food_classes_10_percent/train/steak' klasÃ¶rÃ¼nde 75 veri var.
    '10_food_classes_10_percent/train/pizza' klasÃ¶rÃ¼nde 75 veri var.
    '10_food_classes_10_percent/train/ice_cream' klasÃ¶rÃ¼nde 75 veri var.
    '10_food_classes_10_percent/train/ramen' klasÃ¶rÃ¼nde 75 veri var.
    '10_food_classes_10_percent/train/fried_rice' klasÃ¶rÃ¼nde 75 veri var.
    '10_food_classes_10_percent/train/chicken_curry' klasÃ¶rÃ¼nde 75 veri var.
    '10_food_classes_10_percent/train/grilled_salmon' klasÃ¶rÃ¼nde 75 veri var.
    '10_food_classes_10_percent/train/hamburger' klasÃ¶rÃ¼nde 75 veri var.
    '10_food_classes_10_percent/test' klasÃ¶rÃ¼nde 0 veri var.
    '10_food_classes_10_percent/test/chicken_wings' klasÃ¶rÃ¼nde 250 veri var.
    '10_food_classes_10_percent/test/sushi' klasÃ¶rÃ¼nde 250 veri var.
    '10_food_classes_10_percent/test/steak' klasÃ¶rÃ¼nde 250 veri var.
    '10_food_classes_10_percent/test/pizza' klasÃ¶rÃ¼nde 250 veri var.
    '10_food_classes_10_percent/test/ice_cream' klasÃ¶rÃ¼nde 250 veri var.
    '10_food_classes_10_percent/test/ramen' klasÃ¶rÃ¼nde 250 veri var.
    '10_food_classes_10_percent/test/fried_rice' klasÃ¶rÃ¼nde 250 veri var.
    '10_food_classes_10_percent/test/chicken_curry' klasÃ¶rÃ¼nde 250 veri var.
    '10_food_classes_10_percent/test/grilled_salmon' klasÃ¶rÃ¼nde 250 veri var.
    '10_food_classes_10_percent/test/hamburger' klasÃ¶rÃ¼nde 250 veri var.


EÄŸitim dizinlerinin her birinin 75 gÃ¶rÃ¼ntÃ¼ iÃ§erdiÄŸini ve test dizinlerinin her birinin 250 gÃ¶rÃ¼ntÃ¼ iÃ§erdiÄŸini gÃ¶rebiliriz.

EÄŸitim ve test dosya yollarÄ±mÄ±zÄ± tanÄ±mlayalÄ±m.



```python
train_dir = "10_food_classes_10_percent/train/"
test_dir = "10_food_classes_10_percent/test/"
```

Åžimdi elimizde bir miktar gÃ¶rÃ¼ntÃ¼ verisi var, onu TensorFlow uyumlu bir formata yÃ¼klemenin bir yoluna ihtiyacÄ±mÄ±z var.

Daha Ã¶nce [`ImageDataGenerator`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator) sÄ±nÄ±fÄ±nÄ± kullandÄ±k. Ve bu iyi Ã§alÄ±ÅŸÄ±yor ve hala Ã§ok yaygÄ±n olarak kullanÄ±lÄ±yor olsa da, bu sefer `image_data_from_directory` iÅŸlevini kullanacaÄŸÄ±z.

`ImageDataGenerator`'Ä±n `flow_from_directory` yÃ¶ntemiyle hemen hemen aynÄ± ÅŸekilde Ã§alÄ±ÅŸÄ±r, yani resimlerinizin aÅŸaÄŸÄ±daki dosya biÃ§iminde olmasÄ± gerekir:

```
10_food_classes_10_percent 
â””â”€â”€â”€train 
â”‚   â””â”€â”€â”€pizza
â”‚   â”‚   â”‚   1008104.jpg
â”‚   â”‚   â”‚   1638227.jpg
â”‚   â”‚   â”‚   ...      
â”‚   â””â”€â”€â”€steak
â”‚       â”‚   1000205.jpg
â”‚       â”‚   1647351.jpg
â”‚       â”‚   ...
â”‚   
â””â”€â”€â”€tes
â”‚   â””â”€â”€â”€pizza
â”‚   â”‚   â”‚   1001116.jpg
â”‚   â”‚   â”‚   1507019.jpg
â”‚   â”‚   â”‚   ...      
â”‚   â””â”€â”€â”€steak
â”‚       â”‚   100274.jpg
â”‚       â”‚   1653815.jpg
â”‚       â”‚   ...    
```

`ImageDataGenerator` yerine `tf.keras.prepreprocessing.image_dataset_from_directory()` kullanmanÄ±n en Ã¶nemli nedenlerinden biri, bir Ã¼reteÃ§ yerine bir `tf.data.Dataset` nesnesi oluÅŸturmasÄ±dÄ±r. Bunun ana avantajÄ±, `tf.data.Dataset` API'sinin, daha bÃ¼yÃ¼k veri kÃ¼meleri iÃ§in Ã§ok Ã¶nemli olan ImageDataGenerator API'sinden Ã§ok daha verimli (daha hÄ±zlÄ±) olmasÄ±dÄ±r.


```python
import tensorflow as tf

IMG_SIZE = (224, 224)

train_data_10_percent = tf.keras.preprocessing.image_dataset_from_directory(directory=train_dir,
                                                                            image_size=IMG_SIZE,
                                                                            label_mode="categorical",
                                                                            batch_size=32) 
test_data_10_percent = tf.keras.preprocessing.image_dataset_from_directory(directory=test_dir,
                                                                           image_size=IMG_SIZE,
                                                                           label_mode="categorical")
```

    Found 750 files belonging to 10 classes.
    Found 2500 files belonging to 10 classes.


OlaÄŸanÃ¼stÃ¼! Veri yÃ¼kleyicilerimiz her veri kÃ¼mesi iÃ§in doÄŸru sayÄ±da gÃ¶rÃ¼ntÃ¼ bulmuÅŸ gibi gÃ¶rÃ¼nÃ¼yor.

`image_dataset_from_directory()` iÅŸlevinde ana parametreler ÅŸunlardÄ±r:

- **directory**<br>
GÃ¶rÃ¼ntÃ¼leri yÃ¼klediÄŸimiz hedef dizinin dosya yolu.
- **image_size**<br>
YÃ¼kleyeceÄŸimiz gÃ¶rÃ¼ntÃ¼lerin hedef boyutu (yÃ¼kseklik, geniÅŸlik).
- **batch_size**<br> 
YÃ¼kleyeceÄŸimiz resimlerin toplu batch size'Ä±. Ã–rneÄŸin, toplu batch size 32 (varsayÄ±lan) ise, modele bir seferde 32 resim ve etiketten oluÅŸan gruplar geÃ§irilecektir.

[`tf.keras.preprocessing`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory) belgelerinde gerekirse oynayabileceÄŸimiz daha Ã§ok ÅŸey var.

EÄŸitim veri tipini kontrol edersek, onu verilerimizle ilgili ÅŸekiller iÃ§eren bir `BatchDataset` olarak gÃ¶rmeliyiz.


```python
train_data_10_percent
```




    <BatchDataset shapes: ((None, 224, 224, 3), (None, 10)), types: (tf.float32, tf.float32)>



YukarÄ±daki Ã§Ä±ktÄ±da:

- `(None, 224, 224, 3)`, None'un batchi, 224'Ã¼n yÃ¼kseklik (ve geniÅŸlik) ve 3'Ã¼n renk kanallarÄ± (kÄ±rmÄ±zÄ±, yeÅŸil, mavi) olduÄŸu gÃ¶rÃ¼ntÃ¼lerimizin tensÃ¶r ÅŸeklini ifade eder.
- `(None, 10)`, None'un batch sayÄ±sÄ± ve 10'un olasÄ± etiket sayÄ±sÄ± olduÄŸu (10 farklÄ± gÄ±da sÄ±nÄ±fÄ±) etiketlerin tensÃ¶r ÅŸeklini belirtir.
- Hem gÃ¶rÃ¼ntÃ¼ tensÃ¶rleri hem de etiketler `tf.float32` veri tipindedir.

Batch_size, yalnÄ±zca model eÄŸitimi sÄ±rasÄ±nda kullanÄ±ldÄ±ÄŸÄ±ndan None deÄŸerine eÅŸittir. None'un  image_dataset_from_directory()'deki batch_size parametresiyle doldurulmayÄ± bekleyen bir yer tutucu olarak dÃ¼ÅŸÃ¼nebilirsiniz.

`tf.data.Dataset` API'sini kullanmanÄ±n bir baÅŸka yararÄ± da onunla birlikte gelen iliÅŸkili yÃ¶ntemlerdir.

Ã–rneÄŸin Ã§alÄ±ÅŸtÄ±ÄŸÄ±mÄ±z sÄ±nÄ±flarÄ±n adÄ±nÄ± bulmak istiyorsak `class_names` Ã¶zniteliÄŸini kullanabiliriz.


```python
train_data_10_percent.class_names
```




    ['chicken_curry',
     'chicken_wings',
     'fried_rice',
     'grilled_salmon',
     'hamburger',
     'ice_cream',
     'pizza',
     'ramen',
     'steak',
     'sushi']



Veya Ã¶rnek bir veri yÄ±ÄŸÄ±nÄ± gÃ¶rmek istersek, `take()` yÃ¶ntemini kullanabiliriz.


```python
for images, labels in train_data_10_percent.take(1):
  print("image: ", images[0])
  print("label: ", labels[0])
```

    image:  tf.Tensor(
    [[[151.22958   122.28571    74.90306  ]
      [144.23979   117.45409    79.005104 ]
      [132.43367   111.36224    84.158165 ]
      ...
      [ 19.571407    7.571407    7.571407 ]
      [ 20.428585    6.4285846   6.4285846]
      [ 22.071463    8.071464    8.071464 ]]
    
     [[135.66327   115.30612    79.87755  ]
      [127.168365  109.602036   80.88776  ]
      [127.90306   113.10204    94.43368  ]
      ...
      [ 20.586777    7.0153046   7.0153046]
      [ 21.928572    5.9285717   6.9285717]
      [ 22.928572    6.9285717   7.9285717]]
    
     [[123.96429   116.82143    98.7602   ]
      [138.46939   132.19899   117.61225  ]
      [149.80103   144.08673   135.87756  ]
      ...
      [ 21.688793    5.2602215   5.2602215]
      [ 24.428572    6.          6.214286 ]
      [ 25.214287    6.785714    7.       ]]
    
     ...
    
     [[ 20.341839    9.341838    5.3418384]
      [ 23.204142   12.204142    8.204142 ]
      [ 21.04587    10.04587     8.04587  ]
      ...
      [ 52.566254   22.351992   20.137728 ]
      [ 51.214203   21.785675   17.857056 ]
      [ 57.056206   26.132736   20.903145 ]]
    
     [[ 21.520426   10.520425    6.520425 ]
      [ 21.862259   10.86226     6.86226  ]
      [ 18.214325    7.214325    5.214325 ]
      ...
      [ 55.173496   27.173498   24.173498 ]
      [ 50.79078    21.79078    17.79078  ]
      [ 51.52044    20.520437   17.520437 ]]
    
     [[ 20.06114     9.06114     5.1887035]
      [ 23.071428   12.071428    8.071428 ]
      [ 24.270454   13.270455   11.270455 ]
      ...
      [ 46.65297    19.295761   16.081497 ]
      [ 49.505173   19.50517    17.50517  ]
      [ 52.01549    21.01549    18.01549  ]]], shape=(224, 224, 3), dtype=float32)
    label:  tf.Tensor([1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(10,), dtype=float32)


## Model 0: Keras Ä°ÅŸlevsel API'sini kullanarak bir aktarÄ±m Ã¶ÄŸrenme modeli oluÅŸturma

Pekala, verilerimiz tensÃ¶rlÃ¼, hadi bir model oluÅŸturalÄ±m.

Bunu yapmak iÃ§in [`tf.keras.applications`](https://www.tensorflow.org/api_docs/python/tf/keras/applications) modÃ¼lÃ¼nÃ¼ kullanacaÄŸÄ±z, Ã§Ã¼nkÃ¼ bu modÃ¼l zaten eÄŸitilmiÅŸ (ImageNet'te) bir dizi bilgisayarlÄ± gÃ¶rÃ¼ modelinin yanÄ± sÄ±ra modelimizi oluÅŸturmak iÃ§in Keras Ä°ÅŸlevsel API'sini iÃ§erir.

AÅŸaÄŸÄ±daki adÄ±mlarÄ± uygulayacaÄŸÄ±z:

1. `tf.keras.applications` iÅŸlevi ile `EfficientNetB0` gibi bir hedef model seÃ§erek, `include_top` parametresini `False` olarak ayarlayarak Ã¶nceden eÄŸitilmiÅŸ bir temel model nesnesini gÃ¶rÃ¼ntÃ¼leyin.
2. Ã–nceden eÄŸitilmiÅŸ modeldeki tÃ¼m aÄŸÄ±rlÄ±klarÄ± dondurmak iÃ§in temel modelin `trainable` niteliÄŸini `False` olarak ayarlayÄ±n.
3. Modelimiz iÃ§in bir girdi katmanÄ± tanÄ±mlayÄ±n, Ã¶rneÄŸin modelimiz hangi veri ÅŸeklini beklemelidir? (Bizim modelimiz iÃ§in bu deÄŸer = (224, 224, 3) )
4. [Opsiyonel] Gerekiyorsa girdileri normalleÅŸtirin. ResNetV250 gibi bazÄ± bilgisayarlÄ± gÃ¶rme modelleri, giriÅŸlerinin 0 ve 1 arasÄ±nda olmasÄ± gerekir.
> ðŸ¤” Not: YazÄ±ldÄ±ÄŸÄ± gibi, `tf.keras.applications`  modÃ¼lÃ¼ndeki EfficientNet modelleri, diÄŸer birÃ§ok modelde olduÄŸu gibi, giriÅŸte gÃ¶rÃ¼ntÃ¼lerin normalleÅŸtirilmesini (0 ile 1 arasÄ±ndaki piksel deÄŸerleri) gerektirmez. 

5. Girdileri temel modele iletin.
6. Temel modelin Ã§Ä±ktÄ±larÄ±nÄ±, Ã§Ä±ktÄ± etkinleÅŸtirme katmanÄ±yla uyumlu bir ÅŸekle toplayÄ±n (temel model Ã§Ä±ktÄ± tensÃ¶rlerini etiket tensÃ¶rleriyle aynÄ± ÅŸekle Ã§evirin). Bu, `tf.keras.layers.GlobalAveragePooling2D()` veya t`f.keras.layers.GlobalMaxPooling2D()` kullanÄ±larak yapÄ±labilir, ancak ilki pratikte daha yaygÄ±ndÄ±r.
7. Uygun aktivasyon fonksiyonu ve nÃ¶ron sayÄ±sÄ± ile `tf.keras.layers.Dense()` kullanarak bir Ã§Ä±ktÄ± aktivasyon katmanÄ± oluÅŸturun.
8. `tf.keras.Model()` kullanarak girdi ve Ã§Ä±ktÄ± katmanÄ±nÄ± bir modelde birleÅŸtirin.
9. Uygun kayÄ±p fonksiyonunu kullanarak modeli derleyin ve optimize ediciyi seÃ§in.
10. Modeli istenen sayÄ±da batch ve gerekli callback iÃ§in fit edin (bizim durumumuzda, TensorBoard callback'i ile baÅŸlayacaÄŸÄ±z).


```python
# 1.adÄ±m
base_model = tf.keras.applications.EfficientNetB0(include_top=False)

# 2.adÄ±m
base_model.trainable = False

# 3.adÄ±m
inputs = tf.keras.layers.Input(shape=(224, 224, 3), name="input_layer")

# 4.adÄ±m
# x = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)(inputs)

# 5.adÄ±m
x = base_model(inputs)
print(f"base_model'den sonraki ÅŸekil: {x.shape}")

# 6.adÄ±m
x = tf.keras.layers.GlobalAveragePooling2D(name="global_average_pooling_layer")(x)
print(f"GlobalAveragePooling2D() sonraki ÅŸekil: {x.shape}")

# 7.adÄ±m
outputs = tf.keras.layers.Dense(10, activation="softmax", name="output_layer")(x)

# 8.adÄ±m
model_0 = tf.keras.Model(inputs, outputs)

# 9.adÄ±m
model_0.compile(loss='categorical_crossentropy',
              optimizer=tf.keras.optimizers.Adam(),
              metrics=["accuracy"])

# 10.adÄ±m
history_10_percent = model_0.fit(train_data_10_percent,
                                 epochs=5,
                                 steps_per_epoch=len(train_data_10_percent),
                                 validation_data=test_data_10_percent,
                                 validation_steps=int(0.25 * len(test_data_10_percent)), 
                                 callbacks=[create_tensorboard_callback("transfer_learning", "10_percent_feature_extract")])
```

    Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5
    16711680/16705208 [==============================] - 0s 0us/step
    base_model'den sonraki ÅŸekil: (None, 7, 7, 1280)
    GlobalAveragePooling2D() sonraki ÅŸekil: (None, 1280)
    TensorBoard verilerini bu klasÃ¶re kaydet: transfer_learning/10_percent_feature_extract/20210720-051625


    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.
      category=CustomMaskWarning)


    Epoch 1/5
    24/24 [==============================] - 46s 448ms/step - loss: 1.8824 - accuracy: 0.4133 - val_loss: 1.2809 - val_accuracy: 0.7368
    Epoch 2/5
    24/24 [==============================] - 8s 326ms/step - loss: 1.1362 - accuracy: 0.7440 - val_loss: 0.8795 - val_accuracy: 0.8076
    Epoch 3/5
    24/24 [==============================] - 8s 326ms/step - loss: 0.8402 - accuracy: 0.8000 - val_loss: 0.7170 - val_accuracy: 0.8421
    Epoch 4/5
    24/24 [==============================] - 6s 216ms/step - loss: 0.6891 - accuracy: 0.8427 - val_loss: 0.6364 - val_accuracy: 0.8536
    Epoch 5/5
    24/24 [==============================] - 5s 212ms/step - loss: 0.5935 - accuracy: 0.8560 - val_loss: 0.5980 - val_accuracy: 0.8618


GÃ¼zel! YaklaÅŸÄ±k bir dakikalÄ±k eÄŸitimden sonra modelimiz hem eÄŸitim (%87+ doÄŸruluk) hem de test setlerinde (~%83 doÄŸruluk) inanÄ±lmaz derecede iyi performans gÃ¶steriyor Hepsi transfer Ã¶ÄŸrenmenin gÃ¼cÃ¼ sayesinde.

TensorFlow Hub modellerinde yaptÄ±ÄŸÄ±mÄ±za benzer ÅŸekilde, burada kullandÄ±ÄŸÄ±mÄ±z transfer Ã¶ÄŸrenimi tÃ¼rÃ¼nÃ¼n Ã¶zellik Ã§Ä±karma transfer Ã¶ÄŸrenimi olarak adlandÄ±rÄ±ldÄ±ÄŸÄ±nÄ± not etmek Ã¶nemlidir.

BaÅŸka bir deyiÅŸle, Ã¶zel verilerimizi Ã¶nceden eÄŸitilmiÅŸ bir modele (EfficientNetB0) ilettik ve "hangi kalÄ±plarÄ± gÃ¶rÃ¼yorsunuz?" diye sorduk. Ve ardÄ±ndan Ã§Ä±ktÄ±larÄ±n istediÄŸimiz sÄ±nÄ±f sayÄ±sÄ±na gÃ¶re uyarlandÄ±ÄŸÄ±ndan emin olmak iÃ§in kendi Ã§Ä±ktÄ± katmanÄ±mÄ±zÄ± en Ã¼ste koyduk.

Modelimizi oluÅŸturmak iÃ§in SÄ±ralÄ± API yerine Keras Ä°ÅŸlevsel API'sini de kullandÄ±k. Åžimdilik, bu ana yÃ¶ntemin faydalarÄ± net gÃ¶rÃ¼nmÃ¼yor, ancak daha karmaÅŸÄ±k modeller oluÅŸturmaya baÅŸladÄ±ÄŸÄ±nÄ±zda, muhtemelen Ä°ÅŸlevsel API'yi kullanmak isteyeceksiniz. Bu nedenle, bu model oluÅŸturma yÃ¶ntemine bilmeniz Ã¶nemlidir.

> ðŸ“– Kaynak: SÄ±ralÄ± API'ye karÅŸÄ± Ä°ÅŸlevsel API'nin faydalarÄ±nÄ± ve kullanÄ±m Ã¶rneklerini gÃ¶rmek iÃ§in TensorFlow [Ä°ÅŸlevsel API belgelerine](https://www.tensorflow.org/guide/keras/functional) bakÄ±n.

Modelimizdeki katmanlarÄ± inceleyelim:


```python
for layer_number, layer in enumerate(base_model.layers):
  print(layer_number, layer.name)
```

    0 input_1
    1 rescaling
    2 normalization
    3 stem_conv_pad
    4 stem_conv
    5 stem_bn
    6 stem_activation
    7 block1a_dwconv
    8 block1a_bn
    9 block1a_activation
    10 block1a_se_squeeze
    11 block1a_se_reshape
    12 block1a_se_reduce
    13 block1a_se_expand
    14 block1a_se_excite
    15 block1a_project_conv
    16 block1a_project_bn
    17 block2a_expand_conv
    18 block2a_expand_bn
    19 block2a_expand_activation
    20 block2a_dwconv_pad
    21 block2a_dwconv
    22 block2a_bn
    23 block2a_activation
    24 block2a_se_squeeze
    25 block2a_se_reshape
    26 block2a_se_reduce
    27 block2a_se_expand
    28 block2a_se_excite
    29 block2a_project_conv
    30 block2a_project_bn
    31 block2b_expand_conv
    32 block2b_expand_bn
    33 block2b_expand_activation
    34 block2b_dwconv
    35 block2b_bn
    36 block2b_activation
    37 block2b_se_squeeze
    38 block2b_se_reshape
    39 block2b_se_reduce
    40 block2b_se_expand
    41 block2b_se_excite
    42 block2b_project_conv
    43 block2b_project_bn
    44 block2b_drop
    45 block2b_add
    46 block3a_expand_conv
    47 block3a_expand_bn
    48 block3a_expand_activation
    49 block3a_dwconv_pad
    50 block3a_dwconv
    51 block3a_bn
    52 block3a_activation
    53 block3a_se_squeeze
    54 block3a_se_reshape
    55 block3a_se_reduce
    56 block3a_se_expand
    57 block3a_se_excite
    58 block3a_project_conv
    59 block3a_project_bn
    60 block3b_expand_conv
    61 block3b_expand_bn
    62 block3b_expand_activation
    63 block3b_dwconv
    64 block3b_bn
    65 block3b_activation
    66 block3b_se_squeeze
    67 block3b_se_reshape
    68 block3b_se_reduce
    69 block3b_se_expand
    70 block3b_se_excite
    71 block3b_project_conv
    72 block3b_project_bn
    73 block3b_drop
    74 block3b_add
    75 block4a_expand_conv
    76 block4a_expand_bn
    77 block4a_expand_activation
    78 block4a_dwconv_pad
    79 block4a_dwconv
    80 block4a_bn
    81 block4a_activation
    82 block4a_se_squeeze
    83 block4a_se_reshape
    84 block4a_se_reduce
    85 block4a_se_expand
    86 block4a_se_excite
    87 block4a_project_conv
    88 block4a_project_bn
    89 block4b_expand_conv
    90 block4b_expand_bn
    91 block4b_expand_activation
    92 block4b_dwconv
    93 block4b_bn
    94 block4b_activation
    95 block4b_se_squeeze
    96 block4b_se_reshape
    97 block4b_se_reduce
    98 block4b_se_expand
    99 block4b_se_excite
    100 block4b_project_conv
    101 block4b_project_bn
    102 block4b_drop
    103 block4b_add
    104 block4c_expand_conv
    105 block4c_expand_bn
    106 block4c_expand_activation
    107 block4c_dwconv
    108 block4c_bn
    109 block4c_activation
    110 block4c_se_squeeze
    111 block4c_se_reshape
    112 block4c_se_reduce
    113 block4c_se_expand
    114 block4c_se_excite
    115 block4c_project_conv
    116 block4c_project_bn
    117 block4c_drop
    118 block4c_add
    119 block5a_expand_conv
    120 block5a_expand_bn
    121 block5a_expand_activation
    122 block5a_dwconv
    123 block5a_bn
    124 block5a_activation
    125 block5a_se_squeeze
    126 block5a_se_reshape
    127 block5a_se_reduce
    128 block5a_se_expand
    129 block5a_se_excite
    130 block5a_project_conv
    131 block5a_project_bn
    132 block5b_expand_conv
    133 block5b_expand_bn
    134 block5b_expand_activation
    135 block5b_dwconv
    136 block5b_bn
    137 block5b_activation
    138 block5b_se_squeeze
    139 block5b_se_reshape
    140 block5b_se_reduce
    141 block5b_se_expand
    142 block5b_se_excite
    143 block5b_project_conv
    144 block5b_project_bn
    145 block5b_drop
    146 block5b_add
    147 block5c_expand_conv
    148 block5c_expand_bn
    149 block5c_expand_activation
    150 block5c_dwconv
    151 block5c_bn
    152 block5c_activation
    153 block5c_se_squeeze
    154 block5c_se_reshape
    155 block5c_se_reduce
    156 block5c_se_expand
    157 block5c_se_excite
    158 block5c_project_conv
    159 block5c_project_bn
    160 block5c_drop
    161 block5c_add
    162 block6a_expand_conv
    163 block6a_expand_bn
    164 block6a_expand_activation
    165 block6a_dwconv_pad
    166 block6a_dwconv
    167 block6a_bn
    168 block6a_activation
    169 block6a_se_squeeze
    170 block6a_se_reshape
    171 block6a_se_reduce
    172 block6a_se_expand
    173 block6a_se_excite
    174 block6a_project_conv
    175 block6a_project_bn
    176 block6b_expand_conv
    177 block6b_expand_bn
    178 block6b_expand_activation
    179 block6b_dwconv
    180 block6b_bn
    181 block6b_activation
    182 block6b_se_squeeze
    183 block6b_se_reshape
    184 block6b_se_reduce
    185 block6b_se_expand
    186 block6b_se_excite
    187 block6b_project_conv
    188 block6b_project_bn
    189 block6b_drop
    190 block6b_add
    191 block6c_expand_conv
    192 block6c_expand_bn
    193 block6c_expand_activation
    194 block6c_dwconv
    195 block6c_bn
    196 block6c_activation
    197 block6c_se_squeeze
    198 block6c_se_reshape
    199 block6c_se_reduce
    200 block6c_se_expand
    201 block6c_se_excite
    202 block6c_project_conv
    203 block6c_project_bn
    204 block6c_drop
    205 block6c_add
    206 block6d_expand_conv
    207 block6d_expand_bn
    208 block6d_expand_activation
    209 block6d_dwconv
    210 block6d_bn
    211 block6d_activation
    212 block6d_se_squeeze
    213 block6d_se_reshape
    214 block6d_se_reduce
    215 block6d_se_expand
    216 block6d_se_excite
    217 block6d_project_conv
    218 block6d_project_bn
    219 block6d_drop
    220 block6d_add
    221 block7a_expand_conv
    222 block7a_expand_bn
    223 block7a_expand_activation
    224 block7a_dwconv
    225 block7a_bn
    226 block7a_activation
    227 block7a_se_squeeze
    228 block7a_se_reshape
    229 block7a_se_reduce
    230 block7a_se_expand
    231 block7a_se_excite
    232 block7a_project_conv
    233 block7a_project_bn
    234 top_conv
    235 top_bn
    236 top_activation


Ã‡ok fazla katman var... bunlarÄ±n hepsini elle kodlamak oldukÃ§a uzun zaman alacaktÄ±, yine de transfer Ã¶ÄŸrenmenin gÃ¼cÃ¼ sayesinde bunlardan faydalanabiliriz.

base_model'in bir Ã¶zetine ne dersiniz?


```python
base_model.summary()
```

    Model: "efficientnetb0"
    __________________________________________________________________________________________________
    Layer (type)                    Output Shape         Param #     Connected to                     
    ==================================================================================================
    input_1 (InputLayer)            [(None, None, None,  0                                            
    __________________________________________________________________________________________________
    rescaling (Rescaling)           (None, None, None, 3 0           input_1[0][0]                    
    __________________________________________________________________________________________________
    normalization (Normalization)   (None, None, None, 3 7           rescaling[0][0]                  
    __________________________________________________________________________________________________
    stem_conv_pad (ZeroPadding2D)   (None, None, None, 3 0           normalization[0][0]              
    __________________________________________________________________________________________________
    stem_conv (Conv2D)              (None, None, None, 3 864         stem_conv_pad[0][0]              
    __________________________________________________________________________________________________
    stem_bn (BatchNormalization)    (None, None, None, 3 128         stem_conv[0][0]                  
    __________________________________________________________________________________________________
    stem_activation (Activation)    (None, None, None, 3 0           stem_bn[0][0]                    
    __________________________________________________________________________________________________
    block1a_dwconv (DepthwiseConv2D (None, None, None, 3 288         stem_activation[0][0]            
    __________________________________________________________________________________________________
    block1a_bn (BatchNormalization) (None, None, None, 3 128         block1a_dwconv[0][0]             
    __________________________________________________________________________________________________
    block1a_activation (Activation) (None, None, None, 3 0           block1a_bn[0][0]                 
    __________________________________________________________________________________________________
    block1a_se_squeeze (GlobalAvera (None, 32)           0           block1a_activation[0][0]         
    __________________________________________________________________________________________________
    block1a_se_reshape (Reshape)    (None, 1, 1, 32)     0           block1a_se_squeeze[0][0]         
    __________________________________________________________________________________________________
    block1a_se_reduce (Conv2D)      (None, 1, 1, 8)      264         block1a_se_reshape[0][0]         
    __________________________________________________________________________________________________
    block1a_se_expand (Conv2D)      (None, 1, 1, 32)     288         block1a_se_reduce[0][0]          
    __________________________________________________________________________________________________
    block1a_se_excite (Multiply)    (None, None, None, 3 0           block1a_activation[0][0]         
                                                                     block1a_se_expand[0][0]          
    __________________________________________________________________________________________________
    block1a_project_conv (Conv2D)   (None, None, None, 1 512         block1a_se_excite[0][0]          
    __________________________________________________________________________________________________
    block1a_project_bn (BatchNormal (None, None, None, 1 64          block1a_project_conv[0][0]       
    __________________________________________________________________________________________________
    block2a_expand_conv (Conv2D)    (None, None, None, 9 1536        block1a_project_bn[0][0]         
    __________________________________________________________________________________________________
    block2a_expand_bn (BatchNormali (None, None, None, 9 384         block2a_expand_conv[0][0]        
    __________________________________________________________________________________________________
    block2a_expand_activation (Acti (None, None, None, 9 0           block2a_expand_bn[0][0]          
    __________________________________________________________________________________________________
    block2a_dwconv_pad (ZeroPadding (None, None, None, 9 0           block2a_expand_activation[0][0]  
    __________________________________________________________________________________________________
    block2a_dwconv (DepthwiseConv2D (None, None, None, 9 864         block2a_dwconv_pad[0][0]         
    __________________________________________________________________________________________________
    block2a_bn (BatchNormalization) (None, None, None, 9 384         block2a_dwconv[0][0]             
    __________________________________________________________________________________________________
    block2a_activation (Activation) (None, None, None, 9 0           block2a_bn[0][0]                 
    __________________________________________________________________________________________________
    block2a_se_squeeze (GlobalAvera (None, 96)           0           block2a_activation[0][0]         
    __________________________________________________________________________________________________
    block2a_se_reshape (Reshape)    (None, 1, 1, 96)     0           block2a_se_squeeze[0][0]         
    __________________________________________________________________________________________________
    block2a_se_reduce (Conv2D)      (None, 1, 1, 4)      388         block2a_se_reshape[0][0]         
    __________________________________________________________________________________________________
    block2a_se_expand (Conv2D)      (None, 1, 1, 96)     480         block2a_se_reduce[0][0]          
    __________________________________________________________________________________________________
    block2a_se_excite (Multiply)    (None, None, None, 9 0           block2a_activation[0][0]         
                                                                     block2a_se_expand[0][0]          
    __________________________________________________________________________________________________
    block2a_project_conv (Conv2D)   (None, None, None, 2 2304        block2a_se_excite[0][0]          
    __________________________________________________________________________________________________
    block2a_project_bn (BatchNormal (None, None, None, 2 96          block2a_project_conv[0][0]       
    __________________________________________________________________________________________________
    block2b_expand_conv (Conv2D)    (None, None, None, 1 3456        block2a_project_bn[0][0]         
    __________________________________________________________________________________________________
    block2b_expand_bn (BatchNormali (None, None, None, 1 576         block2b_expand_conv[0][0]        
    __________________________________________________________________________________________________
    block2b_expand_activation (Acti (None, None, None, 1 0           block2b_expand_bn[0][0]          
    __________________________________________________________________________________________________
    block2b_dwconv (DepthwiseConv2D (None, None, None, 1 1296        block2b_expand_activation[0][0]  
    __________________________________________________________________________________________________
    block2b_bn (BatchNormalization) (None, None, None, 1 576         block2b_dwconv[0][0]             
    __________________________________________________________________________________________________
    block2b_activation (Activation) (None, None, None, 1 0           block2b_bn[0][0]                 
    __________________________________________________________________________________________________
    block2b_se_squeeze (GlobalAvera (None, 144)          0           block2b_activation[0][0]         
    __________________________________________________________________________________________________
    block2b_se_reshape (Reshape)    (None, 1, 1, 144)    0           block2b_se_squeeze[0][0]         
    __________________________________________________________________________________________________
    block2b_se_reduce (Conv2D)      (None, 1, 1, 6)      870         block2b_se_reshape[0][0]         
    __________________________________________________________________________________________________
    block2b_se_expand (Conv2D)      (None, 1, 1, 144)    1008        block2b_se_reduce[0][0]          
    __________________________________________________________________________________________________
    block2b_se_excite (Multiply)    (None, None, None, 1 0           block2b_activation[0][0]         
                                                                     block2b_se_expand[0][0]          
    __________________________________________________________________________________________________
    block2b_project_conv (Conv2D)   (None, None, None, 2 3456        block2b_se_excite[0][0]          
    __________________________________________________________________________________________________
    block2b_project_bn (BatchNormal (None, None, None, 2 96          block2b_project_conv[0][0]       
    __________________________________________________________________________________________________
    block2b_drop (Dropout)          (None, None, None, 2 0           block2b_project_bn[0][0]         
    __________________________________________________________________________________________________
    block2b_add (Add)               (None, None, None, 2 0           block2b_drop[0][0]               
                                                                     block2a_project_bn[0][0]         
    __________________________________________________________________________________________________
    block3a_expand_conv (Conv2D)    (None, None, None, 1 3456        block2b_add[0][0]                
    __________________________________________________________________________________________________
    block3a_expand_bn (BatchNormali (None, None, None, 1 576         block3a_expand_conv[0][0]        
    __________________________________________________________________________________________________
    block3a_expand_activation (Acti (None, None, None, 1 0           block3a_expand_bn[0][0]          
    __________________________________________________________________________________________________
    block3a_dwconv_pad (ZeroPadding (None, None, None, 1 0           block3a_expand_activation[0][0]  
    __________________________________________________________________________________________________
    block3a_dwconv (DepthwiseConv2D (None, None, None, 1 3600        block3a_dwconv_pad[0][0]         
    __________________________________________________________________________________________________
    block3a_bn (BatchNormalization) (None, None, None, 1 576         block3a_dwconv[0][0]             
    __________________________________________________________________________________________________
    block3a_activation (Activation) (None, None, None, 1 0           block3a_bn[0][0]                 
    __________________________________________________________________________________________________
    block3a_se_squeeze (GlobalAvera (None, 144)          0           block3a_activation[0][0]         
    __________________________________________________________________________________________________
    block3a_se_reshape (Reshape)    (None, 1, 1, 144)    0           block3a_se_squeeze[0][0]         
    __________________________________________________________________________________________________
    block3a_se_reduce (Conv2D)      (None, 1, 1, 6)      870         block3a_se_reshape[0][0]         
    __________________________________________________________________________________________________
    block3a_se_expand (Conv2D)      (None, 1, 1, 144)    1008        block3a_se_reduce[0][0]          
    __________________________________________________________________________________________________
    block3a_se_excite (Multiply)    (None, None, None, 1 0           block3a_activation[0][0]         
                                                                     block3a_se_expand[0][0]          
    __________________________________________________________________________________________________
    block3a_project_conv (Conv2D)   (None, None, None, 4 5760        block3a_se_excite[0][0]          
    __________________________________________________________________________________________________
    block3a_project_bn (BatchNormal (None, None, None, 4 160         block3a_project_conv[0][0]       
    __________________________________________________________________________________________________
    block3b_expand_conv (Conv2D)    (None, None, None, 2 9600        block3a_project_bn[0][0]         
    __________________________________________________________________________________________________
    block3b_expand_bn (BatchNormali (None, None, None, 2 960         block3b_expand_conv[0][0]        
    __________________________________________________________________________________________________
    block3b_expand_activation (Acti (None, None, None, 2 0           block3b_expand_bn[0][0]          
    __________________________________________________________________________________________________
    block3b_dwconv (DepthwiseConv2D (None, None, None, 2 6000        block3b_expand_activation[0][0]  
    __________________________________________________________________________________________________
    block3b_bn (BatchNormalization) (None, None, None, 2 960         block3b_dwconv[0][0]             
    __________________________________________________________________________________________________
    block3b_activation (Activation) (None, None, None, 2 0           block3b_bn[0][0]                 
    __________________________________________________________________________________________________
    block3b_se_squeeze (GlobalAvera (None, 240)          0           block3b_activation[0][0]         
    __________________________________________________________________________________________________
    block3b_se_reshape (Reshape)    (None, 1, 1, 240)    0           block3b_se_squeeze[0][0]         
    __________________________________________________________________________________________________
    block3b_se_reduce (Conv2D)      (None, 1, 1, 10)     2410        block3b_se_reshape[0][0]         
    __________________________________________________________________________________________________
    block3b_se_expand (Conv2D)      (None, 1, 1, 240)    2640        block3b_se_reduce[0][0]          
    __________________________________________________________________________________________________
    block3b_se_excite (Multiply)    (None, None, None, 2 0           block3b_activation[0][0]         
                                                                     block3b_se_expand[0][0]          
    __________________________________________________________________________________________________
    block3b_project_conv (Conv2D)   (None, None, None, 4 9600        block3b_se_excite[0][0]          
    __________________________________________________________________________________________________
    block3b_project_bn (BatchNormal (None, None, None, 4 160         block3b_project_conv[0][0]       
    __________________________________________________________________________________________________
    block3b_drop (Dropout)          (None, None, None, 4 0           block3b_project_bn[0][0]         
    __________________________________________________________________________________________________
    block3b_add (Add)               (None, None, None, 4 0           block3b_drop[0][0]               
                                                                     block3a_project_bn[0][0]         
    __________________________________________________________________________________________________
    block4a_expand_conv (Conv2D)    (None, None, None, 2 9600        block3b_add[0][0]                
    __________________________________________________________________________________________________
    block4a_expand_bn (BatchNormali (None, None, None, 2 960         block4a_expand_conv[0][0]        
    __________________________________________________________________________________________________
    block4a_expand_activation (Acti (None, None, None, 2 0           block4a_expand_bn[0][0]          
    __________________________________________________________________________________________________
    block4a_dwconv_pad (ZeroPadding (None, None, None, 2 0           block4a_expand_activation[0][0]  
    __________________________________________________________________________________________________
    block4a_dwconv (DepthwiseConv2D (None, None, None, 2 2160        block4a_dwconv_pad[0][0]         
    __________________________________________________________________________________________________
    block4a_bn (BatchNormalization) (None, None, None, 2 960         block4a_dwconv[0][0]             
    __________________________________________________________________________________________________
    block4a_activation (Activation) (None, None, None, 2 0           block4a_bn[0][0]                 
    __________________________________________________________________________________________________
    block4a_se_squeeze (GlobalAvera (None, 240)          0           block4a_activation[0][0]         
    __________________________________________________________________________________________________
    block4a_se_reshape (Reshape)    (None, 1, 1, 240)    0           block4a_se_squeeze[0][0]         
    __________________________________________________________________________________________________
    block4a_se_reduce (Conv2D)      (None, 1, 1, 10)     2410        block4a_se_reshape[0][0]         
    __________________________________________________________________________________________________
    block4a_se_expand (Conv2D)      (None, 1, 1, 240)    2640        block4a_se_reduce[0][0]          
    __________________________________________________________________________________________________
    block4a_se_excite (Multiply)    (None, None, None, 2 0           block4a_activation[0][0]         
                                                                     block4a_se_expand[0][0]          
    __________________________________________________________________________________________________
    block4a_project_conv (Conv2D)   (None, None, None, 8 19200       block4a_se_excite[0][0]          
    __________________________________________________________________________________________________
    block4a_project_bn (BatchNormal (None, None, None, 8 320         block4a_project_conv[0][0]       
    __________________________________________________________________________________________________
    block4b_expand_conv (Conv2D)    (None, None, None, 4 38400       block4a_project_bn[0][0]         
    __________________________________________________________________________________________________
    block4b_expand_bn (BatchNormali (None, None, None, 4 1920        block4b_expand_conv[0][0]        
    __________________________________________________________________________________________________
    block4b_expand_activation (Acti (None, None, None, 4 0           block4b_expand_bn[0][0]          
    __________________________________________________________________________________________________
    block4b_dwconv (DepthwiseConv2D (None, None, None, 4 4320        block4b_expand_activation[0][0]  
    __________________________________________________________________________________________________
    block4b_bn (BatchNormalization) (None, None, None, 4 1920        block4b_dwconv[0][0]             
    __________________________________________________________________________________________________
    block4b_activation (Activation) (None, None, None, 4 0           block4b_bn[0][0]                 
    __________________________________________________________________________________________________
    block4b_se_squeeze (GlobalAvera (None, 480)          0           block4b_activation[0][0]         
    __________________________________________________________________________________________________
    block4b_se_reshape (Reshape)    (None, 1, 1, 480)    0           block4b_se_squeeze[0][0]         
    __________________________________________________________________________________________________
    block4b_se_reduce (Conv2D)      (None, 1, 1, 20)     9620        block4b_se_reshape[0][0]         
    __________________________________________________________________________________________________
    block4b_se_expand (Conv2D)      (None, 1, 1, 480)    10080       block4b_se_reduce[0][0]          
    __________________________________________________________________________________________________
    block4b_se_excite (Multiply)    (None, None, None, 4 0           block4b_activation[0][0]         
                                                                     block4b_se_expand[0][0]          
    __________________________________________________________________________________________________
    block4b_project_conv (Conv2D)   (None, None, None, 8 38400       block4b_se_excite[0][0]          
    __________________________________________________________________________________________________
    block4b_project_bn (BatchNormal (None, None, None, 8 320         block4b_project_conv[0][0]       
    __________________________________________________________________________________________________
    block4b_drop (Dropout)          (None, None, None, 8 0           block4b_project_bn[0][0]         
    __________________________________________________________________________________________________
    block4b_add (Add)               (None, None, None, 8 0           block4b_drop[0][0]               
                                                                     block4a_project_bn[0][0]         
    __________________________________________________________________________________________________
    block4c_expand_conv (Conv2D)    (None, None, None, 4 38400       block4b_add[0][0]                
    __________________________________________________________________________________________________
    block4c_expand_bn (BatchNormali (None, None, None, 4 1920        block4c_expand_conv[0][0]        
    __________________________________________________________________________________________________
    block4c_expand_activation (Acti (None, None, None, 4 0           block4c_expand_bn[0][0]          
    __________________________________________________________________________________________________
    block4c_dwconv (DepthwiseConv2D (None, None, None, 4 4320        block4c_expand_activation[0][0]  
    __________________________________________________________________________________________________
    block4c_bn (BatchNormalization) (None, None, None, 4 1920        block4c_dwconv[0][0]             
    __________________________________________________________________________________________________
    block4c_activation (Activation) (None, None, None, 4 0           block4c_bn[0][0]                 
    __________________________________________________________________________________________________
    block4c_se_squeeze (GlobalAvera (None, 480)          0           block4c_activation[0][0]         
    __________________________________________________________________________________________________
    block4c_se_reshape (Reshape)    (None, 1, 1, 480)    0           block4c_se_squeeze[0][0]         
    __________________________________________________________________________________________________
    block4c_se_reduce (Conv2D)      (None, 1, 1, 20)     9620        block4c_se_reshape[0][0]         
    __________________________________________________________________________________________________
    block4c_se_expand (Conv2D)      (None, 1, 1, 480)    10080       block4c_se_reduce[0][0]          
    __________________________________________________________________________________________________
    block4c_se_excite (Multiply)    (None, None, None, 4 0           block4c_activation[0][0]         
                                                                     block4c_se_expand[0][0]          
    __________________________________________________________________________________________________
    block4c_project_conv (Conv2D)   (None, None, None, 8 38400       block4c_se_excite[0][0]          
    __________________________________________________________________________________________________
    block4c_project_bn (BatchNormal (None, None, None, 8 320         block4c_project_conv[0][0]       
    __________________________________________________________________________________________________
    block4c_drop (Dropout)          (None, None, None, 8 0           block4c_project_bn[0][0]         
    __________________________________________________________________________________________________
    block4c_add (Add)               (None, None, None, 8 0           block4c_drop[0][0]               
                                                                     block4b_add[0][0]                
    __________________________________________________________________________________________________
    block5a_expand_conv (Conv2D)    (None, None, None, 4 38400       block4c_add[0][0]                
    __________________________________________________________________________________________________
    block5a_expand_bn (BatchNormali (None, None, None, 4 1920        block5a_expand_conv[0][0]        
    __________________________________________________________________________________________________
    block5a_expand_activation (Acti (None, None, None, 4 0           block5a_expand_bn[0][0]          
    __________________________________________________________________________________________________
    block5a_dwconv (DepthwiseConv2D (None, None, None, 4 12000       block5a_expand_activation[0][0]  
    __________________________________________________________________________________________________
    block5a_bn (BatchNormalization) (None, None, None, 4 1920        block5a_dwconv[0][0]             
    __________________________________________________________________________________________________
    block5a_activation (Activation) (None, None, None, 4 0           block5a_bn[0][0]                 
    __________________________________________________________________________________________________
    block5a_se_squeeze (GlobalAvera (None, 480)          0           block5a_activation[0][0]         
    __________________________________________________________________________________________________
    block5a_se_reshape (Reshape)    (None, 1, 1, 480)    0           block5a_se_squeeze[0][0]         
    __________________________________________________________________________________________________
    block5a_se_reduce (Conv2D)      (None, 1, 1, 20)     9620        block5a_se_reshape[0][0]         
    __________________________________________________________________________________________________
    block5a_se_expand (Conv2D)      (None, 1, 1, 480)    10080       block5a_se_reduce[0][0]          
    __________________________________________________________________________________________________
    block5a_se_excite (Multiply)    (None, None, None, 4 0           block5a_activation[0][0]         
                                                                     block5a_se_expand[0][0]          
    __________________________________________________________________________________________________
    block5a_project_conv (Conv2D)   (None, None, None, 1 53760       block5a_se_excite[0][0]          
    __________________________________________________________________________________________________
    block5a_project_bn (BatchNormal (None, None, None, 1 448         block5a_project_conv[0][0]       
    __________________________________________________________________________________________________
    block5b_expand_conv (Conv2D)    (None, None, None, 6 75264       block5a_project_bn[0][0]         
    __________________________________________________________________________________________________
    block5b_expand_bn (BatchNormali (None, None, None, 6 2688        block5b_expand_conv[0][0]        
    __________________________________________________________________________________________________
    block5b_expand_activation (Acti (None, None, None, 6 0           block5b_expand_bn[0][0]          
    __________________________________________________________________________________________________
    block5b_dwconv (DepthwiseConv2D (None, None, None, 6 16800       block5b_expand_activation[0][0]  
    __________________________________________________________________________________________________
    block5b_bn (BatchNormalization) (None, None, None, 6 2688        block5b_dwconv[0][0]             
    __________________________________________________________________________________________________
    block5b_activation (Activation) (None, None, None, 6 0           block5b_bn[0][0]                 
    __________________________________________________________________________________________________
    block5b_se_squeeze (GlobalAvera (None, 672)          0           block5b_activation[0][0]         
    __________________________________________________________________________________________________
    block5b_se_reshape (Reshape)    (None, 1, 1, 672)    0           block5b_se_squeeze[0][0]         
    __________________________________________________________________________________________________
    block5b_se_reduce (Conv2D)      (None, 1, 1, 28)     18844       block5b_se_reshape[0][0]         
    __________________________________________________________________________________________________
    block5b_se_expand (Conv2D)      (None, 1, 1, 672)    19488       block5b_se_reduce[0][0]          
    __________________________________________________________________________________________________
    block5b_se_excite (Multiply)    (None, None, None, 6 0           block5b_activation[0][0]         
                                                                     block5b_se_expand[0][0]          
    __________________________________________________________________________________________________
    block5b_project_conv (Conv2D)   (None, None, None, 1 75264       block5b_se_excite[0][0]          
    __________________________________________________________________________________________________
    block5b_project_bn (BatchNormal (None, None, None, 1 448         block5b_project_conv[0][0]       
    __________________________________________________________________________________________________
    block5b_drop (Dropout)          (None, None, None, 1 0           block5b_project_bn[0][0]         
    __________________________________________________________________________________________________
    block5b_add (Add)               (None, None, None, 1 0           block5b_drop[0][0]               
                                                                     block5a_project_bn[0][0]         
    __________________________________________________________________________________________________
    block5c_expand_conv (Conv2D)    (None, None, None, 6 75264       block5b_add[0][0]                
    __________________________________________________________________________________________________
    block5c_expand_bn (BatchNormali (None, None, None, 6 2688        block5c_expand_conv[0][0]        
    __________________________________________________________________________________________________
    block5c_expand_activation (Acti (None, None, None, 6 0           block5c_expand_bn[0][0]          
    __________________________________________________________________________________________________
    block5c_dwconv (DepthwiseConv2D (None, None, None, 6 16800       block5c_expand_activation[0][0]  
    __________________________________________________________________________________________________
    block5c_bn (BatchNormalization) (None, None, None, 6 2688        block5c_dwconv[0][0]             
    __________________________________________________________________________________________________
    block5c_activation (Activation) (None, None, None, 6 0           block5c_bn[0][0]                 
    __________________________________________________________________________________________________
    block5c_se_squeeze (GlobalAvera (None, 672)          0           block5c_activation[0][0]         
    __________________________________________________________________________________________________
    block5c_se_reshape (Reshape)    (None, 1, 1, 672)    0           block5c_se_squeeze[0][0]         
    __________________________________________________________________________________________________
    block5c_se_reduce (Conv2D)      (None, 1, 1, 28)     18844       block5c_se_reshape[0][0]         
    __________________________________________________________________________________________________
    block5c_se_expand (Conv2D)      (None, 1, 1, 672)    19488       block5c_se_reduce[0][0]          
    __________________________________________________________________________________________________
    block5c_se_excite (Multiply)    (None, None, None, 6 0           block5c_activation[0][0]         
                                                                     block5c_se_expand[0][0]          
    __________________________________________________________________________________________________
    block5c_project_conv (Conv2D)   (None, None, None, 1 75264       block5c_se_excite[0][0]          
    __________________________________________________________________________________________________
    block5c_project_bn (BatchNormal (None, None, None, 1 448         block5c_project_conv[0][0]       
    __________________________________________________________________________________________________
    block5c_drop (Dropout)          (None, None, None, 1 0           block5c_project_bn[0][0]         
    __________________________________________________________________________________________________
    block5c_add (Add)               (None, None, None, 1 0           block5c_drop[0][0]               
                                                                     block5b_add[0][0]                
    __________________________________________________________________________________________________
    block6a_expand_conv (Conv2D)    (None, None, None, 6 75264       block5c_add[0][0]                
    __________________________________________________________________________________________________
    block6a_expand_bn (BatchNormali (None, None, None, 6 2688        block6a_expand_conv[0][0]        
    __________________________________________________________________________________________________
    block6a_expand_activation (Acti (None, None, None, 6 0           block6a_expand_bn[0][0]          
    __________________________________________________________________________________________________
    block6a_dwconv_pad (ZeroPadding (None, None, None, 6 0           block6a_expand_activation[0][0]  
    __________________________________________________________________________________________________
    block6a_dwconv (DepthwiseConv2D (None, None, None, 6 16800       block6a_dwconv_pad[0][0]         
    __________________________________________________________________________________________________
    block6a_bn (BatchNormalization) (None, None, None, 6 2688        block6a_dwconv[0][0]             
    __________________________________________________________________________________________________
    block6a_activation (Activation) (None, None, None, 6 0           block6a_bn[0][0]                 
    __________________________________________________________________________________________________
    block6a_se_squeeze (GlobalAvera (None, 672)          0           block6a_activation[0][0]         
    __________________________________________________________________________________________________
    block6a_se_reshape (Reshape)    (None, 1, 1, 672)    0           block6a_se_squeeze[0][0]         
    __________________________________________________________________________________________________
    block6a_se_reduce (Conv2D)      (None, 1, 1, 28)     18844       block6a_se_reshape[0][0]         
    __________________________________________________________________________________________________
    block6a_se_expand (Conv2D)      (None, 1, 1, 672)    19488       block6a_se_reduce[0][0]          
    __________________________________________________________________________________________________
    block6a_se_excite (Multiply)    (None, None, None, 6 0           block6a_activation[0][0]         
                                                                     block6a_se_expand[0][0]          
    __________________________________________________________________________________________________
    block6a_project_conv (Conv2D)   (None, None, None, 1 129024      block6a_se_excite[0][0]          
    __________________________________________________________________________________________________
    block6a_project_bn (BatchNormal (None, None, None, 1 768         block6a_project_conv[0][0]       
    __________________________________________________________________________________________________
    block6b_expand_conv (Conv2D)    (None, None, None, 1 221184      block6a_project_bn[0][0]         
    __________________________________________________________________________________________________
    block6b_expand_bn (BatchNormali (None, None, None, 1 4608        block6b_expand_conv[0][0]        
    __________________________________________________________________________________________________
    block6b_expand_activation (Acti (None, None, None, 1 0           block6b_expand_bn[0][0]          
    __________________________________________________________________________________________________
    block6b_dwconv (DepthwiseConv2D (None, None, None, 1 28800       block6b_expand_activation[0][0]  
    __________________________________________________________________________________________________
    block6b_bn (BatchNormalization) (None, None, None, 1 4608        block6b_dwconv[0][0]             
    __________________________________________________________________________________________________
    block6b_activation (Activation) (None, None, None, 1 0           block6b_bn[0][0]                 
    __________________________________________________________________________________________________
    block6b_se_squeeze (GlobalAvera (None, 1152)         0           block6b_activation[0][0]         
    __________________________________________________________________________________________________
    block6b_se_reshape (Reshape)    (None, 1, 1, 1152)   0           block6b_se_squeeze[0][0]         
    __________________________________________________________________________________________________
    block6b_se_reduce (Conv2D)      (None, 1, 1, 48)     55344       block6b_se_reshape[0][0]         
    __________________________________________________________________________________________________
    block6b_se_expand (Conv2D)      (None, 1, 1, 1152)   56448       block6b_se_reduce[0][0]          
    __________________________________________________________________________________________________
    block6b_se_excite (Multiply)    (None, None, None, 1 0           block6b_activation[0][0]         
                                                                     block6b_se_expand[0][0]          
    __________________________________________________________________________________________________
    block6b_project_conv (Conv2D)   (None, None, None, 1 221184      block6b_se_excite[0][0]          
    __________________________________________________________________________________________________
    block6b_project_bn (BatchNormal (None, None, None, 1 768         block6b_project_conv[0][0]       
    __________________________________________________________________________________________________
    block6b_drop (Dropout)          (None, None, None, 1 0           block6b_project_bn[0][0]         
    __________________________________________________________________________________________________
    block6b_add (Add)               (None, None, None, 1 0           block6b_drop[0][0]               
                                                                     block6a_project_bn[0][0]         
    __________________________________________________________________________________________________
    block6c_expand_conv (Conv2D)    (None, None, None, 1 221184      block6b_add[0][0]                
    __________________________________________________________________________________________________
    block6c_expand_bn (BatchNormali (None, None, None, 1 4608        block6c_expand_conv[0][0]        
    __________________________________________________________________________________________________
    block6c_expand_activation (Acti (None, None, None, 1 0           block6c_expand_bn[0][0]          
    __________________________________________________________________________________________________
    block6c_dwconv (DepthwiseConv2D (None, None, None, 1 28800       block6c_expand_activation[0][0]  
    __________________________________________________________________________________________________
    block6c_bn (BatchNormalization) (None, None, None, 1 4608        block6c_dwconv[0][0]             
    __________________________________________________________________________________________________
    block6c_activation (Activation) (None, None, None, 1 0           block6c_bn[0][0]                 
    __________________________________________________________________________________________________
    block6c_se_squeeze (GlobalAvera (None, 1152)         0           block6c_activation[0][0]         
    __________________________________________________________________________________________________
    block6c_se_reshape (Reshape)    (None, 1, 1, 1152)   0           block6c_se_squeeze[0][0]         
    __________________________________________________________________________________________________
    block6c_se_reduce (Conv2D)      (None, 1, 1, 48)     55344       block6c_se_reshape[0][0]         
    __________________________________________________________________________________________________
    block6c_se_expand (Conv2D)      (None, 1, 1, 1152)   56448       block6c_se_reduce[0][0]          
    __________________________________________________________________________________________________
    block6c_se_excite (Multiply)    (None, None, None, 1 0           block6c_activation[0][0]         
                                                                     block6c_se_expand[0][0]          
    __________________________________________________________________________________________________
    block6c_project_conv (Conv2D)   (None, None, None, 1 221184      block6c_se_excite[0][0]          
    __________________________________________________________________________________________________
    block6c_project_bn (BatchNormal (None, None, None, 1 768         block6c_project_conv[0][0]       
    __________________________________________________________________________________________________
    block6c_drop (Dropout)          (None, None, None, 1 0           block6c_project_bn[0][0]         
    __________________________________________________________________________________________________
    block6c_add (Add)               (None, None, None, 1 0           block6c_drop[0][0]               
                                                                     block6b_add[0][0]                
    __________________________________________________________________________________________________
    block6d_expand_conv (Conv2D)    (None, None, None, 1 221184      block6c_add[0][0]                
    __________________________________________________________________________________________________
    block6d_expand_bn (BatchNormali (None, None, None, 1 4608        block6d_expand_conv[0][0]        
    __________________________________________________________________________________________________
    block6d_expand_activation (Acti (None, None, None, 1 0           block6d_expand_bn[0][0]          
    __________________________________________________________________________________________________
    block6d_dwconv (DepthwiseConv2D (None, None, None, 1 28800       block6d_expand_activation[0][0]  
    __________________________________________________________________________________________________
    block6d_bn (BatchNormalization) (None, None, None, 1 4608        block6d_dwconv[0][0]             
    __________________________________________________________________________________________________
    block6d_activation (Activation) (None, None, None, 1 0           block6d_bn[0][0]                 
    __________________________________________________________________________________________________
    block6d_se_squeeze (GlobalAvera (None, 1152)         0           block6d_activation[0][0]         
    __________________________________________________________________________________________________
    block6d_se_reshape (Reshape)    (None, 1, 1, 1152)   0           block6d_se_squeeze[0][0]         
    __________________________________________________________________________________________________
    block6d_se_reduce (Conv2D)      (None, 1, 1, 48)     55344       block6d_se_reshape[0][0]         
    __________________________________________________________________________________________________
    block6d_se_expand (Conv2D)      (None, 1, 1, 1152)   56448       block6d_se_reduce[0][0]          
    __________________________________________________________________________________________________
    block6d_se_excite (Multiply)    (None, None, None, 1 0           block6d_activation[0][0]         
                                                                     block6d_se_expand[0][0]          
    __________________________________________________________________________________________________
    block6d_project_conv (Conv2D)   (None, None, None, 1 221184      block6d_se_excite[0][0]          
    __________________________________________________________________________________________________
    block6d_project_bn (BatchNormal (None, None, None, 1 768         block6d_project_conv[0][0]       
    __________________________________________________________________________________________________
    block6d_drop (Dropout)          (None, None, None, 1 0           block6d_project_bn[0][0]         
    __________________________________________________________________________________________________
    block6d_add (Add)               (None, None, None, 1 0           block6d_drop[0][0]               
                                                                     block6c_add[0][0]                
    __________________________________________________________________________________________________
    block7a_expand_conv (Conv2D)    (None, None, None, 1 221184      block6d_add[0][0]                
    __________________________________________________________________________________________________
    block7a_expand_bn (BatchNormali (None, None, None, 1 4608        block7a_expand_conv[0][0]        
    __________________________________________________________________________________________________
    block7a_expand_activation (Acti (None, None, None, 1 0           block7a_expand_bn[0][0]          
    __________________________________________________________________________________________________
    block7a_dwconv (DepthwiseConv2D (None, None, None, 1 10368       block7a_expand_activation[0][0]  
    __________________________________________________________________________________________________
    block7a_bn (BatchNormalization) (None, None, None, 1 4608        block7a_dwconv[0][0]             
    __________________________________________________________________________________________________
    block7a_activation (Activation) (None, None, None, 1 0           block7a_bn[0][0]                 
    __________________________________________________________________________________________________
    block7a_se_squeeze (GlobalAvera (None, 1152)         0           block7a_activation[0][0]         
    __________________________________________________________________________________________________
    block7a_se_reshape (Reshape)    (None, 1, 1, 1152)   0           block7a_se_squeeze[0][0]         
    __________________________________________________________________________________________________
    block7a_se_reduce (Conv2D)      (None, 1, 1, 48)     55344       block7a_se_reshape[0][0]         
    __________________________________________________________________________________________________
    block7a_se_expand (Conv2D)      (None, 1, 1, 1152)   56448       block7a_se_reduce[0][0]          
    __________________________________________________________________________________________________
    block7a_se_excite (Multiply)    (None, None, None, 1 0           block7a_activation[0][0]         
                                                                     block7a_se_expand[0][0]          
    __________________________________________________________________________________________________
    block7a_project_conv (Conv2D)   (None, None, None, 3 368640      block7a_se_excite[0][0]          
    __________________________________________________________________________________________________
    block7a_project_bn (BatchNormal (None, None, None, 3 1280        block7a_project_conv[0][0]       
    __________________________________________________________________________________________________
    top_conv (Conv2D)               (None, None, None, 1 409600      block7a_project_bn[0][0]         
    __________________________________________________________________________________________________
    top_bn (BatchNormalization)     (None, None, None, 1 5120        top_conv[0][0]                   
    __________________________________________________________________________________________________
    top_activation (Activation)     (None, None, None, 1 0           top_bn[0][0]                     
    ==================================================================================================
    Total params: 4,049,571
    Trainable params: 0
    Non-trainable params: 4,049,571
    __________________________________________________________________________________________________


Genel modelimiz beÅŸ katmana sahiptir, ancak gerÃ§ekte bu katmanlardan biri `(efficientnetb0)` 236 katmana sahiptir.

Ã‡Ä±ktÄ± ÅŸeklinin giriÅŸ katmanÄ± (resimlerimizin ÅŸekli) iÃ§in (None, 224, 224, 3) olarak nasÄ±l baÅŸladÄ±ÄŸÄ±nÄ±, ancak Ã§Ä±ktÄ± katmanÄ± (etiketlerimizin ÅŸekli) tarafÄ±ndan (None, 10) olarak nasÄ±l dÃ¶nÃ¼ÅŸtÃ¼rÃ¼ldÃ¼ÄŸÃ¼nÃ¼ gÃ¶rebilirsiniz. ), burada None, batch boyutu iÃ§in yer tutucudur. Modeldeki eÄŸitilebilir parametreler yalnÄ±zca Ã§Ä±ktÄ± katmanÄ±ndaki parametrelerdir.

Modelimizin eÄŸitim eÄŸrileri nasÄ±l gÃ¶rÃ¼nÃ¼yor?


```python
plot_loss_curves(history_10_percent)
```


    
![png](TensorFlow_ile_Transfer_Learning_%C4%B0nce_Ayarlama_%28Fine_Tuning%29_files/TensorFlow_ile_Transfer_Learning_%C4%B0nce_Ayarlama_%28Fine_Tuning%29_28_0.png)
    



    
![png](TensorFlow_ile_Transfer_Learning_%C4%B0nce_Ayarlama_%28Fine_Tuning%29_files/TensorFlow_ile_Transfer_Learning_%C4%B0nce_Ayarlama_%28Fine_Tuning%29_28_1.png)
    


## EÄŸitilmiÅŸ Bir Modelden Ã–zellik VektÃ¶rÃ¼ Alma

> ðŸ¤” Soru: tf.keras.layers.GlobalAveragePooling2D() katmanÄ±nÄ± da ne? Daha Ã¶nce gÃ¶rmedim.

[`tf.keras.layers.GlobalAveragePooling2D()`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalAveragePooling2D) katmanÄ±, iÃ§ eksenlerdeki deÄŸerlerin ortalamasÄ±nÄ± alarak 4B tensÃ¶rÃ¼ 2B tensÃ¶re dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.

YukarÄ±da ki tanÄ±m biraz karmaÅŸÄ±k gibi, o yÃ¼zden bir Ã¶rnek gÃ¶relim.



```python
# GiriÅŸ tensÃ¶r ÅŸeklini tanÄ±mlayÄ±n 
input_shape = (1, 4, 4, 3)

# random bir tensÃ¶r oluÅŸturun
tf.random.set_seed(42)
input_tensor = tf.random.normal(input_shape)
print(f"Random input tensor:\n {input_tensor}\n")

# rastgele tensÃ¶rÃ¼ global average pooling 2D katmanÄ±ndan geÃ§irin
global_average_pooled_tensor = tf.keras.layers.GlobalAveragePooling2D()(input_tensor)
print(f"2D global average pooled random tensor:\n {global_average_pooled_tensor}\n")

# TensÃ¶rlerin ÅŸekillerini kontrol edin
print(f"Shape of input tensor: {input_tensor.shape}")
print(f"Shape of 2D global averaged pooled input tensor: {global_average_pooled_tensor.shape}")
```

    Random input tensor:
     [[[[ 0.3274685  -0.8426258   0.3194337 ]
       [-1.4075519  -2.3880599  -1.0392479 ]
       [-0.5573232   0.539707    1.6994323 ]
       [ 0.28893656 -1.5066116  -0.2645474 ]]
    
      [[-0.59722406 -1.9171132  -0.62044144]
       [ 0.8504023  -0.40604794 -3.0258412 ]
       [ 0.9058464   0.29855987 -0.22561555]
       [-0.7616443  -1.8917141  -0.93847126]]
    
      [[ 0.77852213 -0.47338897  0.97772694]
       [ 0.24694404  0.20573747 -0.5256233 ]
       [ 0.32410017  0.02545409 -0.10638497]
       [-0.6369475   1.1603122   0.2507359 ]]
    
      [[-0.41728503  0.4012578  -1.4145443 ]
       [-0.5931857  -1.6617213   0.33567193]
       [ 0.10815629  0.23479682 -0.56668764]
       [-0.35819843  0.88698614  0.52744764]]]]
    
    2D global average pooled random tensor:
     [[-0.09368646 -0.45840448 -0.2885598 ]]
    
    Shape of input tensor: (1, 4, 4, 3)
    Shape of 2D global averaged pooled input tensor: (1, 3)


`tf.keras.layers.GlobalAveragePooling2D()` katmanÄ±nÄ±n giriÅŸ tensÃ¶rÃ¼nÃ¼ (1, 4, 4, 3) ÅŸeklinden (1, 3) ÅŸekline deÄŸiÅŸtiÄŸini gÃ¶rebilirsiniz. Bunu, ortadaki iki eksen boyunca `input_tensor`'un ortalamasÄ±nÄ± alarak yaptÄ±.

Bu iÅŸlemi `tf.reduce_mean()` iÅŸlevini kullanarak ve uygun eksenleri belirleyerekte Ã§oÄŸaltabiliriz.


```python
tf.reduce_mean(input_tensor, axis=[1, 2])
```




    <tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[-0.09368646, -0.45840448, -0.2885598 ]], dtype=float32)>



Bunu yapmak, yalnÄ±zca temel modelin Ã§Ä±ktÄ±sÄ±nÄ±, Ã§Ä±ktÄ± katmanÄ±mÄ±zÄ±n (tf.keras.layers.Dense()) girdi ÅŸekli gereksinimi ile uyumlu hale getirmekle kalmaz, aynÄ± zamanda temel model tarafÄ±ndan bulunan bilgiyi daha dÃ¼ÅŸÃ¼k boyutlu bir Ã¶zellik vektÃ¶rÃ¼nde yoÄŸunlaÅŸtÄ±rÄ±r.

> ðŸ”‘ Not: Ã–zellik Ã§Ä±karma transfer Ã¶ÄŸreniminin nasÄ±l olduÄŸu olarak adlandÄ±rÄ±lmasÄ±nÄ±n nedenlerinden biri, Ã¶nceden eÄŸitilmiÅŸ bir modelin bir Ã¶zellik vektÃ¶rÃ¼ vermesidir (bizim durumumuzda bu, [tf.keras.Layer.GlobalAveragePooling2D()](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalAveragePooling2D)  katmanÄ± Ã§Ä±ktÄ±sÄ±dÄ±r.), daha sonra kalÄ±plarÄ± Ã§Ä±karmak iÃ§in kullanÄ±labilir.

> ðŸ›  AlÄ±ÅŸtÄ±rma: YukarÄ±daki hÃ¼creyle aynÄ±sÄ±nÄ± yapÄ±n, ancak [tf.keras.layers.GlobalMaxPool2D()](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalMaxPool2D) iÃ§in.

## Transfer Learning Deneylerini Ã‡alÄ±ÅŸtÄ±rma

EÄŸitim verilerinin %10'unda transfer Ã¶ÄŸreniminin inanÄ±lmaz sonuÃ§larÄ±nÄ± gÃ¶rdÃ¼k, peki ya eÄŸitim verilerinin %1'i?

Kendi yaptÄ±ÄŸÄ±mÄ±z orijinal CNN modellerinden 100 kat daha az veri kullanarak ne tÃ¼r sonuÃ§lar elde edebileceÄŸimizi dÃ¼ÅŸÃ¼nÃ¼yorsunuz?

AÅŸaÄŸÄ±daki modelleme deneylerini Ã§alÄ±ÅŸtÄ±rÄ±rken neden bu soruyu yanÄ±tlamÄ±yoruz:

- `model_1` : Veri bÃ¼yÃ¼tme ile eÄŸitim verilerinin %1'inde Ã¶zellik Ã§Ä±karma aktarÄ±mÄ± Ã¶ÄŸrenimini kullanÄ±n.
- `model_2`: Veri bÃ¼yÃ¼tme ile eÄŸitim verilerinin %10'unda Ã¶zellik Ã§Ä±karma aktarÄ±mÄ± Ã¶ÄŸrenimini kullanÄ±n.
- `model_3`: Veri artÄ±rma ile eÄŸitim verilerinin %10'unda transfer Ã¶ÄŸrenimini ince ayar kullanarak kullanÄ±n.
- `model_4`: Veri bÃ¼yÃ¼tme ile eÄŸitim verilerinin %100'Ã¼nde transfer Ã¶ÄŸrenimi ince ayarÄ±nÄ± kullanÄ±n.

TÃ¼m deneyler, eÄŸitim verilerinin farklÄ± versiyonlarÄ± Ã¼zerinde yÃ¼rÃ¼tÃ¼lecek olsa da, hepsi aynÄ± test veri setinde deÄŸerlendirilecek ve bu, her deneyin sonuÃ§larÄ±nÄ±n mÃ¼mkÃ¼n olduÄŸunca karÅŸÄ±laÅŸtÄ±rÄ±labilir olmasÄ±nÄ± saÄŸlar.

TÃ¼m deneyler tf.keras.applications modÃ¼lÃ¼ iÃ§erisinde **EfficientNetB0** modeli kullanÄ±larak yapÄ±lacaktÄ±r.

Deneylerimizi takip ettiÄŸimizden emin olmak iÃ§in tÃ¼m model eÄŸitim gÃ¼nlÃ¼klerini gÃ¼nlÃ¼ÄŸe kaydetmek iÃ§in `create_tensorboard_callback()` iÅŸlevimizi kullanacaÄŸÄ±z.

Her modeli Keras Functional API kullanarak oluÅŸturacaÄŸÄ±z ve daha Ã¶nce yaptÄ±ÄŸÄ±mÄ±z gibi `ImageDataGenerator` sÄ±nÄ±fÄ±nda veri bÃ¼yÃ¼tmeyi uygulamak yerine, onu `tf.keras.layers.experimental.preprocessing` modÃ¼lÃ¼nÃ¼ kullanarak doÄŸrudan modelin iÃ§ine inÅŸa edeceÄŸiz.

Veri bÃ¼yÃ¼tmeli eÄŸitim verilerinin %1'inde Ã¶zellik Ã§Ä±karma aktarÄ±mÄ± Ã¶ÄŸrenimini kullanarak deney_1 iÃ§in verileri indirerek baÅŸlayalÄ±m.


```python
# verisetini iÃ§eriye aktaralÄ±m
!gdown --id 1B76twu4qxiFcRrTRwxJnRgZd7sC_Q3Gv
unzip_data("10_food_classes_1_percent.zip")

train_dir_1_percent = "10_food_classes_1_percent/train/"
test_dir = "10_food_classes_1_percent/test/"
```

    Downloading...
    From: https://drive.google.com/uc?id=1B76twu4qxiFcRrTRwxJnRgZd7sC_Q3Gv
    To: /content/10_food_classes_1_percent.zip
    134MB [00:01, 90.6MB/s]



```python
walk_through_dir("10_food_classes_1_percent")
```

    '10_food_classes_1_percent' klasÃ¶rÃ¼nde 0 veri var.
    '10_food_classes_1_percent/train' klasÃ¶rÃ¼nde 0 veri var.
    '10_food_classes_1_percent/train/chicken_wings' klasÃ¶rÃ¼nde 7 veri var.
    '10_food_classes_1_percent/train/sushi' klasÃ¶rÃ¼nde 7 veri var.
    '10_food_classes_1_percent/train/steak' klasÃ¶rÃ¼nde 7 veri var.
    '10_food_classes_1_percent/train/pizza' klasÃ¶rÃ¼nde 7 veri var.
    '10_food_classes_1_percent/train/ice_cream' klasÃ¶rÃ¼nde 7 veri var.
    '10_food_classes_1_percent/train/ramen' klasÃ¶rÃ¼nde 7 veri var.
    '10_food_classes_1_percent/train/fried_rice' klasÃ¶rÃ¼nde 7 veri var.
    '10_food_classes_1_percent/train/chicken_curry' klasÃ¶rÃ¼nde 7 veri var.
    '10_food_classes_1_percent/train/grilled_salmon' klasÃ¶rÃ¼nde 7 veri var.
    '10_food_classes_1_percent/train/hamburger' klasÃ¶rÃ¼nde 7 veri var.
    '10_food_classes_1_percent/test' klasÃ¶rÃ¼nde 0 veri var.
    '10_food_classes_1_percent/test/chicken_wings' klasÃ¶rÃ¼nde 250 veri var.
    '10_food_classes_1_percent/test/sushi' klasÃ¶rÃ¼nde 250 veri var.
    '10_food_classes_1_percent/test/steak' klasÃ¶rÃ¼nde 250 veri var.
    '10_food_classes_1_percent/test/pizza' klasÃ¶rÃ¼nde 250 veri var.
    '10_food_classes_1_percent/test/ice_cream' klasÃ¶rÃ¼nde 250 veri var.
    '10_food_classes_1_percent/test/ramen' klasÃ¶rÃ¼nde 250 veri var.
    '10_food_classes_1_percent/test/fried_rice' klasÃ¶rÃ¼nde 250 veri var.
    '10_food_classes_1_percent/test/chicken_curry' klasÃ¶rÃ¼nde 250 veri var.
    '10_food_classes_1_percent/test/grilled_salmon' klasÃ¶rÃ¼nde 250 veri var.
    '10_food_classes_1_percent/test/hamburger' klasÃ¶rÃ¼nde 250 veri var.


Pekala, her sÄ±nÄ±ftan yalnÄ±zca yedi resmimiz var gibi gÃ¶rÃ¼nÃ¼yor, bu modelimiz iÃ§in biraz zor olmalÄ±.

> ðŸ”‘ Not: Veri alt kÃ¼mesinin %10'unda olduÄŸu gibi, gÃ¶rÃ¼ntÃ¼lerin %1'i orijinal tam eÄŸitim veri kÃ¼mesinden rastgele seÃ§ilmiÅŸtir.

Resimlerimizi `tf.data.Dataset` nesneleri olarak yÃ¼kleme zamanÄ±, bunu yapmak iÃ§in `image_dataset_from_directory()` yÃ¶ntemini kullanacaÄŸÄ±z.


```python
import tensorflow as tf

IMG_SIZE = (224, 224)

train_data_1_percent = tf.keras.preprocessing.image_dataset_from_directory(train_dir_1_percent,
                                                                           label_mode="categorical",
                                                                           batch_size=32,
                                                                           image_size=IMG_SIZE)
test_data = tf.keras.preprocessing.image_dataset_from_directory(test_dir,
                                                                label_mode="categorical",
                                                                image_size=IMG_SIZE)
```

    Found 70 files belonging to 10 classes.
    Found 2500 files belonging to 10 classes.


## DoÄŸrudan modele veri bÃ¼yÃ¼tme ekleme

Daha Ã¶nce eÄŸitim gÃ¶rÃ¼ntÃ¼lerimizi gÃ¼Ã§lendirmek iÃ§in `ImageDataGenerator` sÄ±nÄ±fÄ±nÄ±n farklÄ± parametrelerini kullandÄ±k, bu sefer doÄŸrudan modelin iÃ§ine veri bÃ¼yÃ¼tmeyi inÅŸa edeceÄŸiz.

NasÄ±l?

`tf.keras.layers.experimental.preprocessing` modÃ¼lÃ¼nÃ¼ kullanarak ve Ã¶zel bir veri bÃ¼yÃ¼tme katmanÄ± kullanarak.

Bu, TensorFlow 2.2+ sÃ¼rÃ¼mÃ¼ne eklenen nispeten yeni bir Ã¶zelliktir ancak Ã§ok gÃ¼Ã§lÃ¼dÃ¼r. Modele bir veri bÃ¼yÃ¼tme katmanÄ± eklemek aÅŸaÄŸÄ±daki avantajlara sahiptir:

- GÃ¶rÃ¼ntÃ¼lerin Ã¶n iÅŸlemesi (artÄ±rÄ±lmasÄ±) CPU yerine GPU'da gerÃ§ekleÅŸir (Ã§ok daha hÄ±zlÄ±).
  - GÃ¶rÃ¼ntÃ¼ler en iyi GPU'da Ã¶n iÅŸlenirken, metin ve yapÄ±landÄ±rÄ±lmÄ±ÅŸ veriler CPU'da Ã¶niÅŸlenmeye daha uygundur.
- GÃ¶rÃ¼ntÃ¼ verisi bÃ¼yÃ¼tme yalnÄ±zca eÄŸitim sÄ±rasÄ±nda gerÃ§ekleÅŸir, bu nedenle tÃ¼m modelimizi dÄ±ÅŸa aktarabilir ve baÅŸka bir yerde kullanabiliriz.

> ðŸ¤” Not: Yazma sÄ±rasÄ±nda, veri bÃ¼yÃ¼tme iÃ§in kullandÄ±ÄŸÄ±mÄ±z Ã¶n iÅŸleme katmanlarÄ±, TensorFlow kitaplÄ±ÄŸÄ±nda deneysel durumda. Bu, katmanlarÄ±n kararlÄ± olarak kabul edilmesi gerekmesine raÄŸmen, TensorFlow'un gelecekteki bir sÃ¼rÃ¼mÃ¼nde kodun biraz deÄŸiÅŸebileceÄŸi anlamÄ±na gelir. Mevcut diÄŸer Ã¶n iÅŸleme katmanlarÄ± ve farklÄ± veri artÄ±rma yÃ¶ntemleri hakkÄ±nda daha fazla bilgi iÃ§in [Keras Ã¶n iÅŸleme katmanlarÄ± kÄ±lavuzuna](https://keras.io/guides/preprocessing_layers/) ve [TensorFlow veri artÄ±rma kÄ±lavuzuna](https://www.tensorflow.org/tutorials/images/data_augmentation) bakÄ±n.


Veri bÃ¼yÃ¼tmeyi doÄŸrudan modelimizde kullanmak iÃ§in, yalnÄ±zca veri Ã¶n iÅŸleme katmanlarÄ±ndan oluÅŸan bir Keras SÄ±ralÄ± modeli oluÅŸturacaÄŸÄ±z, daha sonra bu SÄ±ralÄ± modeli baÅŸka bir iÅŸlevsel model iÃ§inde kullanabiliriz.

Bu kafa karÄ±ÅŸtÄ±rÄ±cÄ± geliyorsa, kodda oluÅŸturduÄŸumuzda mantÄ±klÄ± olacaktÄ±r.

KullanacaÄŸÄ±mÄ±z veri bÃ¼yÃ¼tme dÃ¶nÃ¼ÅŸÃ¼mleri ÅŸunlardÄ±r:

- `RandomFlip` - gÃ¶rÃ¼ntÃ¼yÃ¼ yatay veya dikey eksende dÃ¶ndÃ¼rÃ¼r.
- `RandomRotation` - gÃ¶rÃ¼ntÃ¼yÃ¼ belirli bir miktarda rastgele dÃ¶ndÃ¼rÃ¼r.
- `RandomZoom` - bir gÃ¶rÃ¼ntÃ¼yÃ¼ belirtilen miktarda rastgele yakÄ±nlaÅŸtÄ±rÄ±r.
- `RandomHeight` - gÃ¶rÃ¼ntÃ¼ yÃ¼ksekliÄŸini belirli bir miktarda rastgele kaydÄ±rÄ±r.
- `RandomWidth` - gÃ¶rÃ¼ntÃ¼ geniÅŸliÄŸini belirli bir miktarda rastgele kaydÄ±rÄ±r.
- `Rescaling` - gÃ¶rÃ¼ntÃ¼ piksel deÄŸerlerini 0 ile 1 arasÄ±nda olacak ÅŸekilde normalleÅŸtirir, bazÄ± gÃ¶rÃ¼ntÃ¼ modelleri iÃ§in gerekli olduÄŸundan ancak EfficientNetB0'Ä±n tf.keras.applications uygulamasÄ±nÄ± kullandÄ±ÄŸÄ±mÄ±zdan bu gerekli deÄŸildir.

Daha fazla Ã¶zellik var ama ÅŸimdilik bunlar yeterli.


```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.layers.experimental import preprocessing

data_augmentation = keras.Sequential([
  preprocessing.RandomFlip("horizontal"),
  preprocessing.RandomRotation(0.2),
  preprocessing.RandomZoom(0.2),
  preprocessing.RandomHeight(0.2),
  preprocessing.RandomWidth(0.2),
  # preprocessing.Rescaling(1./255)
], name ="data_augmentation")
```

Ve bu kadar! Veri bÃ¼yÃ¼tme SÄ±ralÄ± modelimiz kullanÄ±ma hazÄ±r. Birazdan gÃ¶receÄŸiniz gibi, bunu daha sonra transfer Ã¶ÄŸrenme modelimize bir katman olarak yerleÅŸtirebileceÄŸiz.

Ama bunu yapmadan Ã¶nce, iÃ§inden rastgele gÃ¶rÃ¼ntÃ¼ler seÃ§erek test edelim.


```python
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import os
import random

target_class = random.choice(train_data_1_percent.class_names) # random bir class seÃ§me
target_dir = "10_food_classes_1_percent/train/" + target_class # bir hedef dizini oluÅŸturma
random_image = random.choice(os.listdir(target_dir)) # random bir gÃ¶rÃ¼ntÃ¼yÃ¼ etikeye baÄŸlÄ± olarak seÃ§me
random_image_path = target_dir + "/" + random_image # seÃ§ilen rastgele gÃ¶rÃ¼ntÃ¼nÃ¼n yolu oluÅŸturma
img = mpimg.imread(random_image_path) # seÃ§ilen hedef gÃ¶rÃ¼ntÃ¼yÃ¼ okuma
plt.imshow(img) 

plt.title(f"Original random image from class: {target_class}")
plt.axis(False);

augmented_img = data_augmentation(tf.expand_dims(img, axis=0)) 
plt.figure()
plt.imshow(tf.squeeze(augmented_img)/255.)
plt.title(f"Augmented random image from class: {target_class}")
plt.axis(False);
```


    
![png](TensorFlow_ile_Transfer_Learning_%C4%B0nce_Ayarlama_%28Fine_Tuning%29_files/TensorFlow_ile_Transfer_Learning_%C4%B0nce_Ayarlama_%28Fine_Tuning%29_43_0.png)
    



    
![png](TensorFlow_ile_Transfer_Learning_%C4%B0nce_Ayarlama_%28Fine_Tuning%29_files/TensorFlow_ile_Transfer_Learning_%C4%B0nce_Ayarlama_%28Fine_Tuning%29_43_1.png)
    


YukarÄ±daki kod bloÄŸunu birkaÃ§ kez Ã§alÄ±ÅŸtÄ±rÄ±n ve farklÄ± gÃ¶rÃ¼ntÃ¼ sÄ±nÄ±flarÄ±nda farklÄ± rastgele bÃ¼yÃ¼tmeleri gÃ¶rebilirsiniz. Gelecek transfer Ã¶ÄŸrenme modelimizde veri bÃ¼yÃ¼tme modelini bir katman olarak ekleyeceÄŸimiz iÃ§in, iÃ§inden geÃ§en eÄŸitim gÃ¶rÃ¼ntÃ¼lerinin her birine bu tÃ¼r rastgele bÃ¼yÃ¼tmeler uygulayacaktÄ±r.

Bunu yapmak, eÄŸitim veri setimizi biraz daha Ã§eÅŸitli hale getirecektir. GerÃ§ek hayatta bir yemek fotoÄŸrafÄ± Ã§ekiyormuÅŸsunuz gibi dÃ¼ÅŸÃ¼nebilirsiniz, gÃ¶rÃ¼ntÃ¼lerin hepsi mÃ¼kemmel olmayacak, bazÄ±larÄ± garip ÅŸekillerde yÃ¶nlendirilecek. Bunlar, modelimizin iÅŸlemesini istediÄŸimiz tÃ¼rden gÃ¶rÃ¼ntÃ¼ler.

### Model 1: Veri BÃ¼yÃ¼tme ile Verilerin %1'inde Ã–zellik Ã‡Ä±karma 


```python
# input ÅŸekli base_model olarak ayarlarÄ±n, base_model'in katmanlarÄ±nÄ± dondurun
input_shape = (224, 224, 3)
base_model = tf.keras.applications.EfficientNetB0(include_top=False)
base_model.trainable = False

# input katmanÄ± yaratÄ±n
inputs = layers.Input(shape=input_shape, name="input_layer")

# Veri bÃ¼yÃ¼tme sÄ±ralÄ± modelini katman olarak ekleyin
x = data_augmentation(inputs)

# Base_model girdilerini verin (bÃ¼yÃ¼tmeden sonra) ama onu eÄŸitmeyin
x = base_model(x, training=False)

# base modelin pooling output Ã¶zellikleri
x = layers.GlobalAveragePooling2D(name="global_average_pooling_layer")(x)

# output olarak yoÄŸun bir katman koyun
outputs = layers.Dense(10, activation="softmax", name="output_layer")(x)

# Girdileri ve Ã§Ä±ktÄ±larÄ± olan bir model oluÅŸturun
model_1 = keras.Model(inputs, outputs)

# modeli derleyin
model_1.compile(loss="categorical_crossentropy",
              optimizer=tf.keras.optimizers.Adam(),
              metrics=["accuracy"])

# modeli fit edin
history_1_percent = model_1.fit(train_data_1_percent,
                    epochs=5,
                    steps_per_epoch=len(train_data_1_percent),
                    validation_data=test_data,
                    validation_steps=int(0.25* len(test_data)),
                    callbacks=[create_tensorboard_callback("transfer_learning", "1_percent_data_aug")])
```

    TensorBoard verilerini bu klasÃ¶re kaydet: transfer_learning/1_percent_data_aug/20210720-051748


    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.
      category=CustomMaskWarning)


    Epoch 1/5
    3/3 [==============================] - 13s 3s/step - loss: 2.4057 - accuracy: 0.0429 - val_loss: 2.2374 - val_accuracy: 0.1595
    Epoch 2/5
    3/3 [==============================] - 4s 2s/step - loss: 2.1606 - accuracy: 0.1571 - val_loss: 2.1274 - val_accuracy: 0.2516
    Epoch 3/5
    3/3 [==============================] - 4s 2s/step - loss: 1.9891 - accuracy: 0.3000 - val_loss: 1.9951 - val_accuracy: 0.3503
    Epoch 4/5
    3/3 [==============================] - 3s 2s/step - loss: 1.7833 - accuracy: 0.4714 - val_loss: 1.9065 - val_accuracy: 0.4161
    Epoch 5/5
    3/3 [==============================] - 3s 1s/step - loss: 1.6344 - accuracy: 0.7143 - val_loss: 1.8110 - val_accuracy: 0.4770


SÄ±nÄ±f baÅŸÄ±na yalnÄ±zca 7 eÄŸitim gÃ¶rÃ¼ntÃ¼sÃ¼ kullanarak, transfer Ã¶ÄŸrenmeyi kullanarak modelimiz doÄŸrulama setinde ~%40 doÄŸruluk elde edebildi. Orijinal Food-101 belgesi tÃ¼m verilerle, yani sÄ±nÄ±f baÅŸÄ±na 750 eÄŸitim gÃ¶rÃ¼ntÃ¼sÃ¼ ile %50,67 doÄŸruluk elde ettiÄŸinden bu sonuÃ§ oldukÃ§a ÅŸaÅŸÄ±rtÄ±cÄ±dÄ±r.

Modelimizin bir Ã¶zetini kontrol edersek, girdi katmanÄ±ndan hemen sonra veri bÃ¼yÃ¼tme katmanÄ±nÄ± gÃ¶rmeliyiz.


```python
model_1.summary()
```

    Model: "model_1"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    input_layer (InputLayer)     [(None, 224, 224, 3)]     0         
    _________________________________________________________________
    data_augmentation (Sequentia (None, None, None, 3)     0         
    _________________________________________________________________
    efficientnetb0 (Functional)  (None, None, None, 1280)  4049571   
    _________________________________________________________________
    global_average_pooling_layer (None, 1280)              0         
    _________________________________________________________________
    output_layer (Dense)         (None, 10)                12810     
    =================================================================
    Total params: 4,062,381
    Trainable params: 12,810
    Non-trainable params: 4,049,571
    _________________________________________________________________


Ä°ÅŸte burada. ArtÄ±k doÄŸrudan modelimizin iÃ§ine yerleÅŸtirilmiÅŸ veri bÃ¼yÃ¼tmeye sahibiz. Bu, onu kaydedip baÅŸka bir yere yeniden yÃ¼klesek, veri artÄ±rma katmanlarÄ±nÄ±n da onunla birlikte geleceÄŸi anlamÄ±na gelir.

HatÄ±rlanmasÄ± gereken Ã¶nemli ÅŸey, veri artÄ±rmanÄ±n yalnÄ±zca eÄŸitim sÄ±rasÄ±nda Ã§alÄ±ÅŸtÄ±ÄŸÄ±dÄ±r. DolayÄ±sÄ±yla, modelimizi Ã§Ä±karÄ±m iÃ§in deÄŸerlendirecek veya kullanacak olursak (bir gÃ¶rÃ¼ntÃ¼nÃ¼n sÄ±nÄ±fÄ±nÄ± tahmin ederek) veri bÃ¼yÃ¼tme katmanlarÄ± otomatik olarak kapatÄ±lacaktÄ±r.

Bunu Ã§alÄ±ÅŸÄ±rken gÃ¶rmek iÃ§in modelimizi test verileri Ã¼zerinden deÄŸerlendirelim.


```python
results_1_percent_data_aug = model_1.evaluate(test_data)
results_1_percent_data_aug
```

    79/79 [==============================] - 10s 120ms/step - loss: 1.8225 - accuracy: 0.4588





    [1.8224549293518066, 0.45879998803138733]



Buradaki sonuÃ§lar, eÄŸitim sÄ±rasÄ±nda modelimizin gÃ¼nlÃ¼k Ã§Ä±ktÄ±larÄ±ndan biraz daha iyi/daha kÃ¶tÃ¼ olabilir, Ã§Ã¼nkÃ¼ eÄŸitim sÄ±rasÄ±nda modelimizi validation_steps=int(0.25 * len(test_data)) satÄ±rÄ±nÄ± kullanarak test verilerinin yalnÄ±zca %25'i Ã¼zerinde deÄŸerlendiririz. Bunu yapmak epoch'u hÄ±zlandÄ±rÄ±r ama yine de bize modelimizin nasÄ±l gittiÄŸine dair yeterince fikir verir.

TutarlÄ± kalalÄ±m ve modelimizin kayÄ±p eÄŸrilerini kontrol edelim.



```python
plot_loss_curves(history_1_percent)
```


    
![png](TensorFlow_ile_Transfer_Learning_%C4%B0nce_Ayarlama_%28Fine_Tuning%29_files/TensorFlow_ile_Transfer_Learning_%C4%B0nce_Ayarlama_%28Fine_Tuning%29_52_0.png)
    



    
![png](TensorFlow_ile_Transfer_Learning_%C4%B0nce_Ayarlama_%28Fine_Tuning%29_files/TensorFlow_ile_Transfer_Learning_%C4%B0nce_Ayarlama_%28Fine_Tuning%29_52_1.png)
    


Daha fazla epoch iÃ§in eÄŸitime devam edersek, her iki veri kÃ¼mesindeki metrikler de iyileÅŸecek gibi gÃ¶rÃ¼nÃ¼yor. Ama ÅŸimdilik bunu bÄ±rakalÄ±m, yapacak daha Ã§ok iÅŸimiz var!

### Model 2: %10 Veri ve Veri BÃ¼yÃ¼tme Ä°le Ã–zellik Ã‡Ä±karma

Pekala, veri artÄ±rma ile eÄŸitim verilerinin %1'ini test ettik, veri artÄ±rma ile verilerin %10'unu denemeye ne dersiniz?

Fakat bekle...

> ðŸ¤” Soru: Hangi deneyleri Ã§alÄ±ÅŸtÄ±racaÄŸÄ±nÄ±zÄ± nereden biliyorsunuz?

Harika bir soru.

Buradaki gerÃ§ek ÅŸu ki, Ã§oÄŸu zaman bunu bilemeyeceksiniz. Makine Ã¶ÄŸrenimi hala Ã§ok deneysel bir uygulamadÄ±r. Sadece birkaÃ§ ÅŸeyi denedikten sonra, ne denemeniz gerektiÄŸine dair bir sezgi geliÅŸtirmeye baÅŸlayacaksÄ±nÄ±z.

Benim tavsiyem, merakÄ±nÄ±zÄ± mÃ¼mkÃ¼n olduÄŸunca inatla takip etmenizdir. Bir ÅŸey denemek istediÄŸinizi dÃ¼ÅŸÃ¼nÃ¼yorsanÄ±z, kodunu yazÄ±n ve Ã§alÄ±ÅŸtÄ±rÄ±n. NasÄ±l gittiÄŸini gÃ¶r. Olabilecek en kÃ¶tÃ¼ ÅŸey, neyin iÅŸe yaramadÄ±ÄŸÄ±nÄ±, en deÄŸerli bilgi tÃ¼rÃ¼nÃ¼ bulmanÄ±zdÄ±r.

Pratik aÃ§Ä±dan, daha Ã¶nce bahsettiÄŸimiz gibi, ilk deneyleriniz arasÄ±ndaki sÃ¼reyi mÃ¼mkÃ¼n olduÄŸunca azaltmak isteyeceksiniz. BaÅŸka bir deyiÅŸle, umut verici bir ÅŸey bulmadan Ã¶nce daha az veri ve daha az eÄŸitim yinelemesi kullanarak Ã§ok sayÄ±da daha kÃ¼Ã§Ã¼k deney yapÄ±n ve ardÄ±ndan Ã¶lÃ§eÄŸi bÃ¼yÃ¼tÃ¼n.

Ã–lÃ§ek temasÄ±nda, %1 eÄŸitim verisi artÄ±rma denememizi %10 eÄŸitim verisi artÄ±rÄ±mÄ±na kadar Ã¶lÃ§eklendirelim. Bu cÃ¼mle pek mantÄ±klÄ± deÄŸil ama ne demek istediÄŸimi anladÄ±nÄ±z.

Ã–nceki modelle tamamen aynÄ± adÄ±mlarÄ± uygulayacaÄŸÄ±z, tek fark eÄŸitim verilerinin %1 yerine %10'unu kullanmaktÄ±r.


```python
train_dir_10_percent = "10_food_classes_10_percent/train/"
test_dir = "10_food_classes_10_percent/test/"
```


```python
import tensorflow as tf
IMG_SIZE = (224, 224)
train_data_10_percent = tf.keras.preprocessing.image_dataset_from_directory(train_dir_10_percent,
                                                                            label_mode="categorical",
                                                                            image_size=IMG_SIZE)
test_data = tf.keras.preprocessing.image_dataset_from_directory(test_dir,
                                                                label_mode="categorical",
                                                                image_size=IMG_SIZE)
```

    Found 750 files belonging to 10 classes.
    Found 2500 files belonging to 10 classes.


Harika! Ã‡alÄ±ÅŸmak iÃ§in 10 kat daha fazla gÃ¶rselimiz var, sÄ±nÄ±f baÅŸÄ±na 7 yerine sÄ±nÄ±f baÅŸÄ±na 75.

Veri bÃ¼yÃ¼tmenin yerleÅŸik olduÄŸu bir model oluÅŸturalÄ±m. Daha Ã¶nce oluÅŸturduÄŸumuz veri bÃ¼yÃ¼tme SÄ±ralÄ± modelini yeniden kullanabiliriz, ancak pratik yapmak iÃ§in yeniden oluÅŸturacaÄŸÄ±z.


```python
import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras.layers.experimental import preprocessing
from tensorflow.keras.models import Sequential

data_augmentation = Sequential([
  preprocessing.RandomFlip('horizontal'),
  preprocessing.RandomHeight(0.2),
  preprocessing.RandomWidth(0.2),
  preprocessing.RandomZoom(0.2),
  preprocessing.RandomRotation(0.2),
], name="data_augmentation")

input_shape = (224, 224, 3)

base_model = tf.keras.applications.EfficientNetB0(include_top=False)
base_model.trainable = False

inputs = layers.Input(shape=input_shape, name="input_layer") 
x = data_augmentation(inputs)
x = base_model(x, training=False) 
x = layers.GlobalAveragePooling2D(name="global_average_pooling_layer")(x)
outputs = layers.Dense(10, activation="softmax", name="output_layer")(x)
model_2 = tf.keras.Model(inputs, outputs)

model_2.compile(loss="categorical_crossentropy",
              optimizer=tf.keras.optimizers.Adam(lr=0.001),
              metrics=["accuracy"])
```

    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.
      "The `lr` argument is deprecated, use `learning_rate` instead.")


#### ModelCheckpoint ile Callback OluÅŸturma

Modelimiz derlenmiÅŸ ve fit olmaya hazÄ±r, peki neden hala Ã§alÄ±ÅŸtÄ±rmadÄ±k?

Pekala, bu deney iÃ§in yeni bir callback'i, ModelCheckpoint callback'ini tanÄ±tacaÄŸÄ±z.

[ModelCheckpoint](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint) callback'i, modelinizi bir bÃ¼tÃ¼n olarak [SavedModel](https://www.tensorflow.org/tutorials/keras/save_and_load#manually_save_weights) biÃ§iminde veya aÄŸÄ±rlÄ±klarÄ± (kalÄ±plar) yalnÄ±zca eÄŸitilirken belirli bir dizine kaydetme yeteneÄŸi verir.

Modelinizin uzun sÃ¼re eÄŸitim gÃ¶receÄŸini dÃ¼ÅŸÃ¼nÃ¼yorsanÄ±z ve eÄŸitim sÄ±rasÄ±nda yedeklerini almak istiyorsanÄ±z bu yararlÄ±dÄ±r. Bu aynÄ± zamanda, modelinizin daha uzun sÃ¼re eÄŸitilmesinden yararlanabileceÄŸini dÃ¼ÅŸÃ¼nÃ¼yorsanÄ±z, belirli bir kontrol noktasÄ±ndan yeniden yÃ¼kleyebilir ve oradan eÄŸitime devam edebilirsiniz.

Ã–rneÄŸin, 5 dÃ¶nem iÃ§in bir Ã¶zellik Ã§Ä±karma transferi Ã¶ÄŸrenme modeline uyduÄŸunuzu ve eÄŸitim eÄŸrilerini kontrol ettiÄŸinizi ve hala iyileÅŸtiÄŸini gÃ¶rdÃ¼ÄŸÃ¼nÃ¼zÃ¼ ve baÅŸka bir 5 dÃ¶nem iÃ§in ince ayarÄ±n yardÄ±mcÄ± olup olmayacaÄŸÄ±nÄ± gÃ¶rmek istediÄŸinizi varsayalÄ±m, kontrol noktasÄ±nÄ± yÃ¼kleyebilir, dondurabilirsiniz. temel model katmanlarÄ±nÄ±n bir kÄ±smÄ±nÄ± (veya tamamÄ±nÄ±) ve ardÄ±ndan eÄŸitime devam edin.

AslÄ±nda, yapacaÄŸÄ±mÄ±z ÅŸey tam olarak bu.

Ama Ã¶nce bir ModelCheckpoint callbackini oluÅŸturalÄ±m. Bunu yapmak iÃ§in, kaydetmek istediÄŸimiz bir dizini belirtmeliyiz.


```python
checkpoint_path = "ten_percent_model_checkpoints_weights/checkpoint.ckpt" 

checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,
                                                         save_weights_only=True,
                                                         save_best_only=False,
                                                         save_freq="epoch", 
                                                         verbose=1)
```

> ðŸ¤” Soru: TÃ¼m modeli kaydetme (SavedModel biÃ§imi) ile yalnÄ±zca aÄŸÄ±rlÄ±klarÄ± kaydetme arasÄ±ndaki fark nedir?

SavedModel formatÄ±, bir modelin mimarisini, aÄŸÄ±rlÄ±klarÄ±nÄ± ve eÄŸitim yapÄ±landÄ±rmasÄ±nÄ± tek bir klasÃ¶re kaydeder. Modelinizi tam olarak baÅŸka bir yerde olduÄŸu gibi yeniden yÃ¼klemenizi Ã§ok kolaylaÅŸtÄ±rÄ±r. Ancak, tÃ¼m bu ayrÄ±ntÄ±larÄ± baÅŸkalarÄ±yla paylaÅŸmak istemiyorsanÄ±z, yalnÄ±zca aÄŸÄ±rlÄ±klarÄ± kaydedip paylaÅŸmak isteyebilirsiniz (bunlar yalnÄ±zca insan tarafÄ±ndan yorumlanamayan sayÄ±larÄ±n bÃ¼yÃ¼k tensÃ¶rleri olacaktÄ±r). Disk alanÄ± bir sorunsa, yalnÄ±zca aÄŸÄ±rlÄ±klarÄ± kaydetmek daha hÄ±zlÄ±dÄ±r ve tÃ¼m modeli kaydetmekten daha az yer kaplar.

Daha sonra ince ayar yapacaÄŸÄ±mÄ±z iÃ§in, bir initial_epochs deÄŸiÅŸkeni oluÅŸturacaÄŸÄ±z ve daha sonra kullanmak Ã¼zere 5'e ayarlayacaÄŸÄ±z. AyrÄ±ca checkpoint_callback'imizi geri arama listemize ekleyeceÄŸiz.


```python
initial_epochs = 5
history_10_percent_data_aug = model_2.fit(train_data_10_percent,
                                          epochs=initial_epochs,
                                          validation_data=test_data,
                                          validation_steps=int(0.25 * len(test_data)),
                                          callbacks=[create_tensorboard_callback("transfer_learning", "10_percent_data_aug"), 
                                                     checkpoint_callback])
```

    TensorBoard verilerini bu klasÃ¶re kaydet: transfer_learning/10_percent_data_aug/20210720-052655
    Epoch 1/5


    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.
      category=CustomMaskWarning)


    24/24 [==============================] - 24s 738ms/step - loss: 1.9837 - accuracy: 0.3427 - val_loss: 1.4756 - val_accuracy: 0.6727
    
    Epoch 00001: saving model to ten_percent_model_checkpoints_weights/checkpoint.ckpt
    Epoch 2/5
    24/24 [==============================] - 11s 460ms/step - loss: 1.3512 - accuracy: 0.6787 - val_loss: 1.0259 - val_accuracy: 0.7796
    
    Epoch 00002: saving model to ten_percent_model_checkpoints_weights/checkpoint.ckpt
    Epoch 3/5
    24/24 [==============================] - 13s 542ms/step - loss: 1.0439 - accuracy: 0.7373 - val_loss: 0.8319 - val_accuracy: 0.8092
    
    Epoch 00003: saving model to ten_percent_model_checkpoints_weights/checkpoint.ckpt
    Epoch 4/5
    24/24 [==============================] - 12s 489ms/step - loss: 0.8858 - accuracy: 0.7840 - val_loss: 0.7018 - val_accuracy: 0.8339
    
    Epoch 00004: saving model to ten_percent_model_checkpoints_weights/checkpoint.ckpt
    Epoch 5/5
    24/24 [==============================] - 13s 528ms/step - loss: 0.7971 - accuracy: 0.8013 - val_loss: 0.6600 - val_accuracy: 0.8355
    
    Epoch 00005: saving model to ten_percent_model_checkpoints_weights/checkpoint.ckpt


Åžuna bakar mÄ±sÄ±n! GÃ¶rÃ¼nÃ¼ÅŸe gÃ¶re ModelCheckpoint geri aramamÄ±z iÅŸe yaradÄ± ve modelimiz her epoch'ta Ã§ok fazla ek yÃ¼k olmadan aÄŸÄ±rlÄ±klarÄ±nÄ± kurtardÄ± (tÃ¼m modeli kaydetmek yalnÄ±zca aÄŸÄ±rlÄ±klardan daha uzun sÃ¼rÃ¼yor).

Modelimizi deÄŸerlendirelim ve kayÄ±p eÄŸrilerini kontrol edelim.


```python
results_10_percent_data_aug = model_2.evaluate(test_data)
results_10_percent_data_aug
```

    79/79 [==============================] - 10s 116ms/step - loss: 0.6898 - accuracy: 0.8188





    [0.6897677183151245, 0.8187999725341797]




```python
plot_loss_curves(history_10_percent_data_aug)
```


    
![png](TensorFlow_ile_Transfer_Learning_%C4%B0nce_Ayarlama_%28Fine_Tuning%29_files/TensorFlow_ile_Transfer_Learning_%C4%B0nce_Ayarlama_%28Fine_Tuning%29_65_0.png)
    



    
![png](TensorFlow_ile_Transfer_Learning_%C4%B0nce_Ayarlama_%28Fine_Tuning%29_files/TensorFlow_ile_Transfer_Learning_%C4%B0nce_Ayarlama_%28Fine_Tuning%29_65_1.png)
    


Bunlara bakÄ±ldÄ±ÄŸÄ±nda, modelimizin %10 veri ve veri bÃ¼yÃ¼tme ile performansÄ±, veri artÄ±rma olmadan %10 veri iÃ§eren model kadar iyi deÄŸil (yukarÄ±daki model_0 sonuÃ§larÄ±na bakÄ±n), ancak eÄŸriler doÄŸru yÃ¶nde ilerliyor, yani daha uzun sÃ¼re antrenman yapmaya karar verirsek, metrikleri muhtemelen iyileÅŸir.

KaydedilmiÅŸ model aÄŸÄ±rlÄ±klarÄ±nÄ± yÃ¼klemek iÃ§in [load_weights()](https://www.tensorflow.org/tutorials/keras/save_and_load#checkpoint_callback_options) yÃ¶ntemini kullanabilirsiniz ve bu yÃ¶ntemi, kaydedilmiÅŸ aÄŸÄ±rlÄ±klarÄ±nÄ±zÄ±n depolandÄ±ÄŸÄ± yola iletebilirsiniz.


```python
model_2.load_weights(checkpoint_path)
loaded_weights_model_results = model_2.evaluate(test_data)
```

    79/79 [==============================] - 10s 116ms/step - loss: 0.6898 - accuracy: 0.8188


Åžimdi daha Ã¶nce eÄŸitilmiÅŸ modelimiz ile yÃ¼klenen modelin sonuÃ§larÄ±nÄ± karÅŸÄ±laÅŸtÄ±ralÄ±m. Bu sonuÃ§lar tam olarak aynÄ± olmasa da Ã§ok yakÄ±n olmalÄ±dÄ±r. KÃ¼Ã§Ã¼k farklÄ±lÄ±klarÄ±n nedeni, hesaplanan sayÄ±larÄ±n kesinlik dÃ¼zeyinden kaynaklanmaktadÄ±r.


```python
results_10_percent_data_aug == loaded_weights_model_results
```




    False




```python
import numpy as np

print(np.array(results_10_percent_data_aug) - np.array(loaded_weights_model_results))
```

    [-1.78813934e-07  0.00000000e+00]


### Model 3: Verilerin %10'unda Mevcut Bir Modelde Ä°nce Ayar Yapma (Fine Tuning)

Åžimdiye kadar kaydedilen modelimiz, eÄŸitim verilerinin %10'u ve veri artÄ±rma Ã¼zerinde 5 epoch boyunca Ã¶zellik Ã§Ä±karma aktarÄ±mÄ± Ã¶ÄŸrenimi kullanÄ±larak eÄŸitildi.

Bu, temel modeldeki (EfficientNetB0) tÃ¼m katmanlarÄ±n eÄŸitim sÄ±rasÄ±nda dondurulduÄŸu anlamÄ±na gelir.

Bir sonraki denememiz iÃ§in transfer Ã¶ÄŸreniminin ince ayarÄ±na geÃ§eceÄŸiz. Bu, bazÄ± katmanlarÄ±nÄ± (Ã¼ste en yakÄ±n olanlarÄ±) Ã§Ã¶zmemiz ve modeli birkaÃ§ dÃ¶nem daha Ã§alÄ±ÅŸtÄ±rmamÄ±z dÄ±ÅŸÄ±nda aynÄ± temel modeli kullanacaÄŸÄ±mÄ±z anlamÄ±na gelir.

Ä°nce ayar fikri, Ã¶nceden eÄŸitilmiÅŸ modeli kendi verilerimize gÃ¶re daha fazla Ã¶zelleÅŸtirmeye baÅŸlamaktÄ±r.

> ðŸ”‘ Not: Ä°nce ayar genellikle en iyi, birkaÃ§ epoch iÃ§in ve bÃ¼yÃ¼k miktarda veri iÃ§eren bir Ã¶zellik Ã§Ä±karma modelini eÄŸittikten sonra Ã§alÄ±ÅŸÄ±r.

YÃ¼klenen modelimizin performansÄ±nÄ± doÄŸruladÄ±k, katmanlarÄ±na bir gÃ¶z atalÄ±m.


```python
model_2.layers
```




    [<tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff39a499790>,
     <tensorflow.python.keras.engine.sequential.Sequential at 0x7ff4b85e9810>,
     <tensorflow.python.keras.engine.functional.Functional at 0x7ff311a008d0>,
     <tensorflow.python.keras.layers.pooling.GlobalAveragePooling2D at 0x7ff3119c8390>,
     <tensorflow.python.keras.layers.core.Dense at 0x7ff311984f90>]




```python
for layer in model_2.layers:
  print(layer.trainable)
```

Ä°yi gÃ¶rÃ¼nÃ¼yor. Bir girdi katmanÄ±na, bir SÄ±ralÄ± katmana (veri artÄ±rma modeli), bir Ä°ÅŸlevsel katmana (EfficientNetB0), bir havuz katmanÄ±na ve bir YoÄŸun katmana (Ã§Ä±kÄ±ÅŸ katmanÄ±) sahibiz.

Bir Ã¶zete ne dersiniz?


```python
model_2.summary()
```

    Model: "model_2"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    input_layer (InputLayer)     [(None, 224, 224, 3)]     0         
    _________________________________________________________________
    data_augmentation (Sequentia (None, None, None, 3)     0         
    _________________________________________________________________
    efficientnetb0 (Functional)  (None, None, None, 1280)  4049571   
    _________________________________________________________________
    global_average_pooling_layer (None, 1280)              0         
    _________________________________________________________________
    output_layer (Dense)         (None, 10)                12810     
    =================================================================
    Total params: 4,062,381
    Trainable params: 12,810
    Non-trainable params: 4,049,571
    _________________________________________________________________


Pekala, `efficientnetb0` katmanÄ±ndaki tÃ¼m katmanlar donmuÅŸ gibi gÃ¶rÃ¼nÃ¼yor. Bunu `trainable_variables` niteliÄŸini kullanarak onaylayabiliriz.


```python
print(len(model_2.layers[2].trainable_variables))
```

    0


Bu, base modelimiz ile aynÄ±dÄ±r.


```python
print(len(base_model.trainable_variables))
```

    0


EÄŸitilebilir olup olmadÄ±klarÄ±nÄ± gÃ¶rmek iÃ§in katman katman bile kontrol edebiliriz.


```python
for layer_number, layer in enumerate(base_model.layers):
  print(layer_number, layer.name, layer.trainable)
```

    0 input_3 False
    1 rescaling_2 False
    2 normalization_2 False
    3 stem_conv_pad False
    4 stem_conv False
    5 stem_bn False
    6 stem_activation False
    7 block1a_dwconv False
    8 block1a_bn False
    9 block1a_activation False
    10 block1a_se_squeeze False
    11 block1a_se_reshape False
    12 block1a_se_reduce False
    13 block1a_se_expand False
    14 block1a_se_excite False
    15 block1a_project_conv False
    16 block1a_project_bn False
    17 block2a_expand_conv False
    18 block2a_expand_bn False
    19 block2a_expand_activation False
    20 block2a_dwconv_pad False
    21 block2a_dwconv False
    22 block2a_bn False
    23 block2a_activation False
    24 block2a_se_squeeze False
    25 block2a_se_reshape False
    26 block2a_se_reduce False
    27 block2a_se_expand False
    28 block2a_se_excite False
    29 block2a_project_conv False
    30 block2a_project_bn False
    31 block2b_expand_conv False
    32 block2b_expand_bn False
    33 block2b_expand_activation False
    34 block2b_dwconv False
    35 block2b_bn False
    36 block2b_activation False
    37 block2b_se_squeeze False
    38 block2b_se_reshape False
    39 block2b_se_reduce False
    40 block2b_se_expand False
    41 block2b_se_excite False
    42 block2b_project_conv False
    43 block2b_project_bn False
    44 block2b_drop False
    45 block2b_add False
    46 block3a_expand_conv False
    47 block3a_expand_bn False
    48 block3a_expand_activation False
    49 block3a_dwconv_pad False
    50 block3a_dwconv False
    51 block3a_bn False
    52 block3a_activation False
    53 block3a_se_squeeze False
    54 block3a_se_reshape False
    55 block3a_se_reduce False
    56 block3a_se_expand False
    57 block3a_se_excite False
    58 block3a_project_conv False
    59 block3a_project_bn False
    60 block3b_expand_conv False
    61 block3b_expand_bn False
    62 block3b_expand_activation False
    63 block3b_dwconv False
    64 block3b_bn False
    65 block3b_activation False
    66 block3b_se_squeeze False
    67 block3b_se_reshape False
    68 block3b_se_reduce False
    69 block3b_se_expand False
    70 block3b_se_excite False
    71 block3b_project_conv False
    72 block3b_project_bn False
    73 block3b_drop False
    74 block3b_add False
    75 block4a_expand_conv False
    76 block4a_expand_bn False
    77 block4a_expand_activation False
    78 block4a_dwconv_pad False
    79 block4a_dwconv False
    80 block4a_bn False
    81 block4a_activation False
    82 block4a_se_squeeze False
    83 block4a_se_reshape False
    84 block4a_se_reduce False
    85 block4a_se_expand False
    86 block4a_se_excite False
    87 block4a_project_conv False
    88 block4a_project_bn False
    89 block4b_expand_conv False
    90 block4b_expand_bn False
    91 block4b_expand_activation False
    92 block4b_dwconv False
    93 block4b_bn False
    94 block4b_activation False
    95 block4b_se_squeeze False
    96 block4b_se_reshape False
    97 block4b_se_reduce False
    98 block4b_se_expand False
    99 block4b_se_excite False
    100 block4b_project_conv False
    101 block4b_project_bn False
    102 block4b_drop False
    103 block4b_add False
    104 block4c_expand_conv False
    105 block4c_expand_bn False
    106 block4c_expand_activation False
    107 block4c_dwconv False
    108 block4c_bn False
    109 block4c_activation False
    110 block4c_se_squeeze False
    111 block4c_se_reshape False
    112 block4c_se_reduce False
    113 block4c_se_expand False
    114 block4c_se_excite False
    115 block4c_project_conv False
    116 block4c_project_bn False
    117 block4c_drop False
    118 block4c_add False
    119 block5a_expand_conv False
    120 block5a_expand_bn False
    121 block5a_expand_activation False
    122 block5a_dwconv False
    123 block5a_bn False
    124 block5a_activation False
    125 block5a_se_squeeze False
    126 block5a_se_reshape False
    127 block5a_se_reduce False
    128 block5a_se_expand False
    129 block5a_se_excite False
    130 block5a_project_conv False
    131 block5a_project_bn False
    132 block5b_expand_conv False
    133 block5b_expand_bn False
    134 block5b_expand_activation False
    135 block5b_dwconv False
    136 block5b_bn False
    137 block5b_activation False
    138 block5b_se_squeeze False
    139 block5b_se_reshape False
    140 block5b_se_reduce False
    141 block5b_se_expand False
    142 block5b_se_excite False
    143 block5b_project_conv False
    144 block5b_project_bn False
    145 block5b_drop False
    146 block5b_add False
    147 block5c_expand_conv False
    148 block5c_expand_bn False
    149 block5c_expand_activation False
    150 block5c_dwconv False
    151 block5c_bn False
    152 block5c_activation False
    153 block5c_se_squeeze False
    154 block5c_se_reshape False
    155 block5c_se_reduce False
    156 block5c_se_expand False
    157 block5c_se_excite False
    158 block5c_project_conv False
    159 block5c_project_bn False
    160 block5c_drop False
    161 block5c_add False
    162 block6a_expand_conv False
    163 block6a_expand_bn False
    164 block6a_expand_activation False
    165 block6a_dwconv_pad False
    166 block6a_dwconv False
    167 block6a_bn False
    168 block6a_activation False
    169 block6a_se_squeeze False
    170 block6a_se_reshape False
    171 block6a_se_reduce False
    172 block6a_se_expand False
    173 block6a_se_excite False
    174 block6a_project_conv False
    175 block6a_project_bn False
    176 block6b_expand_conv False
    177 block6b_expand_bn False
    178 block6b_expand_activation False
    179 block6b_dwconv False
    180 block6b_bn False
    181 block6b_activation False
    182 block6b_se_squeeze False
    183 block6b_se_reshape False
    184 block6b_se_reduce False
    185 block6b_se_expand False
    186 block6b_se_excite False
    187 block6b_project_conv False
    188 block6b_project_bn False
    189 block6b_drop False
    190 block6b_add False
    191 block6c_expand_conv False
    192 block6c_expand_bn False
    193 block6c_expand_activation False
    194 block6c_dwconv False
    195 block6c_bn False
    196 block6c_activation False
    197 block6c_se_squeeze False
    198 block6c_se_reshape False
    199 block6c_se_reduce False
    200 block6c_se_expand False
    201 block6c_se_excite False
    202 block6c_project_conv False
    203 block6c_project_bn False
    204 block6c_drop False
    205 block6c_add False
    206 block6d_expand_conv False
    207 block6d_expand_bn False
    208 block6d_expand_activation False
    209 block6d_dwconv False
    210 block6d_bn False
    211 block6d_activation False
    212 block6d_se_squeeze False
    213 block6d_se_reshape False
    214 block6d_se_reduce False
    215 block6d_se_expand False
    216 block6d_se_excite False
    217 block6d_project_conv False
    218 block6d_project_bn False
    219 block6d_drop False
    220 block6d_add False
    221 block7a_expand_conv False
    222 block7a_expand_bn False
    223 block7a_expand_activation False
    224 block7a_dwconv False
    225 block7a_bn False
    226 block7a_activation False
    227 block7a_se_squeeze False
    228 block7a_se_reshape False
    229 block7a_se_reduce False
    230 block7a_se_expand False
    231 block7a_se_excite False
    232 block7a_project_conv False
    233 block7a_project_bn False
    234 top_conv False
    235 top_bn False
    236 top_activation False


GÃ¼zel! GÃ¶rÃ¼nÃ¼ÅŸe gÃ¶re son 10 dÄ±ÅŸÄ±ndaki tÃ¼m katmanlar donmuÅŸ ve eÄŸitilemez. Bu, Ã§Ä±ktÄ± katmanÄ±yla birlikte yalnÄ±zca temel modelin son 10 katmanÄ±nÄ±n aÄŸÄ±rlÄ±klarÄ±nÄ±n eÄŸitim sÄ±rasÄ±nda gÃ¼ncelleneceÄŸi anlamÄ±na gelir.

> ðŸ¤” Soru: Modeli neden yeniden derledik?

Modellerinizde her deÄŸiÅŸiklik yaptÄ±ÄŸÄ±nÄ±zda, onlarÄ± yeniden derlemeniz gerekir.

Bizim durumumuzda, Ã¶ncekiyle tamamen aynÄ± kayÄ±p, optimize edici ve metrikleri kullanÄ±yoruz, ancak bu sefer optimize edicimizin Ã¶ÄŸrenme oranÄ± Ã¶ncekinden 10 kat daha kÃ¼Ã§Ã¼k olacak (Adam'Ä±n varsayÄ±lan deÄŸeri olan 0,001 yerine 0,0001).

Bunu, modelin Ã¶nceden eÄŸitilmiÅŸ modeldeki mevcut aÄŸÄ±rlÄ±klarÄ±n Ã¼zerine Ã§ok hÄ±zlÄ± yazmaya Ã§alÄ±ÅŸmamasÄ± iÃ§in yapÄ±yoruz. BaÅŸka bir deyiÅŸle, Ã¶ÄŸrenmenin daha kademeli olmasÄ±nÄ± istiyoruz.

> ðŸ”‘ Not: Ä°nce ayar sÄ±rasÄ±nda Ã¶ÄŸrenme oranÄ±nÄ± ayarlamak iÃ§in belirlenmiÅŸ bir standart yoktur, ancak 2,6x-10x+'lik azalmalar uygulamada iyi sonuÃ§ veriyor gibi gÃ¶rÃ¼nmektedir.

Åžimdi kaÃ§ tane eÄŸitilebilir deÄŸiÅŸkenimiz var?


```python
fine_tune_epochs = initial_epochs + 5

history_fine_10_percent_data_aug = model_2.fit(train_data_10_percent,
                                               epochs=fine_tune_epochs,
                                               validation_data=test_data,
                                               initial_epoch=history_10_percent_data_aug.epoch[-1], 
                                               validation_steps=int(0.25 * len(test_data)),
                                               callbacks=[create_tensorboard_callback("transfer_learning", "10_percent_fine_tune_last_10")]) 
```

    TensorBoard verilerini bu klasÃ¶re kaydet: transfer_learning/10_percent_fine_tune_last_10/20210720-053326
    Epoch 5/10


    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.
      category=CustomMaskWarning)


    24/24 [==============================] - 14s 559ms/step - loss: 0.7338 - accuracy: 0.8067 - val_loss: 0.6053 - val_accuracy: 0.8454
    Epoch 6/10
    24/24 [==============================] - 12s 499ms/step - loss: 0.6676 - accuracy: 0.8267 - val_loss: 0.5780 - val_accuracy: 0.8503
    Epoch 7/10
    24/24 [==============================] - 13s 514ms/step - loss: 0.6145 - accuracy: 0.8453 - val_loss: 0.5514 - val_accuracy: 0.8602
    Epoch 8/10
    24/24 [==============================] - 12s 493ms/step - loss: 0.5557 - accuracy: 0.8613 - val_loss: 0.5248 - val_accuracy: 0.8569
    Epoch 9/10
    24/24 [==============================] - 10s 410ms/step - loss: 0.5442 - accuracy: 0.8640 - val_loss: 0.5263 - val_accuracy: 0.8487
    Epoch 10/10
    24/24 [==============================] - 12s 483ms/step - loss: 0.5360 - accuracy: 0.8520 - val_loss: 0.4952 - val_accuracy: 0.8618


> ðŸ”‘ Not: Ä°nce ayar, Ã¶zellik Ã§Ä±karmadan genellikle dÃ¶nem baÅŸÄ±na Ã§ok daha uzun sÃ¼rer (aÄŸ genelinde daha fazla aÄŸÄ±rlÄ±ÄŸÄ±n gÃ¼ncellenmesi nedeniyle).

GÃ¶rÃ¼nÃ¼ÅŸe gÃ¶re modelimiz birkaÃ§ yÃ¼zde doÄŸruluk puanÄ± kazanmÄ±ÅŸ! Onu deÄŸerlendirelim.


```python
results_fine_tune_10_percent = model_2.evaluate(test_data)
```

    79/79 [==============================] - 10s 116ms/step - loss: 0.5341 - accuracy: 0.8472


UnutmayÄ±n, eÄŸitim sÄ±rasÄ±nda test verilerinin yalnÄ±zca %25'ini deÄŸerlendirdiÄŸimiz iÃ§in modeli deÄŸerlendirmenin sonuÃ§larÄ± eÄŸitimden elde edilen Ã§Ä±ktÄ±lardan biraz farklÄ± olabilir.

Pekala, ince ayardan Ã¶nce ve sonra modelimizin performansÄ±nÄ± deÄŸerlendirmek iÃ§in bir yola ihtiyacÄ±mÄ±z var. Ã–ncesini ve sonrasÄ±nÄ± karÅŸÄ±laÅŸtÄ±ran bir fonksiyon yazmaya ne dersiniz?


```python
def compare_historys(original_history, new_history, initial_epochs=5):
    acc = original_history.history["accuracy"]
    loss = original_history.history["loss"]

    print(len(acc))

    val_acc = original_history.history["val_accuracy"]
    val_loss = original_history.history["val_loss"]

    total_acc = acc + new_history.history["accuracy"]
    total_loss = loss + new_history.history["loss"]

    total_val_acc = val_acc + new_history.history["val_accuracy"]
    total_val_loss = val_loss + new_history.history["val_loss"]

    print(len(total_acc))
    print(total_acc)

    plt.figure(figsize=(8, 8))
    plt.subplot(2, 1, 1)
    plt.plot(total_acc, label='Training Accuracy')
    plt.plot(total_val_acc, label='Validation Accuracy')
    plt.plot([initial_epochs-1, initial_epochs-1],
              plt.ylim(), label='Start Fine Tuning') 
    plt.legend(loc='lower right')
    plt.title('Training and Validation Accuracy')

    plt.subplot(2, 1, 2)
    plt.plot(total_loss, label='Training Loss')
    plt.plot(total_val_loss, label='Validation Loss')
    plt.plot([initial_epochs-1, initial_epochs-1],
              plt.ylim(), label='Start Fine Tuning')
    plt.legend(loc='upper right')
    plt.title('Training and Validation Loss')
    plt.xlabel('epoch')
    plt.show()
```

Model eÄŸitimimizin geÃ§miÅŸ deÄŸiÅŸkenlerini kaydetmenin kullanÄ±ÅŸlÄ± olduÄŸu yer burasÄ±dÄ±r. Modelimizin son 10 katmanÄ±na ince ayar yaptÄ±ktan sonra neler olduÄŸunu gÃ¶relim.


```python
compare_historys(original_history=history_10_percent_data_aug, 
                 new_history=history_fine_10_percent_data_aug, 
                 initial_epochs=5)
```

    5
    11
    [0.3426666557788849, 0.6786666512489319, 0.737333357334137, 0.7839999794960022, 0.8013333082199097, 0.8066666722297668, 0.8266666531562805, 0.8453333377838135, 0.8613333106040955, 0.8640000224113464, 0.8519999980926514]



    
![png](TensorFlow_ile_Transfer_Learning_%C4%B0nce_Ayarlama_%28Fine_Tuning%29_files/TensorFlow_ile_Transfer_Learning_%C4%B0nce_Ayarlama_%28Fine_Tuning%29_89_1.png)
    


Ä°nce ayardan sonra eÄŸriler doÄŸru yÃ¶ne gidiyor gibi gÃ¶rÃ¼nÃ¼yor. Ancak unutmayÄ±n, ince ayarÄ±n genellikle daha bÃ¼yÃ¼k miktarda veriyle en iyi sonucu verdiÄŸine dikkat edilmelidir.

#### Model 4: Mevcut Bir Modelde TÃ¼m Verilerin Ä°nce AyarÄ±nÄ± Yapma (Fine Tuning)

Bir modelin ince ayarÄ±nÄ±n genellikle daha fazla veriyle nasÄ±l Ã§alÄ±ÅŸtÄ±ÄŸÄ± hakkÄ±nda yeterince konuÅŸtuk, hadi deneyelim.

10 yemek sÄ±nÄ±fÄ± veri setimizin tam sÃ¼rÃ¼mÃ¼nÃ¼ indirerek baÅŸlayacaÄŸÄ±z.


```python
!gdown --id 1EJHNCG19hJG6XwIFxt2rpah-Q1Ikrbxw
unzip_data("10_food_classes_all_data.zip")

train_dir = "10_food_classes_all_data/train/"
test_dir = "10_food_classes_all_data/test/"
```


```python
walk_through_dir("10_food_classes_all_data")
```

Åžimdi gÃ¶rÃ¼ntÃ¼leri tensÃ¶r veri kÃ¼melerine Ã§evireceÄŸiz.


```python
import tensorflow as tf
IMG_SIZE = (224, 224)
train_data_10_classes_full = tf.keras.preprocessing.image_dataset_from_directory(train_dir,
                                                                                 label_mode="categorical",
                                                                                 image_size=IMG_SIZE)

test_data = tf.keras.preprocessing.image_dataset_from_directory(test_dir,
                                                                label_mode="categorical",
                                                                image_size=IMG_SIZE)
```

Bu iyi gÃ¶rÃ¼nÃ¼yor. Ã‡alÄ±ÅŸmak iÃ§in eÄŸitim sÄ±nÄ±flarÄ±nda 10 kat daha fazla gÃ¶rselimiz var.

Test veri seti, Ã¶nceki deneylerimiz iÃ§in kullandÄ±ÄŸÄ±mÄ±zla aynÄ±dÄ±r.

Åžimdi olduÄŸu gibi, model_2'miz verilerin yÃ¼zde 10'unda ince ayar yapÄ±ldÄ±, bu nedenle tÃ¼m verilerde ince ayar yapmaya baÅŸlamak ve deneylerimizi tutarlÄ± tutmak iÃ§in 5 epoch'tan sonra kontrol ettiÄŸimiz aÄŸÄ±rlÄ±klara geri dÃ¶ndÃ¼rmemiz gerekiyor.

Bunu gÃ¶stermek iÃ§in Ã¶nce mevcut model_2'yi deÄŸerlendireceÄŸiz.



```python
model_2.evaluate(test_data)
```

Bunlar, result_fine_tune_10_percent ile aynÄ± deÄŸerlerdir.


```python
results_fine_tune_10_percent
```

Åžimdi modeli kaydedilen aÄŸÄ±rlÄ±klara geri dÃ¶ndÃ¼receÄŸiz.


```python
model_2.load_weights(checkpoint_path)
```

Ve sonuÃ§lar, result_10_percent_data_aug ile aynÄ± olmalÄ±dÄ±r.


```python
model_2.evaluate(test_data)
```


```python
results_10_percent_data_aug
```

Pekala, Ã¶nceki adÄ±mlar oldukÃ§a kafa karÄ±ÅŸtÄ±rÄ±cÄ± gÃ¶rÃ¼nebilir ancak tek yaptÄ±ÄŸÄ±mÄ±z:

1. Verilerin %10'unda (tÃ¼m temel model katmanlarÄ± donmuÅŸ halde) 5 epoch iÃ§in bir Ã¶zellik Ã§Ä±karma transferi Ã¶ÄŸrenme modeli eÄŸitildi ve ModelCheckpoint kullanÄ±larak modelin aÄŸÄ±rlÄ±klarÄ± kaydedildi.
2. Temel modelin ilk 10 katmanÄ± dondurulmamÄ±ÅŸ olarak, 5 epoch daha iÃ§in aynÄ± %10'luk veri Ã¼zerinde aynÄ± modelde ince ayar yapÄ±ldÄ±.
3. Her seferinde sonuÃ§larÄ± ve eÄŸitim gÃ¼nlÃ¼klerini kaydedildi.
4. 2 ile aynÄ± adÄ±mlarÄ± ancak tÃ¼m verilerle yapmak iÃ§in modeli 1'den yeniden yÃ¼kledi.

2 ile aynÄ± adÄ±mlar?
> Evet, temel modelin son 10 katmanÄ±na tam veri seti ile 5 dÃ¶nem daha ince ayar yapacaÄŸÄ±z ama Ã¶nce kendimize hangi katmanlarÄ±n eÄŸitilebilir olduÄŸunu hatÄ±rlatalÄ±m.


```python
for layer_number, layer in enumerate(model_2.layers):
  print(layer_number, layer.name, layer.trainable)
```


```python
for layer_number, layer in enumerate(base_model.layers):
  print(layer_number, layer.name, layer.trainable)
```

Ä°yi gÃ¶rÃ¼nÃ¼yor! Son 10 katman eÄŸitilebilir (dondurulmamÄ±ÅŸ). Ä°nce ayara baÅŸlamadan Ã¶nce yapmamÄ±z gereken bir adÄ±m daha var.

Ne olduÄŸunu hatÄ±rlÄ±yor musun?

Sana bir ipucu vereceÄŸim. AÄŸÄ±rlÄ±klarÄ± modelimize yeniden yÃ¼kledik ve modellerimizde her deÄŸiÅŸiklik yaptÄ±ÄŸÄ±mÄ±zda ne yapmamÄ±z gerekiyor?

OnlarÄ± yeniden derleyin! Bu daha Ã¶nce olduÄŸu gibi olacak.


```python
model_2.compile(loss="categorical_crossentropy",
                optimizer=tf.keras.optimizers.Adam(lr=0.0001),
                metrics=["accuracy"])
```

Pekala, tÃ¼m verilerde ince ayar yapma zamanÄ±!


```python
fine_tune_epochs = initial_epochs + 5

history_fine_10_classes_full = model_2.fit(train_data_10_classes_full,
                                           epochs=fine_tune_epochs,
                                           initial_epoch=history_10_percent_data_aug.epoch[-1],
                                           validation_data=test_data,
                                           validation_steps=int(0.25 * len(test_data)),
                                           callbacks=[create_tensorboard_callback("transfer_learning", "full_10_classes_fine_tune_last_10")])
```

> ðŸ”‘ Not: EÄŸitim epoch baÅŸÄ±na daha uzun sÃ¼rdÃ¼, ancak bu mantÄ±klÄ± Ã§Ã¼nkÃ¼ Ã¶ncekinden 10 kat daha fazla eÄŸitim verisi kullanÄ±yoruz.

TÃ¼m test verilerini deÄŸerlendirelim.


```python
results_fine_tune_full_data = model_2.evaluate(test_data)
results_fine_tune_full_data
```

GÃ¼zel! TÃ¼m verilerle yapÄ±lan ince ayar, modelimize hÄ±z kazandÄ±rmÄ±ÅŸ gibi gÃ¶rÃ¼nÃ¼yor, eÄŸitim eÄŸrileri nasÄ±l gÃ¶rÃ¼nÃ¼yor?


```python
compare_historys(original_history=history_10_percent_data_aug,
                 new_history=history_fine_10_classes_full,
                 initial_epochs=5)
```

GÃ¶rÃ¼nÃ¼ÅŸe gÃ¶re bu ekstra veriler yardÄ±mcÄ± oldu! Bu kÄ±vrÄ±mlar harika gÃ¶rÃ¼nÃ¼yor. Ve daha uzun sÃ¼re antrenman yaparsak, geliÅŸmeye devam edebilirler.

## Deney Verilerimizi TensorBoard'da GÃ¶rÃ¼ntÃ¼leme

Åžu anda deneysel sonuÃ§larÄ±mÄ±z defterimizin her yerine daÄŸÄ±lmÄ±ÅŸ durumda. BunlarÄ± birisiyle paylaÅŸmak istersek, bir sÃ¼rÃ¼ farklÄ± grafik ve metrik alacaklardÄ±r... eÄŸlenceli bir zaman deÄŸil.

Ama tahmin et ne oldu?

YardÄ±mcÄ± fonksiyonumuz `create_tensorflow_callback()` ile yaptÄ±ÄŸÄ±mÄ±z TensorBoard geri Ã§aÄŸrÄ±sÄ± sayesinde modelleme deneylerimizi sÃ¼rekli takip ediyoruz.

BunlarÄ± TensorBoard.dev'e yÃ¼kleyip kontrol etmeye ne dersiniz?

Tensorboard dev upload komutu ile deneylerimizin kaydedildiÄŸi dizine iletebiliriz.

> ðŸ”‘ Not: TensorBoard.dev'e yÃ¼klediÄŸiniz her ÅŸeyin herkese aÃ§Ä±k hale geleceÄŸini unutmayÄ±n. PaylaÅŸmak istemediÄŸiniz kayÄ±tlarÄ± varsa yÃ¼klemeyin.


```python
!tensorboard dev upload --logdir ./transfer_learning \
  --name "Transfer learning experiments" \
  --description "A series of different transfer learning experiments with varying amounts of data and fine-tuning" \
  --one_shot
```

SonuÃ§larÄ± TensorBoard.dev'e yÃ¼kledikten sonra, deneylerimizi gÃ¶rÃ¼ntÃ¼lemek ve karÅŸÄ±laÅŸtÄ±rmak ve gerekirse sonuÃ§larÄ±mÄ±zÄ± baÅŸkalarÄ±yla paylaÅŸmak iÃ§in kullanabileceÄŸimiz paylaÅŸÄ±labilir bir baÄŸlantÄ± alÄ±rÄ±z.

> ðŸ¤” Soru: Hangi model en iyi performansÄ± gÃ¶sterdi? Sizce bu neden? Ä°nce ayar nasÄ±l gitti?

Tensorboard dev list komutunu kullanarak Ã¶nceki tÃ¼m TensorBoard.dev deneylerinizi bulmak iÃ§in.


```python
!tensorboard dev list
```

Ve Ã¶nceki bir denemeyi kaldÄ±rmak (ve genel gÃ¶rÃ¼ntÃ¼lemeden silmek) istiyorsanÄ±z ÅŸu komutu kullanabilirsiniz:


```python
# !tensorboard dev delete --experiment_id OUbW0O3pRqqQgAphVBxi8Q
```
